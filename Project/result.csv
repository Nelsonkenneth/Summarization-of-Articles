author,title,summary
Justin Lee,Chatbots were the next big thing: what happened? – The Startup – Medium,"But a bot isn’t the same as a human. Remember: a bot that does ONE thing well is infinitely more helpful than a bot that does multiple things poorly. Building a bot for the sake of it, letting it loose and hoping for the best will never end well:
The vast majority of bots are built using decision-tree logic, where the bot’s canned response relies on spotting specific keywords in the user input. The next wave will be multimodal apps, where you can say what you want (like with Siri) and get back information as a map, text, or even a spoken response. Are chatbots cheaper or faster than apps? At the end of the day, no amount of witty quips or human-like mannerisms will save a bot from conversational failure. Despite the constant hype around AI, we’re still a long way from achieving anything remotely human-like. Conversational UIs are built to replicate the way humans prefer to communicate — with other humans. Computers aren’t good at understanding human emotion. What is too complicated is trying to complete these tasks with a bot — and having the bot fail."
Conor Dewey,Python for Data Science: 8 Concepts You May Have Forgotten,"Lambda functions are used for creating small, one-time and anonymous function objects in Python. The basic syntax of lambda functions is:
Note that lambda functions can do everything that regular functions can do, as long as there’s just one expression. So given a starting and stopping point, as well as a number of values, linspace will evenly space them out for you in a NumPy array. Check out the simple example below and the upcoming video to get a better feel for the power of lambda functions:
Once you have a grasp on lambda functions, learning to pair them with the map and filter functions can be a powerful tool. The filter function takes in a list and a rule, much like map, however it returns a subset of the original list by comparing each element against the boolean filtering rule. For creating quick and easy Numpy arrays, look no further than the arange and linspace functions. Note that the list function simply converts the output to list type. Think of apply as a map function, but made for Pandas DataFrames or more specifically, for Series. Writing out a for loop every time you need to define some sort of list is tedious, luckily Python has a built-in way to address this problem in just one line of code. Ever get tired of creating function after function for limited use cases?"
William Koehrsen,Automated Feature Engineering in Python – Towards Data Science,"Deep feature synthesis in turn stacks feature primitives — aggregations, which act across a one-to-many relationship between tables, and transformations, functions applied to one or more columns in a single table — to build new features from multiple tables. Deep feature synthesis stacks multiple transformation and aggregation operations (which are called feature primitives in the vocab of featuretools) to create features from data spread across many tables. Feature engineering, also known as feature creation, is the process of constructing new features from existing data to train a machine learning model. Using concepts of entitysets, entities, and relationships, featuretools can perform deep feature synthesis to create new features. A deep feature is simply a feature made of stacking multiple primitives and dfs is the name of process that makes these features. For example, we have the month each client joined which is a transformation feature primitive:
We also have a number of aggregation primitives such as the average payment amounts for each client:
Even though we specified only a few feature primitives, featuretools created many new features by combining and stacking these primitives. To do this, we use the same ft.dfs function call but do not pass in any feature primitives:
Featuretools has built many new features for us to use. These are simply the basic operations that we use to form new features:
New features are created in featuretools using these primitives either by themselves or stacking multiple primitives. To make features with specified primitives we use the ft.dfs function (standing for deep feature synthesis). Automated feature engineering has solved one problem, but created another: too many features."
Gant Laborde,Machine Learning: how to go from Zero to Hero – freeCodeCamp,"If you want to do something new, not just new to you, but to the world, you can do it with ML. Even if you’re not a Java buff, the presentation Jim gives on all things Machine Learning is a pretty cool 1.5+ hour introduction into ML concepts, which includes more info on many of the examples above. It’s cool watching data go through a trained model, but you can even watch your neural network get trained. But more so, if you do make it through, you’ll have a deep understanding of the implementation of Machine Learning that will catapult you into successfully applying it in new and world-changing ways. So how does something like that even work? You can teach a computer to play video games, understand language, and even how to identify people or things. In this approach, you’ll understand Machine Learning down to the algorithms and the math. The only problem is we don’t know Machine Learning, and we don’t know how to hook it up to video games. One of the classic real-world examples of Machine Learning in action is the iris data set from 1936. I’m talking about Machine Learning."
Emmanuel Ameisen,Reinforcement Learning from scratch – Insight Data,"We learn our policy directly with policy gradients (defined above), and learn a value function using something called Advantage. While the value function tells us how good we estimate each action to be, the policy is the function that determines which actions we end up taking. 4+/Leveraging deep learning for representations
In practice, many state of the art RL methods require learning both a policy and value estimates. This allows us to learn a good value function. On-policy methods can only learn from actions that were taken following our policy (remember, a policy is the method we use to determine which actions to take). Previously, we were first learning a value function Q for each action in each state and then building a policy on top. The approach we will use here is called Policy Gradients, and is an on-policy method. We will use a method called Temporal Difference (TD) learning to learn a good Q function. In Vanilla Policy Gradient, we still use Monte Carlo Estimates, but we learn our policy directly through a loss function that increases the probability of choosing rewarding actions. A note about off-policy vs on-policy learning: The methods we used previously, are off-policy methods, meaning we can generate data with any strategy(using epsilon greedy for example) and learn from it."
Irhum Shafkat,Intuitively Understanding Convolutions for Deep Learning,"Each filter in a convolution layer produces one and only one output channel, and they do it like so:
Each of the kernels of the filter “slides” over their respective input channels, producing a processed version of each. Even if were we to apply a kernel of the same size (3×3), having the same local area, to the output of the strided convolution, the kernel would have a larger effective receptive field:
This is because the output of the strided layer still does represent the same image. If this were a standard fully connected layer, you’d have a weight matrix of 25×9 = 225 parameters, with every output feature being the weighted sum of every single input feature. The output features are essentially, the weighted sums (with the weights being the values of the kernel itself) of the input features located roughly in the same location of the output pixel on the input layer. Each filter actually happens to be a collection of kernels, with there being one kernel for every single input channel to the layer, and each kernel being unique. Convolutions allow us to do this transformation with only 9 parameters, with each output feature, instead of “looking at” every input feature, only getting to “look” at input features coming from roughly the same location. And with the single filter case down, the case for any number of filters is identical: Each filter processes the input with its own, different set of kernels and a scalar bias with the process described above, producing a single output channel. Followed by a final pooling layer, which collapses each 7×7 grid into a single pixel, each channel is a feature detector with a receptive field equivalent to the entire image. Both are methods of increasing the receptive field, but dilated convolutions are a single layer, while this takes place on a regular convolution following a strided convolution, with a nonlinearity inbetween)
This expansion of the receptive field allows the convolution layers to combine the low level features (lines, edges), into higher level features (curves, textures), as we see in the mixed3a layer. The kernels of a filter each produce one version of each channel, and the filter as a whole produces one overall output channel."
Sam Drozdov,An intro to Machine Learning for designers – UX Collective,"Since machine learning is now more accessible than ever before, designers today have the opportunity to think about how machine learning can be applied to improve their products. Supervised learning allows us to make predictions using correctly labeled data. Understanding the problem you are trying to solve and the available data will constrain the types of machine learning you can use (e.q. Machine learning allows us to make predictions about how a user might behave next. For the same reasons, designers should know about machine learning. Reinforcement learning doesn’t use an existing data set. If you are interested in learning more about machine learning, here are some helpful resources:
Thanks for reading. Depending on the application and what data is available, there are different types of machine learning algorithms to choose from. Over the past decade new algorithms, better hardware, and more data have made machine learning an order of magnitude more effective. Machine learning can help create user-centric products by personalizing experiences to the individuals who use them."
Conor Dewey,The Big List of DS/ML Interview Resources – Towards Data Science,"Data science interviews certainly aren’t easy. Specifically, I highly recommend checking out the first two links regarding 120 Data Science Interview Questions. This won’t be covered in every single data science interview, but it’s certainly not uncommon. If you’re interested in receiving my weekly rundown of interesting articles and resources focused on data science, machine learning, and artificial intelligence, then subscribe to Self Driven Data Science using the form below! Machine learning is a complex field that is a virtual guarantee in data science interviews today. Here’s some of the more general resources covering data science as a whole. Through this exciting and somewhat (at times, very) painful process, I’ve accumulated a plethora of useful resources that helped me prepare for and eventually pass data science interviews. A data science interview typically isn’t complete without checking your knowledge of SQL. Even Data Scientists cannot escape the dreaded algorithmic coding interview. Lastly, this post is part of an ongoing initiative to ‘open-source’ my experience applying and interviewing at data science positions, so if you enjoyed this content then be sure to follow me for more stuff like this."
Abhishek Parbhakar,Must know Information Theory concepts in Deep Learning (AI),"Cross entropy between two probability distributions p and q defined over same set of outcomes is given by:
Mutual information is a measure of mutual dependency between two probability distributions or random variables. The probability distribution of experiment is used to calculate the entropy. Mutual information of two discrete random variables X and Y is defined as:
where p(x,y) is the joint probability distribution of X and Y, and p(x) and p(y) are the marginal probability distribution of X and Y respectively. Cross entropy is used to compare two probability distributions. KL divergence between ‘P’ and ‘Q’ tells us how much information we lose when we try to approximate data given by ‘P’ with ‘Q’. Entropy gives a measure of uncertainty in an experiment. KL divergence of a probability distribution Q from another probability distribution P is defined as:
KL divergence is commonly used in unsupervised machine learning technique Variational Autoencoders. For example, in an deterministic experiment, we always know the outcome, so no new information gained is here from observing the outcome and hence entropy is zero. The information gained for a outcome of an experiment is defined as a function of probability of occurrence of that outcome. KL divergence is another measure to find similarities between two probability distributions."
Aman Dalmia,What I learned from interviewing at multiple AI companies and start-ups,"Also, when I was preparing, I noticed people using a lot of resources but as per my experience over the past months, I realised that one can do away with a few minimal ones for most roles in AI, all of which I’m going to mention at the end of the post. There are mostly two types of interviews — one, where the interviewer has come with come prepared set of questions and is going to just ask you just that irrespective of your profile and the second, where the interview is based on your CV. Now we come to the conclusion of the interview where the interviewer would ask you if you have any questions for them. People DO go through your Github because that’s the only way they have to validate what you have mentioned in your CV, given that there’s a lot of noise today with people associating all kinds of buzzwords with their profile. The motive of this kind of interview is to really check whether whatever you have written on your CV is true or not:
There would be a lot of questions on what could be done differently or if “X” was used instead of “Y”, what would have happened. This post arose from a discussion with one of my juniors on the lack of really fulfilling job opportunities offered through campus placements for people working in AI. Also, you won’t need to prepare answers to various kinds of questions that you get asked during an interview. It’s really easy to think that your interview is done and just say that you have nothing to ask. The resources mentioned in the Minimal Resources you need for preparation section should suffice, but make sure that you don’t miss out one bit among them. In the process, not only did I get an opportunity to interact with many great minds, but also had a peek at myself along with a sense of what people really look for when interviewing someone."
Sophia Arakelyan,From Ballerina to AI Researcher: Part I – buZZrobot,"I’ll dedicate the sequence of blog posts during the OpenAI Scholars program to several aspects of AI technology. Finding your true calling — the key component of happiness
My primary goal with the series of blog posts “From Ballerina to AI researcher” is to show that it’s never too late to embrace a new field, start over again, and find your true calling. I feel lucky that I found my true passion — AI. Founder of buZZrobot.com
The publication aims to cover practical aspects of AI technology, use cases along with interviews with notable people in the AI field. The transformer architecture reduced this problem thanks to the multi-head self-attention mechanism. To me, the technology itself and the AI community — researchers, scientists, people who dedicate their lives to building the most powerful technology of all time with the mission to benefit humanity and make it safe for us — is a great source of energy. The structure of the blog post series
Today, I’m giving an overall intro of what I’m going to cover in my “From Ballerina to AI Researcher” series. Last year, I published the article “From Ballerina to AI writer” where I described how I embraced the technical part of AI without a technical background. Recently, I’ve become a participant in the OpenAI Scholarship Program (OpenAI is a non-profit that gathers top AI researchers to ensure the safety of AI to benefit humanity). Every week for the next three months I’ll publish blog posts sharing my story of transformation from a person dedicated to 15 years of professional dancing and then writing about tech and AI to actually conducting AI research."
Dr. GP Pulipaka,"3 Ways to Apply Latent Semantic Analysis on Large-Corpus Text on macOS Terminal, JupyterLab, and...","Natural language processing in Python with NLTK library applies a low-rank approximation to the term-document matrix. Latent semantic analysis is a technique that applies singular value decomposition and principal component analysis (PCA). Matrix decompositions and latent semantic indexing. The latent semantic analysis is a particular technique in semantic space to parse through the document and identify the words with polysemy with NLKT library. The document can be represented with Z x Y Matrix A, the rows of the matrix represent the document in the collection. An Introduction to Latent Semantic Analysis. Mounting Google Drive on Google Colab. Once the drive is mounted, Colab has access to the datasets from Google drive. Latent semantic analysis
Applying latent semantic analysis on large datasets of text and documents represents the contextual meaning through mathematical and statistical computation methods on large corpus of text. Later, the low-rank approximation aids in indexing and retrieving the document known as latent semantic indexing by clustering the number of words in the document."
Scott Santens,Deep Learning Is Going to Teach Us All the Lesson of Our Lives: Jobs Are for Machines,"During a panel discussion at the end of 2015 at Singularity University, prominent data scientist Jeremy Howard asked “Do you want half of people to starve because they literally can’t add economic value, or not?” before going on to suggest, ”If the answer is not, then the smartest way to distribute the wealth is by implementing a universal basic income.”
AI pioneer Chris Eliasmith, director of the Centre for Theoretical Neuroscience, warned about the immediate impacts of AI on society in an interview with Futurism, “AI is already having a big impact on our economies... My suspicion is that more countries will have to follow Finland’s lead in exploring basic income guarantees for people.”
Moshe Vardi expressed the same sentiment after speaking at the 2016 annual meeting of the American Association for the Advancement of Science about the emergence of intelligent machines, “we need to rethink the very basic structure of our economic system... we may have to consider instituting a basic income guarantee.”
Even Baidu’s chief scientist and founder of Google’s “Google Brain” deep learning project, Andrew Ng, during an onstage interview at this year’s Deep Learning Summit, expressed the shared notion that basic income must be “seriously considered” by governments, citing “a high chance that AI will create massive labor displacement.”
When those building the tools begin warning about the implications of their use, shouldn’t those wishing to use those tools listen with the utmost attention, especially when it’s the very livelihoods of millions of people at stake? The result amazingly is us, and what we’ve been learning about how we work, we’ve now begun applying to the way machines work. Everything we do is generating data like never before, and lots of data is exactly what machines need in order to learn to learn. The idea is to put machines to work for us, but empower ourselves to seek out the forms of remaining work we as humans find most valuable, by simply providing everyone a monthly paycheck independent of work. So the answer to when the game of Go would fall to machines wasn’t even close to ten years. However, despite all these milestones, when asked to estimate when a computer would defeat a prominent Go player, the answer even just months prior to the announcement by Google of AlphaGo’s victory, was by experts essentially, “Maybe in another ten years.” A decade was considered a fair guess because Go is a game so complex I’ll just let Ken Jennings of Jeopardy fame, another former champion human defeated by AI, describe it:
Such confounding complexity makes impossible any brute-force approach to scan every possible move to determine the next best move. Is it to allow us to work more hours for even less pay? It’s routine manual work that Henry Ford transformed by paying people middle class wages to perform, and it’s routine cognitive work that once filled US office spaces. Rules can be written for work that doesn’t change, and that work can be better handled by machines. Is it even possible that many of the jobs we’re creating don’t need to exist at all, and only do because of the incomes they provide?"
Adam Geitgey,Machine Learning is Fun! – Adam Geitgey – Medium,"If we could just figure out the perfect weights to use that work for every house, our function could predict house prices! Machine learning is the idea that there are generic algorithms that can tell you something interesting about a set of data without you having to write any custom code specific to the problem. Once you start seeing how easily machine learning techniques can be applied to problems that seem really hard (like handwriting recognition), you start to get the feeling that you could use machine learning to solve any problem and get an answer as long as you have enough data. To build your app, you feed your training data about each house into your machine learning algorithm. Here’s one way:
First, write a simple equation that represents Step #2 above:
Now let’s re-write exactly the same equation, but using a bunch of machine learning math jargon (that you can ignore for now):
This equation represents how wrong our price estimating function is for the weights we currently have set. A dumb way to figure out the best weights would be something like this:
Start with each weight set to 1.0:
Run every house you know about through your function and see how far off the function is at guessing the correct price for each house:
For example, if the first house really sold for $250,000, but your function guessed it sold for $178,000, you are off by $72,000 for that single house. You can think of machine learning algorithms as falling into one of two main categories — supervised learning and unsupervised learning. Maybe a better definition for “learning” in this case is “figuring out an equation to solve a specific problem based on some example data”. But there’s a problem — you can glance at a house and have a pretty good idea of what a house is worth, but your trainees don’t have your experience so they don’t know how to price their houses. But it’s important to remember that machine learning only works if the problem is actually solvable with the data that you have."
Adam Geitgey,Machine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networks,"But right now, our neural network can’t do this. But now we want to process images with our neural network. To a computer, an image is really just a grid of numbers that represent how dark each pixel is:
To feed an image into our neural network, we simply treat the 18x18 pixel image as an array of 324 numbers:
The handle 324 inputs, we’ll just enlarge our neural network to have 324 input nodes:
Notice that our neural network also has two outputs now (instead of just one). We need to be smarter about how we process images into our neural network. How in the world do we feed images into a neural network instead of just numbers? Instead of feeding entire images into our neural network as one grid of numbers, we’re going to do something a lot smarter that takes advantage of the idea that an object is the same no matter where it appears in a picture. We created a small neural network to estimate the price of a house based on how many bedrooms it had, how big it was, and which neighborhood it was in:
We also know that the idea of machine learning is that the same generic algorithms can be reused with different data to solve different problems. A neural network takes numbers as input. Now that we have a trained neural network, we can use it! Our program can now recognize birds in images!"
Adam Geitgey,Machine Learning is Fun! Part 4: Modern Face Recognition with Deep Learning,"The end result is we turn the original image into a very simple representation that captures the basic structure of a face in a simple way:
To find faces in this HOG image, all we have to do is find the part of our image that looks the most similar to a known HOG pattern that was extracted from a bunch of other training faces:
Using this technique, we can now easily find faces in any image:
If you want to try this step out yourself using Python and dlib, here’s code showing how to generate and view HOG representations of images. So all we need to do ourselves is run our face images through their pre-trained network to get the 128 measurements for each face. For each step, we’ll learn about a different machine learning algorithm. To find faces in an image, we’ll start by making our image black and white because we don’t need color data to find faces:
Then we’ll look at every single pixel in our image one at a time. Then we could measure our unknown face the same way and find the known face with the closest measurements. But instead of training the network to recognize pictures objects like we did last time, we are going to train it to generate 128 measurements for each face. We need to build a pipeline where we solve each step of face recognition separately and pass the result of the current step to the next step. Machine learning people call the 128 measurements of each face an embedding. The simplest approach to face recognition is to directly compare the unknown face we found in Step 2 with all the pictures we have of people that have already been tagged. It turns out that the measurements that seem obvious to us humans (like eye color) don’t really make sense to a computer looking at individual pixels in an image."
Xiaohan Zeng,"I interviewed at five top companies in Silicon Valley in five days, and luckily got five job offers","Three are machine learning engineer (LinkedIn, Google, Facebook), one is data engineer (Salesforce), and one is software engineer in general (Airbnb). In the five days from July 24th to 28th 2017, I interviewed at LinkedIn, Salesforce Einstein, Google, Airbnb, and Facebook, and got all five job offers. The bar is very high and the process is quite long, including one pre-screening questionnaire, one phone screening, one coding assignment, and one full onsite. One thing that I felt special about Google’s interviews is that the analysis of algorithm complexity is really important. I was able to skip the second round phone screening with Airbnb and Salesforce because I got the onsite at LinkedIn and Facebook after only one phone screening. The only difference is in the duration: For some companies like LinkedIn it’s one hour, while for Facebook and Airbnb it’s 45 minutes. Therefore I needed to prepare for three different areas: coding, machine learning, and system design. For machine learning positions some companies would ask ML questions. Here are some resources that I found really helpful:
Although system design interviews can cover a lot of topics, there are some general guidelines for how to approach the problem:
With all that said, the best way to practice for system design interviews is to actually sit down and design a system, i.e. I flew in on Sunday, had five full days of interviews with around 30 interviewers at some best tech companies in the world, and very luckily, got job offers from all five of them."
Gil Fewster,The mind-blowing AI announcement from Google that you probably missed.,"Up until September of last year, Google Translate used phrase-based translation. All that changed in September, when Google gave their translation tool a new engine: the Google Neural Machine Translation system (GNMT). Phrase-based translation has no capacity to make educated guesses at words it doesn’t recognize, and can’t learn from new input. Not only was the article competing with the pre-Christmas rush that most of us were navigating — it was also tucked away on Google’s Research Blog, beneath the geektastic headline Zero-Shot Translation with Google’s Multilingual Neural Machine Translation System. Here’s Google’s Mike Schuster, Nikhil Thorat, and Melvin Johnson from the original blog post:
Here you can see an advantage of Google’s new neural machine over the old phrase-based approach. A neural computing system designed to translate content from one human language into another developed its own internal language to make the task more efficient. Which is a shame, because it may just be the most astonishing article about machine learning that I read last year. Google Translate invented its own language to help it translate more effectively. The short version is that Google Translate got smart. Phrase-based translation is a blunt instrument."
Adam Geitgey,Machine Learning is Fun! Part 2 – Adam Geitgey – Medium,"Once I had all that data, I could use it to build a neural network to model how likely it is that you would start with any given letter. There just aren’t enough levels in the original Super Mario Brothers game to provide enough data for a really good model. I ended up with text files that looked like this:
Looking at the text file, you can see that Mario levels don’t really have much of a pattern if you read them line-by-line:
The patterns in a level really emerge when you think of the level as a series of columns:
So in order for the algorithm to find the patterns in our data, we need to feed the data in column-by-column. First, we need a data set for training our model. Because of this, it can model more cases than we could capture in one simple model. Let’s say I need to guess the next letter you are going to type at any point in your story. Can we use the same model that generated fake Hemingway text to generate fake Super Mario Brothers levels? With a lot more training, the model gets to the point where it generates perfectly valid data:
Let’s sample an entire level’s worth of data from our model and rotate it back horizontal:
This data looks great! Let’s use the first few words of Ernest Hemingway’s The Sun Also Rises as an example:
What letter is going to come next? But the one thing this kind of model can’t do is respond to patterns in data over time."
David Venturi,"Every single Machine Learning course on the internet, ranked by your reviews",It has a 4-star weighted average rating over 3 reviews. It has a 3.6-star weighted average rating over 5 reviews. It has a 4-star weighted average rating over 4 reviews. It has a 4.5-star weighted average rating over 6 reviews. It has a 2-star weighted average rating over 2 reviews. It has a 4.5-star weighted average rating over 2 reviews. Free and paid options available. Free and paid options available. Free and paid options available. Free and paid options available.
Michael Jordan,Artificial Intelligence — The Revolution Hasn’t Happened Yet,"As for the necessity argument, it is sometimes argued that the human-imitative AI aspiration subsumes IA and II aspirations, because a human-imitative AI system would not only be able to solve the classical problems of AI (as embodied, e.g., in the Turing test), but it would also be our best bet for solving IA and II problems. First, although one would not know it from reading the newspapers, success in human-imitative AI has in fact been limited — we are very far from realizing human-imitative AI aspirations. We need to solve IA and II problems on their own merits, not as a mere corollary to a human-imitative AI agenda. Whereas civil engineering and chemical engineering were built on physics and chemistry, this new engineering discipline will be built on ideas that the preceding century gave substance to — ideas such as “information,” “algorithm,” “data,” “uncertainty,” “computing,” “inference,” and “optimization.” Moreover, since much of the focus of the new discipline will be on data from and about humans, its development will require perspectives from the social sciences and humanities. One could argue that an AI system would not only imitate human intelligence, but also “correct” it, and would also scale to arbitrarily large problems. However, the current focus on doing AI research via the gathering of data, the deployment of “deep learning” infrastructure, and the demonstration of systems that mimic certain narrowly-defined human skills — with little in the way of emerging explanatory principles — tends to deflect attention from major open problems in classical AI. In the current era, we have a real opportunity to conceive of something historically new — a human-centric engineering discipline. It is those challenges that need to be in the forefront, and in such an effort a focus on human-imitative AI may be a distraction. The past two decades have seen major progress — in industry and academia — in a complementary aspiration to human-imitative AI that is often referred to as “Intelligence Augmentation” (IA). These are classical goals in human-imitative AI, but in the current hubbub over the “AI revolution,” it is easy to forget that they are not yet solved."
Eran Kampf,Data Mining — Handling Missing Values the Database – DeveloperZen,"Replace missing values of an attribute with the mean (or median if its discrete) value for that attribute in the database. Assuming the state of residence attribute data is missing for some students. Another example could be using a decision tree to try and predict the probable value in the missing attribute, according to other attributes in the data. For example, let’s say we have a database of students enrolment data (age, SAT score, state of residence, etc.) Let’s say you have a cars pricing database that, among other things, classifies cars to “Luxury” and “Low budget” and you’re dealing with missing values in the cost field. This is usually done when the class label is missing (assuming your data mining goal is classification), or many attributes are missing from the row (not just one). So how can you handle missing values in your database? One of the important stages of data mining is preprocessing, where we prepare the data for mining. I’ve recently answered Predicting missing data values in a database on StackOverflow and thought it deserved a mention on DeveloperZen. ...) and our goal for the data mining process."
Oliver Lindberg,"Interview with Google’s Alfred Spector on voice search, hybrid intelligence and more","VP of research Alfred Spector talks to Oliver Lindberg about the technologies emerging from Google Labs — from voice search to hybrid intelligence and beyond
This article originally appeared in issue 198 of .net magazine in 2010 and was republished at www.techradar.com. One of the areas Google is making significant advances in is voice search. “We need to get computers and people communicating in both directions, so the computer learns from the human and makes the human more effective.”
Examples of ‘hybrid intelligence’ are Google Suggest, which instantly offers popular searches as you type a search query, and the ‘did you mean?’ feature in Google search, which corrects you when you misspell a query in the search bar. “Voice is one of these grand technology challenges in computer science,” Spector explains. “One idea is to take all of the voices that the system hears over time into one huge pan-human voice model. “Can a computer understand the human voice? Google’s a pretty good search engine, right? We hired one of the best database researchers in the world, Alon Halevy, to lead it.”
Google is aiming to make more information available more easily across multiple devices, whether it’s images, videos, speech or maps, no matter which language we’re using. It’s the most spoken language in the world, but as it isn’t exactly keyboard-friendly, voice search could become immensely popular in China. It’s a fundamentally challenging problem.”
One example of conceptual search is Google Image Swirl, which was added to Labs in November."
Xu Wenhao,建议的程序员学习LDA算法的步骤 – 蒸汽与魔法,"这一阵为了工作上的关系,花了点时间学习了一下LDA算法,说实话,对于我这个学CS而非学数学的人来说,除了集体智慧编程这本书之外基本没怎么看过机器学习的人来说,一开始还真是摸不太到门道,前前后后快要四个月了,算是基本了解了这个算法的实现,记录一下,也供后来人快速入门做个参考。
一开始直接就下了Blei的原始的那篇论文来看,但是看了个开头就被Dirichlet分布和几个数学公式打倒,然后因为专心在写项目中的具体的代码,也就先放下了。但是因为发现完全忘记了本科学的概率和统计的内容,只好回头去看大学时候概率论的教材,发现早不知道借给谁了,于是上网买了本,花了几天时间大致回顾了一遍概率论的知识,什么贝叶斯全概率公式,正态分布,二项分布之类的。后来晚上没事儿的时候,去水木的AI版转了转,了解到了Machine Learning的圣经PRML,考虑到反正也是要长期学习了,搞了电子版,同时上淘宝买了个打印胶装的版本。春节里每天晚上看一点儿,扫了一下前两章,再次回顾了一下基本数学知识,然后了解了下贝叶斯学派那种采用共轭先验来建模的方式。于是再次尝试回头去看Blei的那篇论文,发现还是看不太懂,于是又放下了。然后某天Tony让我准备准备给复旦的同学们share一下我们项目中LDA的使用,为了不露怯,又去翻论文,正好看到Science上这篇Topic Models Vs. Unstructured Data的科普性质的文章,翻了一遍之后,再去PRML里看了一遍Graphic Models那一张,觉得对于LDA想解决的问题和方法了解了更清楚了。之后从search engine里搜到这篇文章,然后根据推荐读了一部分的Gibbs Sampling for the Uninitiated。之后忘了怎么又搜到了Mark Steyvers和Tom Griffiths合著的Probabilistic Topic Models,在某个周末往返北京的飞机上读完了,觉得基本上模型训练过程也明白了。再之后就是读了一下这个最简版的LDA Gibbs Sampling的实现,再回过头读了一下PLDA的源码,基本上算是对LDA有了个相对清楚的了解。
这样前前后后,也过去了三个月,其实不少时间都是浪费掉的,比如Blei的论文在没有任何相关知识的情况下一开始读了好几次,都没读完而且得到到信息也很有限,如果重新总结一下,我觉得对于我们这些门外汉程序员来说,想了解LDA大概需要这些知识:
基本上这样一圈下来,基本概念和算法实现都应该搞定了,当然,数学证明其实没那么容易就搞定,但是对于工程师来说,先把这些搞定就能干活了,这个步骤并不适合各位读博士发论文的同学们,但是这样先看看也比较容易对于这些数学问题的兴趣,不然,成天对这符号和数学公式,没有整块业余时间的我是觉得还是容易退缩放弃的。
发现作为工程师来说,还是看代码比较有感觉,看实际应用的实例比较有感觉,看来不能把大部分时间花在PRML上,还是要多对照着代码看。
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story. Facebook Messenger & Chatbot, Machine Learning & Big Data
生命如此短暂,掌握技艺却要如此长久"
Netflix Technology Blog,Netflix Recommendations: Beyond the 5 stars (Part 1),"Similarity is also an important source of personalization in our service. Streaming has not only changed the way our members interact with the service, but also the type of data available to use in our algorithms. Netflix launched an instant streaming service in 2007, one year after the Netflix Prize began. Knowing about your friends not only gives us another signal to use in our personalization algorithms, but it also allows for different rows that rely mostly on your social circle to generate recommendations. It is also used to generate rows of “adhoc genres” based on similarity to titles that a member has interacted with recently. Also, our focus on improving Netflix personalization had shifted to the next level by then. Now it is clear that the Netflix Prize objective, accurate prediction of a movie’s rating, is just one of the many components of an effective recommendation system that optimizes our members enjoyment. It is important to keep in mind that Netflix’ personalization is intended to handle a household that is likely to have different people with different tastes. by Xavier Amatriain and Justin Basilico (Personalization Science and Engineering)
In this two-part blog post, we will open the doors of one of the most valued Netflix assets: our recommendation system. One of the reasons our focus in the recommendation algorithms has changed is because Netflix as a whole has changed dramatically in the last few years."
Netflix Technology Blog,Netflix Recommendations: Beyond the 5 stars (Part 2),"But member ratings are only one of the many data sources we have and rating predictions are only part of our solution. One obvious way to approach this is to use the member’s predicted rating of each item as an adjunct to item popularity. An interesting follow-up question that we have faced is how to integrate our machine learning approaches into this data-driven A/B test culture at Netflix. Since the most common way of presenting recommended items is in some form of list, such as the various rows on Netflix, we need an appropriate ranking model that can use a wide variety of information to come up with an optimal ranking of the items for each of our members. At Netflix, we are fortunate to have many relevant data sources and smart people who can select optimal algorithms to turn data into product features. A/B tests let us try radical ideas or test many approaches at the same time, but the key advantage is that they allow our decisions to be data-driven. For the purposes of illustration, let us start with a very simple scoring approach by choosing our ranking function to be a linear combination of popularity and predicted rating. Thus, the goal becomes to find a personalized ranking function that is better than item popularity, so we can better satisfy members with varying tastes. Another possible answer involves formulating this as a machine learning problem: select positive and negative examples from your historical data and let a machine learning algorithm learn the weights that optimize your goal. This is an incomplete list of methods you should probably know about if you are working in machine learning for personalization:
Consumer Data Science
The abundance of source data, measurements and associated experiments allow us to operate a data-driven organization."
Wolf Garbe,1000x Faster Spelling Correction algorithm (2012) – Wolf Garbe – Medium,"Generate terms with an edit distance (deletes only) from the input term and search them in the dictionary. Symmetric Delete Spelling Correction (SymSpell) Generate terms with an edit distance (deletes only) from each dictionary term and add them together with the original term to the dictionary. If the edit distance is 0 the term is spelled correctly, if the edit distance is <=2 the dictionary term is used as spelling suggestion. For a word of length n, an alphabet size of a, an edit distance of 1, there will be just n deletions, for a total of n terms at search time. The number x of deletes for a single dictionary entry depends on the maximum edit distance: x=n for edit distance=1, x=n*(n-1)/2 for edit distance=2, x=n!/d!/(n-d)! Naive approachThe obvious way of doing this is to compute the edit distance from the query term to each dictionary term, before selecting the string(s) of minimum edit distance as spelling suggestion. Peter NorvigGenerate all possible terms with an edit distance (deletes + transposes + replaces + inserts) from the query term and search them in the dictionary. independent of the dictionary size (but depending on the average term length and maximum edit distance), because our index is based on a Hash Table which has an average search time complexity of O(1). Three ways to search for minimum edit distance in a dictionary:
1. For a word of length n, an alphabet size a, an edit distance d=1, there will be n deletions, n-1 transpositions, a*n alterations, and a*(n+1) insertions, for a total of 2n+2an+a-1 terms at search time."
Paul Christiano,Formalizing indirect normativity – AI Alignment,"We must make a distinction between two possible sources of moral value: it could be the case that a U-maximizer carries out simulations on physical hardware in order to better understand U, and these simulations have moral value, or it could be the case that the hypothetical emulations themselves have moral value. Finally, the values of the simulations in this process may diverge from the values of the original human models, for one reaosn or another. This possibility is particularly troubling in light of the incentives our scheme creates — anyone who can manipulate H’s behavior can have a significant effect on the future of our world, and so many may be motivated to create simulations of H.
A realistic U-maximizer will not be able to carry out the process described in the definition of U–in fact, this process probably requires immensely more computing resources than are available in the universe. We can imagine many ways in which this process can fail to work as intended–the original brain emulations may accurately model human behavior, the original subject may deviate from the intended plans, or simulated humans can make an error when interacting with their virtual environment which causes the process to get hijacked by some unintended dynamic. That is, the output of this process is defined in terms of what a particular human would do, in a situation which that human knows will never come to pass. This process could be used to acquire the power necessary to define a utility function in one of the above frameworks, or understanding value-preserving self-modification or machine intelligence may itself prove an important ingredient in formalizing what it is we value. Therefore a U-maximizer will be sensitive to the possible suffering of simulations it runs while trying to learn about U–as long as it believes that we may might be concerned about the simulations’ welfare, upon reflection, it can rely as much as possible on approaches which do not involve running simulations, which deprive simulations of the first-person experience of discomfort, or which estimate outcomes by running simulations in more pleasant circumstances. Moreover, the community of humans being simulated in our process has access to a simulation of whatever U-maximizer is operating under this uncertainty, and has a detailed understanding of that uncertainty. Human cognition does not reside in a physical system with sharp boundaries, and it is not clear how you would define or use a simulation of the “input-output” behavior of such an object. In the second case, a U-maximizer in our world may have little ability to influence the welfare of hypothetical simulations invoked in the definition of U."
Robbie Tilton,Emotional Computing – Robbie Tilton – Medium,"If an increased computational IQ can allow a human to computer relationship to feel more like a human to human interaction, what would the advancement of computational EQ bring us? (A) was informed that (B) was an advanced computer chat-bot with the capacity to feel, understand, learn, and speak like a human. Investigating the human to computer relationship through reverse engineering the Turing test
Humans are getting closer to creating a computer with the ability to feel and think. In an experiment between a human and a human disguised as a computer, the Turing test is reverse engineered in order to understand the role computers will play as they become more adept to the processes of the human mind. Advances in computer IQ have been astonishing and have proved that machines are capable of answering difficult questions accurately, are able to hold a conversation with human-like understanding, and allow for emotional connections between a human and machine. (B) participants were asked if they would like to pursue a friendship with the person they chatted with. This also hits directly on Jefferson Lister’s quote, “Not until a machine can write a sonnet or compose a
concerto because of thoughts and emotions felt, and not by the chance fall of symbols, could we agree that machine equals brain-that is, not only write it but know that it had written it.”
Participants were given a chat-room simulation between two participants (A) a human interrogator and (B) a human disguised as a computer. The results also imply that humans aren’t really sure what they want out of Artificial Intelligence in the future and that we are not certain that an Affective computer would even enjoy a users company and/or conversation. In this paper we will uncover what it means for humans to interact with machines of greater intelligence and attempt to predict the future of human to computer interactions. (A) participants were asked if they felt (B)’s Artificial Intelligence could improve their relationship to computers if integrated in their daily products."
Netflix Technology Blog,System Architectures for Personalization and Recommendation,"However, computation can be done offline, nearline, or online. Batch computation of results is the offline computation process defined above in which we use existing models and corresponding input data to compute results that will be used at a later time either for subsequent online processing or direct presentation to the user. Online computation can respond quickly to events and use the most recent data. Much of the computation we need to do when running personalization machine learning algorithms can be done offline. As mentioned above, our algorithmic results can be computed either online in real-time, offline in batch, or nearline in between. Online computation can respond better to recent events and user interaction, but has to respond to requests in real-time. Regardless of whether we are doing an online or offline computation, we need to think about how an algorithm will handle three kinds of inputs: models, data, and signals. The nearline computation is done in response to user events so that the system can be more responsive between requests. At Netflix, we store offline and intermediate results in various repositories to be later consumed at request time: the primary data stores we use are Cassandra, EVCache, and MySQL. Of course, we can think of using a combination of both where the bulk of the recommendations are computed offline and we add some freshness by post-processing the lists with online algorithms that use real-time signals."
James Faghmous ,New to Machine Learning? Avoid these three mistakes,"Data leakage occurs when the data you are using to learn a hypothesis happens to have the information you are trying to predict. Machine learning is a field of computer science where algorithms improve their performance at a certain task as more data are observed.To do so, algorithms select a hypothesis that best explains the data at hand with the hope that the hypothesis would generalize to future (unseen) data. The most basic form of data leakage would be to use the same data that we want to predict as input to our model (e.g. As the house’s size increases, so does its price in linear increments.” Now using this hypothesis, I can predict the price of an unseen datapoint based on its size. As the dimensions of the data increase, the hypotheses that explain the data become more complex.However, given that we are using a finite sample of observations to learn our hypothesis, finding an adequate hypothesis that generalizes to unseen data is nontrivial. If your model’s performance is poor on the newly collected data it may be a sign of data leakage. use the price of a house to predict the price of the same house). One way to spot data leakage is if you are doing very poorly on unseen independent data. The hypothesis selected by the algorithm (the blue curve) to explain the data is so complex that it fits through every single data point. The razor was violated when the hypothesis or model selected to describe the relationship between environmental data and seasonal hurricane counts was generated using a four-layer neural network."
Datafiniti,Classifying Websites with Neural Networks – Knowledge from Data: The Datafiniti Blog,"While it’s easy for you or me to realize that the above web page is selling some jeans, a computer would have a hard time making the distinction from the above page from either of the following web pages:
Or
Both of these pages share many similarities to the actual product page, but also have many key differences. Neural networks provide a structure for using the output of one set of input data to adjust A and B to the most likely best values for the next set of input data. We feel pretty good about our ability to classify and extract product data. In order to introduce more complex relationships in our data, we can introduce “hidden” layers in this model, which would end up looking something like:
For a more detailed explanation of neural networks, you can check out the following links:
In our product page classifier algorithm, we setup a neural network with 1 input layer with 27 nodes, 1 hidden layer with 25 nodes, and 1 output layer with 3 output nodes. In the meantime, we’re also working on classifying other types of pages, such as business data, company team pages, event data, and more.As we roll-out these classifiers and data extractors, we’re including each one in our crawl of the entire Internet. Neural networks are better-suited toward classification problems, and extracting data from a web page is a different type of problem. We also want to pull out the actual structured data! Our input layer modeled several features, including:
Our output layer had the following:
Our algorithm for the neural network took the following steps:
The ultimate output is two sets of input layers (T1 and T2), that we can use in a matrix equation to predict page type for any given web page. We don’t actually use neural networks for doing this. In order to see if we could use diameter and heights to determine poisonous-ness, we could set up the following equation:
A * (diameter) + B * (height) = 0 or 1 for not-poisonous / poisonous
We would then try various combinations of A and B for all possible diameters and heights until we found a combination that correctly determined poisonous-ness for as many mushrooms as possible."
Arjan Haring 🔮🔨,Reinventing Social Sciences in the Era of Big Data – I love experiments – Medium,"A central hypothesis in my work is that in order to advance our quantitative understanding of social interaction, we cannot get by with noisy, incomplete big data: We need good data. Now that we know about all the communication channels, we can begin to understand what kind of things one may learn from a single channel. large cell phone data sets or Facebook, when that’s the only data available. Although we’ve made amazing progress in network science, for example, it’s still a fact that our fundamental understanding of dynamic/multi-channel networks is still in its infancy, there aren’t a lot of easily interpretable models that really explain the underlying networks. My (humble) research goal is to reinvent social sciences in the age of big data. We like to call this type of data ‘Deep Data’: A densely connected group of participants (all the links), observations across many communication channels, high frequency observations (minute-by-minute scale), but with long observation windows (years of collection), and with behavioral data supplemented by classic questionnaires, as well as the possibility of running intervention experiments. Or, as a society we may be able to increase spread of things we support, such as tolerance, good exercise habits, etc ... and similarly, we can use an understanding influence in social systems to inhibit negative behavior such as intolerance, smoking, etc. Lots of network science is still about unweighted, undirected static networks; we are already using this dataset to create better models for dynamic, multiplex networks. My heart is still with the network science. We can capture detailed interaction patterns, such as face-to-face (via bluetooth), social network data (e.g."
Eventbrite,Multi-Index Locality Sensitive Hashing for Fun and Profit,"To compare every M message to every other message we first insert its bit vector into an LshTable (an O(K) operation, K is constant). Upon the lookup() of a bit vector, we once again split it into chunks and for each chunk look up the associated hash table for a chunk that’s close (zero or one bits off). More pseudo-code:
When comparing two messages, if the hashes are equal in the same position in the minHash vector, then the bits in the equivalent position after bit sampling should be also equal. if two bit vectors are within a certain radius of bits, then they are similar. During add() of a bit vector, we split the bit vector into K chunks. So, we can emulate the Jaccard similarity of two minHashes by counting the equal bits in the two bit vectors (aka. So then, when given a set of M messages, we simply compute the similarity of a message to every other message. For each of these chunks, we add the original bit vector into the associated hash table under the index chunk. The unequal bits can be found by taking the bit-wise XOR of the two bit vectors. So, we’ve detailed how to find similar messages in a very large set of messages efficiently."
Akash Shende,Color Based Object Segmentation – Akash Shende – Medium,"It’s not able to create mask for complete t-shirt, also it mask eyes which aren’t blue. To find blue t-shirt in given image, I used OpenCV’s inRange method: Which takes color (or greyscale) image, lower & higher range value as its parameter and returns binary image, where pixel value set to 0 when input pixel doesn’t fall in specified range, otherwise pixel value set to 1. Create binary mask that separates blue T-shirt from rest. Now the inRange method able to mask only t-shirt. Following function converts a pixel at X, Y location into its corresponding normalized rgb pixel. Let R,G,B are pixel values, then normalized pixel g(x,y) is calculated as,divide the individual color component by sum of all color components and multiply by 255. This function accepts 8 bit RGB image matrix of size 800x600 and returns normalized RGB image. Normalization of color plane reduces variation in light by averaging pixel values, thus it removes highlighted and shadowed region and make image flatten. With the help of this function and after determining range values, I ended up with this mask. Thus, it creates different shades of blue and results into partial segmentation."
Hrishikesh Huilgolkar,Traveling santa Problem — An incompetent algorist’s attempt,"But this required the problem to be in Distance Matrix format.Then I found a major problem. scipy.spatial.distance.squareform() can create a square distance matrix from compressed matrix but that would waste a lot of ram.So I created a custom function which divided compressed matrix by rows so LKH can read it. (Correct me if I am wrong)
Solution:A simple solution was to divide the map in manageable chunks.I used scipy’s distance matrix creation function scipy.spatial.distance.pdist() It creates distance matrix from coordinates.The matrix created by pdist is in compressed form (a flattened matrix of upper diagonal elements. The problem was that the first path was so good that the second path struggled to find good path. I still had to figure out how to make it find the second path.A simple idea to get 2 disjoint paths is to generate first path and then make weight of those edges infinite and run LKH on the problem again. If I combine 18 squares of first path and 18 squares of second path, I will have a path whose distance will be approximately average of the two paths.I tried this trick and used different combinations of the two paths squares and got the best score of 6,807,498
For new path1, select blue squres from old path1 and grey square from old path2
Use remaining squares for new path2.Remove cross lines
My squares were joined in a zigzag manner. Through trial and error, I found that on my laptop with 4 GB ram, a 6 by 6 grid in the above format was manageable for both creating distance matrix and for LKH. But after dividing the problem in grids, time taken by distance calculation was manageable so I stuck with euclidean distance. Solving this problem would require expertise on data structures and some good familiarity with TSP problems and its many heuristic algorithms. I ran LKH on resulting distance matrices and joined the individual solutions."
Adam Geitgey,Machine Learning is Fun! – Adam Geitgey – Medium,"If we could just figure out the perfect weights to use that work for every house, our function could predict house prices! Machine learning is the idea that there are generic algorithms that can tell you something interesting about a set of data without you having to write any custom code specific to the problem. Once you start seeing how easily machine learning techniques can be applied to problems that seem really hard (like handwriting recognition), you start to get the feeling that you could use machine learning to solve any problem and get an answer as long as you have enough data. To build your app, you feed your training data about each house into your machine learning algorithm. Here’s one way:
First, write a simple equation that represents Step #2 above:
Now let’s re-write exactly the same equation, but using a bunch of machine learning math jargon (that you can ignore for now):
This equation represents how wrong our price estimating function is for the weights we currently have set. A dumb way to figure out the best weights would be something like this:
Start with each weight set to 1.0:
Run every house you know about through your function and see how far off the function is at guessing the correct price for each house:
For example, if the first house really sold for $250,000, but your function guessed it sold for $178,000, you are off by $72,000 for that single house. You can think of machine learning algorithms as falling into one of two main categories — supervised learning and unsupervised learning. Maybe a better definition for “learning” in this case is “figuring out an equation to solve a specific problem based on some example data”. But there’s a problem — you can glance at a house and have a pretty good idea of what a house is worth, but your trainees don’t have your experience so they don’t know how to price their houses. But it’s important to remember that machine learning only works if the problem is actually solvable with the data that you have."
Shivon Zilis,The Current State of Machine Intelligence – Shivon Zilis – Medium,"Some have used this term interchangeably with machine learning and artificial intelligence, but I want to focus on the intelligence methods rather than data, storage, and computation pieces of the puzzle for this landscape (though of course data technologies enable machine intelligence). I mean “machine intelligence” as a unifying term for what others call machine learning and artificial intelligence. Which companies are on the landscape? All about machine intelligence for good. I would have preferred to avoid a different label but when I tried either “artificial intelligence” or “machine learning” both proved to too narrow: when I called it “artificial intelligence” too many people were distracted by whether certain companies were “true AI,” and when I called it “machine learning,” many thought I wasn’t doing justice to the more “AI-esque” like the various flavors of deep learning. Now we’re seeing a similar explosion of companies calling themselves artificial intelligence, machine learning, or somesuch — collectively I call these “machine intelligence” (I’ll get into the definitions in a second). The last wave of technology companies to IPO didn’t have demos that most of us would watch, so why should machine intelligence companies? We’re also investors in a few other machine intelligence companies that aren’t focusing on areas that were a fit for this landscape, so we left them off. (The 2016 Machine Intelligence landscape and post can be found here)
I spent the last three months learning about every artificial intelligence, machine learning, or data related startup I could find — my current list has 2,529 of them to be exact. ☺
I tried to pick companies that used machine intelligence methods as a defining part of their technology."
AirbnbEng,Architecting a Machine Learning System for Risk – Airbnb Engineering & Data Science – Medium,"Because feature-transformation, model building, model validation, deployment and testing are all carried out in a single script, a data scientist or engineer is able to quickly iterate on a model based on new features or more recent data, and then rapidly deploy the new model into production. We then use our transformed features to train and cross-validate the model using our favorite PMML-compatible machine learning library, and upload the PMML model to Openscoring. http://airbnb.io
Creative engineers and data scientists building a world where you can belong anywhere. A schematic of our model-building pipeline using PMML is illustrated above. In keeping with our service-oriented architecture, we built a separate fraud prediction service to handle deriving all the features for a particular model. This service can then calculate all the features for the “reservation creation” model, and send these features to our Openscoring service, which is described in more detail below. We automate the process of feature extraction based on a set of rules encoded in the names and types of variables in the features json; thus, new features can be incorporated into the model pipeline with no changes to the existing code. Smaller companies may only have a few ML models in production and a small number of analysts, and can take time to manually curate data and train the model in many non-standardized steps. Scikit-learn is a Python package that supports many standard machine learning models, and includes helpful utilities for validating models and performing feature transformations. Creative engineers and data scientists building a world where you can belong anywhere."
Yingjie Miao ,From word2vec to doc2vec: an approach driven by Chinese restaurant process,"It’s natural to think that if we can first group words into clusters, words like “chicken”, “pepper” may stay in one cluster, along with other clusters of “junk” words. ), and vectors in clusters. Now let’s take the cluster sum vector (which is the sum of all vectors from this cluster), and test if it really preserves semantic. Now we find the cluster C whose cluster sum is most similar to current vector. Based on similarity among cluster sum vectors? For example, for a chicken recipe document, the clusters look like this:
Apparently, the first cluster is most relevant. If we can identify the “relevant” clusters, and only summing up word vectors from relevant clusters, we should have a good doc vector. If this is a chicken recipe document, I shouldn’t even consider words like “definitely”, “use”, “my” in the summation. 2) what is the minimal set of words that can reconstruct cluster sum vector (in the sense of cosine similarity)? Basic idea is to use CRP to drive a clustering process and summing word vectors in the right cluster."
Pinterest Engineering,Building a smarter home feed – Pinterest Engineering – Medium,"In addition to providing high availability of the home feed, the smart feed service is responsible for combining new Pins returned by the content generator with those that previously appeared in the home feed. The content generator assembles available Pins into chunks for consumption by the user as part of their home feed. We can separate these into the chunk returned by the content generator and the materialized feed managed by the smart feed service. The smart feed service relies on the content generator to provide new Pins. To the materialized Pins we add the Pins from the content generator in the chunk. When a user accesses the home feed, we ask the content generator for new Pins since their last visit. Distinct from the smart feed worker, the smart feed content generator is concerned primarily with defining what “new” means in the context of a home feed. The smart feed worker is the first to process Pins and has two primary responsibilities — to accept incoming Pins and assign some score proportional to their quality or value to the receiving user, and to remember these scored Pins in some storage for later consumption. In the event the content generator encounters an exception while generating a chunk, or if it simply takes too long to produce one, the smart feed service will return the content contained in the materialized feed. The core feature of the smart feed architecture is its separation of available, but unseen, content and content that’s already been presented to the user."
Nikhil Dandekar,What makes a good data scientist/engineer? – Towards Data Science,"Some examples of relevance products are:
These folks need to be strong at data science and engineering to be successful. I will talk about the qualities needed to be a good data scientist-engineer who ships relevance products to users. The term data scientist has been used lately to describe a wide variety of skills & roles. Some places call these folks as Machine Learning engineers since most of the work they do involves Machine Learning. But beyond that, there are a bunch of not-so-obvious skills that you can’t learn from a book. You obviously need to have (or be able to learn quickly) the required “book” knowledge. Engineering Manager doing Machine Learning @ Google. More generally, I feel relevance engineer is a good term to describe them. In this post I will focus on a particular flavor of data scientist. Here are some of those, in no particular order:
This list is by no means exhaustive, but does capture some of the qualities of the smartest folks I have worked with."
Jeff Smith,Modeling Madly – Data Engineering – Medium,"Which leads me to my next point...
One of the best things about working in data science is all of the really smart people. But this is machine learning, and sometimes our data is just big. But, of course, the corollary is that one of the worst things about working in data science is all of the really smart people. If you’re really lucky, you might be using Spark and not Hadoop, in which case it might not take hours to learn your model. This is a great characteristic to build into the plan of a hack but one that simply does not apply to a machine learning hack. Taking on a challenge like building a new model in a hackathon is also a great learning experience, especially if you get to work as part of a strong team. Even with a well-oiled workflow, some of those tasks can take all of the time your average one-day hackathon is scheduled for. When you’re doing a more traditional web app hack at a hackathon, you can almost run out of time and still come up with something pretty good as long as you get that last bug fixed before the demo. While I‘ve been working on data science and machine learning systems for a while, I’ve found that trying to do so under extreme constraints can be a distinctly different experience. We’re hiring all sorts of smart people to build systems for machine learning and more."
Chris Jagers,Explaining the Wolfram Language – Chris Jagers – Medium,"With Wolfram Language, that hard stuff becomes easy. But with Wolfram, many of those data sets are already built in. Stephen Wolfram calls this a knowledge-based language because it has smart-objects built in that can be computed. Easy. Easy. Even though the Wolfram website has signaled intent to make it more broadly deployable within commercial services, I don’t think this is the proper way to use the language. Let’s say we want our system to determine the difference between poetry and prose. The Wolfram Community is full of very smart people using the language for research and exploration. While Stephen Wolfram would love for his language to be used within commercial products, I think he resents having to play nice with lower level languages. Mathematica is still the most powerful and polished way to access the Wolfram Language."
John Wittenauer,"Machine Learning Exercises In Python, Part 1 – John Wittenauer – Medium","Our solution looks like and optimal linear model of the data set. plot the cost as a function of the model parameters for every possible value of the parameters) you would see that it looks like a “bowl” shape with a “basin” representing the optimal solution. The cost function evaluates the quality of our model by calculating the error between our model’s prediction for a data point, using the model parameters, and the actual data point. If you’re familiar with linear algebra, you may be aware that there’s another way to find the optimal parameters for a linear model called the “normal equation” which basically solves the problem at once using a series of matrix calculations. Fortunately this data set only has one dependent variable, so we can toss it in a scatter plot to get a better idea of what it looks like. One quick way to evaluate just how good our regression model is might be to look at the total error of our new solution on the data set:
4.5159555030789118
That’s certainly a lot better than 32, but it’s not a very intuitive way to look at it. We can use pandas to load the data into a data frame and display the first few rows using the “head” function. There are lots of different types and variances of linear regression that are outside the scope of this discussion so I won’t go into that here, but to put it simply — we’re trying to create a *linear model* of the data X, using some number of parameters theta, that describes the variance of the data such that given a new data point that’s not in X, we could accurately predict what the outcome y would be without actually knowing what y is. If you look closely at the gradient descent function, it has parameters called alpha and iters. We now have a parameter vector descibing what we believe is the optimal linear model for our data set."
Pinterest Engineering,Building the interests platform – Pinterest Engineering – Medium,"Figure 2: System components
Raw input to the system includes existing data about Pins, boards, Pinners, and search queries, as well as explicit human curation signals about interests. Specifically, it’s responsible for producing high quality data on interests, interest relationships, and their association with Pins, boards, and Pinners. With the Interests dictionary, we can associate Pins, boards, and Pinners with representative interests. Ningning Hu | Pinterest engineer, Discovery
The core value of Pinterest is to help people find the things they care about, by connecting them to Pins and people that relate to their interests. Interests feeds provide Pinners with a continuous feed of Pins that are highly related. Figure 4: Computing related interests
The quality of the related interests is surprisingly high given the simplicity of the algorithm. Figure 3: Exploring interests
In order to match interests to Pinners, we need to aggregate all the information related with a person’s interests. One of the initial ways we began showing people related content was through related Pins. With this data, we’re able to construct a continuously evolving interest dictionary, which provides the foundation to support other key components, such as interest feeds, interest recommendations, and related interests. We’ll continue testing different ways of exposing a person’s interests and related content, based on implicit signals, as well as explicit signals (such as the ability to create custom categories of interests)."
Christopher Nguyen,Algorithms of the Mind – Deep Learning 101 – Medium,"So what does all this have to do with machine learning? The technology of machine learning is giving us new ways to think about the science of human thought ... and imagination. These layers act as a kind of associative memory, mapping back-and-forth from image and concept, from concept to image, all in one neural network. Language does indeed influence our perception of the world, in the same way that labels in supervised machine learning influence the model’s ability to discriminate among categories. With machine learning, we also observe something similar. The connection to machine learning and computer science is more recent, especially with the advances in big data and deep learning. By definition, these models are trained to discriminate much more effectively between categories that have provided labels, than between other possible categories for which we have not provided labels. When fed with huge amounts of text, images, or audio data, the latest deep learning architectures are demonstrating near or even better-than-human performance in language translation, image classification, and speech recognition. And if we recognized them as such, perhaps Sapir-Whorf would not be such a controversy, and more of a reflection of supervised and unsupervised human learning. We are seeing, quite literally, a machine imagining an image of the concept of “8”."
Per Harald Borgen,Machine Learning in a Week – Learning New Stuff – Medium,"Friday, I continued working on the Kaggle tutorials, and also started Udacity’s Intro to Machine Learning. I recommend you trying out Kaggle after having a little bit of a theoretical and practical understanding of machine learning. Before my machine learning week, I had been reading about the subject for a while, and had gone through half of Andrew Ng’s course on Coursera and a few other theoretical courses. The problem we decided to solve was the following:
I played around with the dataset, spent a few hours cleaning up the data, and used the Scikit Learn map to find a suitable algorithm for the problem. But it’s also more practical, as it teaches you Scikit Learn, which is a whole lot easier to apply to the real world than writing algorithms from the ground up in Octave, as you do in the Coursera course. As you go along, you realize that fetching and cleaning up the data can be much more time consuming than doing the actually machine learning. This module gives you a wealth of algorithms to choose from, reducing the actual machine learning to a few lines of code. In that case, the Udacity’s Intro to Machine Learning might be a better place to start. I started off the week by looking for video tutorials which involved Scikit Learn. Kaggle is a platform for machine learning competitions, where you can submit solutions to problems released by companies or organizations ."
Ahmed El Deeb,What to do with “small” data? – Rants on Machine Learning – Medium,"From an initial set of possible models, which is the most appropriate model to fit our data? By Ahmed El Deeb
Many technology companies now have teams of smart data-scientists, versed in big-data infrastructure tools and machine learning algorithms, but every now and then, a data set with very few data points turns up and none of these algorithms seem to be working properly anymore. Most data science, relevance, and machine learning activities in technology companies have been focused around “Big Data” and scenarios with huge data sets. 8- Do use Model Averaging
Model averaging has similar effects to regularization is that it reduces variance and enhances generalization, but it is a generic technique that can be used with any type of models or even with heterogeneous sets of models. When you don’t have so many data points to begin with, you need to start from a fairly small set of possible hypotheses (e.g. Cleaning up your data could be crucial here to get sensible models. 9- Try Bayesian Modeling and Model Averaging
Again, not a favorite technique of mine, but Bayesian inference may be well suited for dealing with smaller data sets, especially if you can use domain expertise to construct sensible priors. 5- Do clean up your data
With small data sets, noise and outliers are especially troublesome. In a way, each data point we use for fitting down-votes all models that make it unlikely, or up-vote models that agree with it. Small data sets arise is several situations:
Problems of small-data are numerous, but mainly revolve around high variance:
1- Hire a statistician
I’m not kidding!"
Matt Fogel,The 7 Best Data Science and Machine Learning Podcasts,"Website | iTunes
Data Stories is a little more focused on data visualization than data science, but there is often some interesting overlap between the topics. Website | iTunes
This podcast features Ben Lorica, O’Reilly Media’s Chief Data Scientist speaking with other experts about timely big data and data science topics. Website | iTunes
Each week, hosts Chris Albon and Jonathon Morgan, both experienced technologists and data scientists, talk about the latest news in data science over drinks. I’ve listened to a bunch of machine learning and data science podcasts in the last few months, so I thought I’d share my favorites:
A great starting point on some of the basics of data science and machine learning. Data science and machine learning have long been interests of mine, but now that I’m working on Fuzzy.ai and trying to make AI and machine learning accessible to all developers, I need to keep on top of all the news in both fields. Website | iTunes
Hosted by Katie Malone and Ben Jaffe of online education startup Udacity, this weekly podcast covers diverse topics in data science and machine learning: teaching specific concepts like Hidden Markov Models and how they apply to real-world problems and datasets. Listening to Partially Derivative is a great way to keep up on the latest data news. Website | iTunes
Billing itself as “A Gentle Introduction to Artificial Intelligence and Machine Learning”, this podcast can still get quite technical and complex, covering topics like: “How to Reason About Uncertain Events using Fuzzy Set Theory and Fuzzy Measure Theory” and “How to Represent Knowledge using Logical Rules”. Every other week, hosts Katherine Gorman and Ryan Adams speak with a guest about their work, and news stories related to machine learning. Website | iTunes
The newest podcasts on this list, with 8 episodes released as of this writing."
Illia Polosukhin,TensorFlow Tutorial— Part 1 – Illia Polosukhin – Medium,"First, make sure you have installed TensorFlow and Scikit Learn with few helpful libs, including Scikit Flow that is simplifying a lot of work with TensorFlow:
You can get dataset and the code from http://github.com/ilblackdragon/tf_examples
Quick look at the data (use iPython or iPython notebook for ease of interactive exploration):
Let’s test how we can predict Survived class, based on float variables in Scikit Learn:
We separate dataset into features and target, fill in N/A in the data with zeros and build a logistic regression. UPD (April 20, 2016): Scikit Flow has been merged into TensorFlow since version 0.8 and now called TensorFlow Learn or tf.learn. Now using tf.learn (previously Scikit Flow):
Congratulations, you just built your first TensorFlow model! TF.Learn is a library that wraps a lot of new APIs by TensorFlow with nice and familiar Scikit Learn API. Looking under the hood of TF.Learn, we just used three parts:
Even as you get more familiar with TensorFlow, pieces of Scikit Flow will be useful (like graph_actions and layers and host of other ops and tools). A reasonable question, why as a Data Scientist, who already has a number of tools in your toolbox (R, Scikit Learn, etc), you care about yet another framework? Part 2 — Deep Neural Networks, Custom TensorFlow models with Scikit Flow and Digit recognition with Convolutional Networks. Google released a machine learning framework called TensorFlow and it’s taking the world by storm. TensorFlow is all about a building and executing graph. Predicting on the training data gives us some measure of accuracy (of cause it doesn’t properly evaluate the model quality and test dataset should be used, but for simplicity we will look at train only for now)."
Ahmed El Deeb,The Unreasonable Effectiveness of Random Forests – Rants on Machine Learning – Medium,"It’s very common for machine learning practitioners to have favorite algorithms. Machine Learning practitioner and hobbyist. Rants about machine learning and its future evaluating several supervised learning algorithms on many different datasets. My favorite out-of-the-box algorithm is (as you might have guessed) the Random Forest, and it’s the second modeling technique I typically try on any given data set (after a linear model). This beautiful visualization from scikit-learn illustrates the modelling capacity of a decision forest:
Here’s a paper by Leo Breiman, the inventor of the algorithms describing random forests. And even for a given problem and a given dataset, any single model will likely be beaten by an ensemble of diverse models trained by diverse algorithms anyway. It’s a bit irrational, since no algorithm strictly dominates in all applications, the performance of ML algorithms varies wildly depending on the application and the dimensionality of the dataset. Here’s another amazing paper by Rich Caruana et al. Some like SVMs for the elegance of their formulation or the quality of the available implementations, some like decision rules for their simplicity and interpretability, and some are crazy about neural networks for their flexibility."
Christophe Bourguignat,6 Tricks I Learned From The OTTO Kaggle Challenge – Christophe Bourguignat – Medium,"At the beginning of the competition, it appeared quickly that — once again — Gradient Boosting Trees was one of the best performing algorithm, provided that you find the right hyper parameters. On the scikit-learn implementation, most important hyper parameters are learning_rate (the shrinkage parameter), n_estimators (the number of boosting stages), and max_depth (limits the number of nodes in the tree, the best value depends on the interaction of the input variables). XGBoost is a Gradient Boosting implementation heavily used by kagglers, and I now understand why. I also discovered that two other parameters were crucial for this competition. We added to an initial set of 93 features, new features being the predictions of N different classifiers (Random Forest, GBM, Neural Networks, ...). One of the secret of the competition was to run several times the same algorithm, with random selection of observations and features, and take the average of the output. We heavily used stacking. Here are a few things I learned from the OTTO Group Kaggle competition. I must admit I never paid attention on it before this challenge : namely subsample (the fraction of samples to be used for fitting the individual base learners), and max_features (the number of features to consider when looking for the best split). We tested two tricks :
This is one of the great functionalities of the last scikit-learn version (0.16)."
I'Boss Potiwarakorn,"Machine Learning เรียนอะไร, รู้ไปทําไม – O v e r f i t t e d – Medium","แน่นอนว่า Machine Learning คือคําตอบ
สมองของมนุษย์นั้นมีความสามารถที่น่าทึ่งมากมาย เช่น การตระหนักรู้ อารมณ์ความรู้สึก ความทรงจํา ความสามารถในการควบคุมร่างกาย รวมถึงประสาทสัมผัสทั้งห้าที่ทําให้เรามีความสามารถในการรับรู้ แต่ก็มีปัญหาบางอย่างที่ซับซ้อน และไม่เหมาะที่จะแก้ปัญหาโดยการใช้สมองของมนุษย์เพียงอย่างเดียว
เมื่อต้องเขียนโปรแกรมที่จัดการกับข้อมูลจํานวนมาก และมีรูปแบบที่แตกต่างกันออกไป เป็นเรื่องยากที่เราจะทําความเข้าใจข้อมูลและเขียนโปรแกรมที่จะตอบสนองต่อมัน เมื่อมีข้อมูลเข้ามาเพิ่มและมีลักษณะที่ต่างไปอีกก็เหมือนกับ requirement เปลี่ยนตลอดเวลา เราก็ต้องวิเคราะห์ข้อมูลใหม่และแก้โปรแกรมของเราเรื่อยๆซึ่งลําบากมาก
Arthur Samuel หนึ่งในผู้บุกเบิก Computer Gaming, Artificial Intelligence และ Machine Learning ชาวอเมริกัน ได้นิยาม Machine Learning เอาไว้ว่า เป็น “การศึกษาเกี่ยวกับการทําให้คอมพิวเตอร์มีความสามารถที่จะเรียนรู้โดยที่ไม่ต้องเขียนโปรแกรมลงไปตรงๆ”
กล่าวคือ Machine Learning นั้น ไม่ได้กําหนดลงไปในโปรแกรมว่า สําหรับลักษณะ A, B ใดๆ หากข้อมูลมีลักษณะแบบ A ต้องทําอย่างไร แบบ B ต้องทําอย่างไร แต่เป็นโปรแกรมที่ทําความเข้าใจความสัมพันธ์ของข้อมูล แล้วสร้างวิธีการตอบสนองต่อข้อมูลขึ้นมาเอง
ในเมื่อโปรแกรมสามารถเปลี่ยนแปลงวิธีการตอบสนองต่อข้อมูลได้ด้วยตัวเอง เราจึงไม่จําเป็นต้องคอยวิเคราะห์ข้อมูลและแก้โปรแกรมทุกครั้งที่มีข้อมูลใหม่เข้ามาอีกต่อไป
ตัวผมเองเคยสงสัยว่า Machine Learning, Artificial Intelligence และData Mining มันต่างกันยังไง รู้สึกว่ามันก็เป็นเรื่องที่คล้ายกันมากๆ แต่แล้วผมก็พบความจริงว่า จริงๆแล้ว ทั้ง AI (Artificial Intelligence) และ Data Mining นั้นนํา Machine Learning ไปใช้ สิ่งที่ต่างกันก็คือเป้าหมาย
AI นั้นโฟกัสที่การสร้าง Intelligent Agent หรือตัวตนที่มีความคิดขึ้นมา ซึ่งไม่จําเป็นที่จะต้องใช้ Machine Learning ก็ได้ ถึงแม้จะใช้เพียงแค่การ Search หากสามารถตอบสนองได้อย่างชาญฉลาด ก็สามารถเรียกว่าเป็น AI ได้
แต่ไม่จําเป็น ไม่ได้แปลว่าไม่ได้นําไปใช้ ในทางกลับกัน Machine Learning ถูกนําไปใช้ประโยชน์ใน AI เยอะมากๆ โดยถูกใช้เพื่อที่จะสร้างความรู้ใหม่ๆ และนําไปสู่การตอบสนองต่อเหตุการณ์ที่ต่างออกไปจากที่กําหนดไว้แต่แรก
ส่วน Data Mining นั้น เป็นขั้นตอนการวิเคราะห์ใน knowledge discovery หรือการค้นหาความรู้ โดยจะเปลี่ยนจากข้อมูลดิบ(data)ให้เป็นข้อมูลที่ทําความเข้าใจได้(information) เพื่อที่จะนําไปใช้ต่อในอนาคต
Data Mining ใช้วิธีการของทั้ง AI, Machine Learning, สถิติ และ Database System ในการได้มาซึ่งข้อมูลเชิงลึก หรือ insight
โดยสรุปแล้ว ทั้ง 3 ศาสตร์นั้นมีความเกี่ยวข้องกันอย่างมาก และต่างก็นําวิธีการของกันและกันไปใช้ จึงไม่แปลกที่จะรู้สึกว่ามันดูคล้ายๆกัน เพียงแต่เป้าหมายของมันต่างกัน ทําให้วิธีการนั้นไม่เหมือนกันซะทีเดียว
Machine Learning Algorithm นั้น โดยพื้นฐานแล้วจะแบ่งออกได้เป็น 2 ประเภทคือ supervised learning กับ unsupervised learning
supervised learning คือ การเรียนรู้ที่ได้รับคําแนะนํา
สมมติว่าเราเกิดเสียความทรงจํา ไม่สามารถแยกแยะ แอปเปิ้ล มะม่วง และส้มออกจากกันได้ คุณหมอผู้น่ารักจึงเอา แอปเปิ้ล มะม่วง และส้ม มาให้ดู ผลไม้ทั้งหมดที่คุณหมอเอามาให้ดูนี้เรียกว่า training data คือข้อมูลที่นํามาใช้ในการฝึกสอน
คุณหมอเริ่มจากการนําแอปเปิ้ลหลากหลายแบบมาให้ดู และบอกว่า “นี่คือแอปเปิ้ล” นี่เป็นการให้ label หรือป้ายที่แปะบอกว่าข้อมูลที่ได้มานี้คืออะไร
เมื่อเราได้เห็นแอปเปิ้ลก็จะพบว่า แอปเปิ้ลนั้นมีสีแดง หรือสีเขียว รูปทรงของแอปเปิ้ลนั้นหากผ่าครึ่งจะมีลักษณะคล้ายผีเสื้อ สิ่งเหล่านี้เรียกว่า feature หรือคุณสมบัติของข้อมูล
หลังจากนั้นคุณหมอก็นํามะม่วงและส้มมาให้ดู และพบว่า มะม่วงมีสีเขียวหรือเหลือง รูปทรงค่อนข้างยาว ส่วนส้มมีสีส้ม และมีรูปทรงเป็นทรงรี
เมื่อได้ข้อมูลมากเพียงพอ ก็จะเริ่มแยกแยะ แอปเปิ้ล มะม่วง และส้ม ออกจากกันได้ ต่อมา หากเจอส้มเปลือกสีเขียวก็อาจจะเดาได้ว่าเป็นส้ม เพราะรูปทรงของมัน
นี่เป็นตัวอย่างหนึ่งของ classification หรือการจัดหมวดหมู่ เป็น supervised learning แบบหนึ่งที่ใช้กับข้อมูลที่ไม่ต่อเนื่อง (discrete)
regression เป็นการวิเคราะห์สําหรับข้อมูลที่ต่อเนื่อง (continuous)
ภาพด้านบนแสดงถึง linear regression หรือ line of best fit ซึ่งเป็น regression แบบหนึ่ง
จะเห็นได้ว่าเรามี training data อยู่ ซึ่งก็ไม่ได้เรียงตัวกันเป็นเส้นตรง แต่ก็พอจะเห็นรูปแบบและแนวโน้มของข้อมูล หากเราลากเส้นโดยพยายามให้ผลรวมของระยะห่างจาก training data ซึ่งเป็นความคลาดเคลื่อนน้อยที่สุด เราก็จะได้ model ที่พอจะทํานายค่า y ต่อๆไปได้
George E. P. Box นักสถิติศาสตร์กล่าวไว้ว่า
อย่างที่เห็นในภาพ เส้นสีน้ําเงินนั่นคือ model หรือแบบจําลองเพื่อทํานายค่า y ที่มี x สูงกว่านี้ แต่จะเห็นได้ว่าแทบไม่มีจุดไหนที่ตรงเป๊ะๆเลย เราใช้ได้แค่พอทํานายได้คร่าวๆเท่านั้น
หาก supervised learning คือการเรียนรู้ที่มีคําแนะนํา unsupervised learning ก็คือการไปตายเอาดาบหน้า ไม่มีใครมาแนะนําเรา แต่คงต้องขอไปลุยซักตั้ง
เมื่อ supervised learning มี classification ทางด้าน unsupervised learning ก็จะมี clustering ซึ่งหลายคนรวมถึงผมเอง เมื่อได้รู้จักครั้งแรก ก็สงสัยว่า เอ๊ะ มันต่างกันยังไง classification เป็นการจัดหมวดหมู่ ส่วน clustering เป็นการจัดกลุ่ม ฟังๆดูก็คล้ายๆกันอยู่ดี
เช่นนั้นแล้ว ลองกลับไปสวมบทผู้ป่วยสูญเสียความทรงจํา กับคุณหมอน่ารักอีกสักครั้งนะครับ
คราวนี้ คุณหมอ เอาผลไม้มาอีกสามชนิดที่หน้าตาไม่เหมือนทั้งแอปเปิ้ล มะม่วงและส้ม แต่คุณหมอไม่ยอมบอกอะไรเกี่ยวกับเจ้าพวกนี้เลยสักนิด หรือก็คือตอนนี้ ผลไม้เหล่านี้ไม่มี label แต่คุณหมอก็สั่งให้เราแยกมันออกมาเป็นสามกลุ่ม เมื่อเราสังเกต feature ของผลไม้พวกนี้ก็จะพอแยกแยะได้ว่าผลไม้ลูกไหนควรจะอยู่กลุ่มเดียวกัน
เมื่อแยกได้สามกลุ่มแล้ว คุณหมอก็เดินจากไปเสียเฉยๆ ไม่บอกไม่กล่าวอะไร สุดท้ายเราก็รู้แค่ว่า ผลไม้แต่ละลูกอยู่กลุ่มเดียวกับใคร แต่บอกไม่ได้ว่ามันคืออะไรกันแน่ นี่คือ clustering ต่างจาก classification ที่บอกเราตั้งแต่แรกว่าผลไม้แต่ละชนิดมีชื่อว่าอะไรบ้าง
หลายครั้งที่ข้อมูลนั้นมี feature หลายชนิด หรือก็คือเป็นข้อมูลที่มีมิติมาก เมื่อเป็นเช่นนั้นแล้วก็จะเป็นเรื่องยากที่จะแสดงภาพ หรือ visualize ข้อมูล เราจึงควรลดมิติของข้อมูลลง โดยพยายามคงความหมายเดิมอยู่
dimensionality reduction หรือ dimension reduction เป็นการลดมิติของข้อมูล ซึ่งนอกจากจะทําให้ง่ายที่จะ visualize แล้ว เมื่อมีมิติที่น้อยลง นั่นหมายถึงมี feature ที่น้อยลง ซึ่งทําให้ performance ดีขึ้น และลด space complexity อีกด้วย
นอกจากประเภทของ Machine Learning Algorithm แบบ basic ที่เขียนไว้ด้านบนแล้วยังมี Semi-supervised Learning และ Reinforcement Learning ที่พี่ต้า @konpat เขียนเอาไว้ครับ
สุดท้ายนี้ ผมเชื่อว่า Machine Learning เป็นศาสตร์หนึ่งที่สําคัญมากๆสําหรับวงการคอมพิวเตอร์ในปัจจุบัน และอนาคต และส่วนตัวผมคิดว่าศาสตร์นี้มันเท่มากๆเลยนะ ใครสนใจใน Machine Learning ก็เข้ามาคุยกันได้นะครับ :D
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story. มันคืออะไรอะ?” จะมีความมั่นใจมากพอที่จะตอบเขาไปว่า
“รู้ดิ นั่งๆ เดี๋ยวเล่าให้ฟัง”
ก่อนที่จะอธิบายว่า Machine Learning คืออะไร ขอโม้สักเล็กน้อย ให้เห็นความสําคัญของมันเสียก่อน แล้วจะพบว่า Machine Learning นี่แทบจะเป็นส่วนหนึ่งของชีวิตประจําวันไปแล้ว
Machine Learning เป็นเรื่องที่ใกล้ตัวเรามากๆ ยิ่งสําหรับคนที่ใช้ Internet เป็นประจํานั้น แทบทุกวันจะได้ใช้ประโยชน์จาก Machine Learning ไม่ว่าจะรู้ตัวหรือไม่ก็ตาม ยกตัวอย่างเช่น เมื่อเราต้องการค้นหาอะไรบางอย่างด้วย Google Search แต่ไม่ค่อยแน่ใจ คับคล้ายคับคลาว่ามันน่าจะสะกดแบบนี้ “machn leaning” (ตัวอย่างโง่นิดนึง ๕๕๕๕๕) ปรากฏว่า...
โอ้ พระสงฆ์! ครั้งแรกที่ผมได้ยินคําคํานี้ ผมก็พูดกับตัวเองในใจ “เครื่องจักรที่เรียนรู้ได้ด้วยตัวเองงั้นเหรอ?” ใครมาถามว่ารู้จัก Machine Learning หรือเปล่า? Machine Learning จริงๆแล้วมันคืออะไรกันแน่? ยังมีตัวอย่างอีกมากมายที่นํา Machine Learning ไปใช้ เช่น Spam Filtering, Face Recognition, Handwriting Recognition
แม้แต่การทํา marketing ในปัจจุบันก็เริ่มใช้ประโยชน์ Machine Learning เช่นกัน อาทิ การแบ่งกลุ่มลูกค้า(Customer Segmentation) การทํานายการสูญเสียลูกค้า(Customer Churn Prediction) เป็นต้น
และที่จะไม่พูดถึงไม่ได้เลยคือ Facebook’s Friend Suggestions ที่ผมเองก็ไม่รู้ว่าทําไมมันถึงแนะนําสาวสวยให้ผมอย่างสม่ําเสมอ แต่สิ่งที่ผมมั่นใจคือ Facebook ไม่ได้ใช้คนมานั่งเลือกให้แน่ๆ แล้วมันทําได้ยังไงกัน? ผมก็ได้แต่บอกเขาไปว่า “เคยได้ยินแต่ชื่อว่ะ”
ผมหวังเป็นอย่างยิ่งว่า คุณที่หลงเข้ามาอ่านบทความนี้ ที่อาจจะเป็นเหมือนผมที่เคยได้ยินมาแต่ชื่อ เมื่ออ่านจบ หากมีใครมาถามว่า “รู้จัก Machine Learning หรือเปล่า? Software Choreographer at ThoughtWorks; Functional Programming, DevOps and Machine Learning Enthusiast
a half-score-day-a-story blog by League of Machine Learning from Chulalongkorn University มันคืออะไรอะ? มันเดาใจเราได้ เป็นหมอดูหรืออย่างไร!? แน่นอนว่าโค้ดก็คงจะไม่ใช่แบบด้านล่างนี้แน่ๆ
แล้วทําไม Google ถึงได้รู้ใจเรากันนะ?"
samim,Generating Stories about Images – samim – Medium,"Using Taylor Swift Model. This experiment explores how to generate little romantic stories about images (incl. New machine-learning experiments are enabling us to generate stories based on the content of images. neural-storyteller gives us a fascinating glimpse into the future of storytelling. Using Romantic Novel Model. This experiment started by running 5000 randomly selected web-images through neural-storyteller and experimenting with hyper-parameters. guest star Taylor Swift). Neural-storyteller’s outputs are creative and often comedic. It combines recurrent neural networks (RNN), skip-thoughts vectors and other techniques to generate little story about images. neural-storyteller comes with 2 pre-trained models: One trained on 14 million passages of romance novels, the other trained on Taylor Swift Lyrics."
AirbnbEng,How Airbnb uses Machine Learning to Detect Host Preferences,"All these findings pointed to the same conclusion: if we could promote in our search results hosts who would be more likely to accept an accommodation request resulting from that search query, we would expect to see happier guests and hosts and more matches that turned into fun vacations (or productive business trips). At a two-sided marketplace like Airbnb, we also wanted to personalize search by the preference of the hosts whose listings would appear in the search results. I found similar dispersion in hosts’ tendency to accept other trip characteristics like the number of guests, whether it is a weekend trip etc. On average, for each trip characteristic we could not determine the preference for about 26% of hosts, because they never received an accommodation request that met those trip characteristics. For each search query that a guest enters on Airbnb’s search engine, our model computes the likelihood that relevant hosts will want to accommodate the guest’s request. Indeed, when I plotted hosts’ tendency to accept over the sum of the check-in gap and the check-out gap (3+1= 4 in the example above), as in the next plot, I found the effect that I expected to see: hosts were more likely to accept requests that fit well in their calendar and minimize gap days. Indeed, the dispersion in preferences reveals that some hosts like last minute stays better than far in advance stays — those in the bottom right — even though on average hosts prefer longer notice. And maybe hosts in big markets, like my friend, are different from hosts in smaller markets. For example, on average Airbnb hosts prefer accommodation requests that are at least a week in advance over last minute requests. To test the online performance of the model, we launched an experiment that used the predicted probability of host acceptance as a significant weight in our ranking algorithm that also includes many other features that capture guests’ preferences."
Adam Geitgey,Machine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networks,"But right now, our neural network can’t do this. But now we want to process images with our neural network. To a computer, an image is really just a grid of numbers that represent how dark each pixel is:
To feed an image into our neural network, we simply treat the 18x18 pixel image as an array of 324 numbers:
The handle 324 inputs, we’ll just enlarge our neural network to have 324 input nodes:
Notice that our neural network also has two outputs now (instead of just one). We need to be smarter about how we process images into our neural network. How in the world do we feed images into a neural network instead of just numbers? Instead of feeding entire images into our neural network as one grid of numbers, we’re going to do something a lot smarter that takes advantage of the idea that an object is the same no matter where it appears in a picture. We created a small neural network to estimate the price of a house based on how many bedrooms it had, how big it was, and which neighborhood it was in:
We also know that the idea of machine learning is that the same generic algorithms can be reused with different data to solve different problems. A neural network takes numbers as input. Now that we have a trained neural network, we can use it! Our program can now recognize birds in images!"
Adam Geitgey,Machine Learning is Fun! Part 4: Modern Face Recognition with Deep Learning,"The end result is we turn the original image into a very simple representation that captures the basic structure of a face in a simple way:
To find faces in this HOG image, all we have to do is find the part of our image that looks the most similar to a known HOG pattern that was extracted from a bunch of other training faces:
Using this technique, we can now easily find faces in any image:
If you want to try this step out yourself using Python and dlib, here’s code showing how to generate and view HOG representations of images. So all we need to do ourselves is run our face images through their pre-trained network to get the 128 measurements for each face. For each step, we’ll learn about a different machine learning algorithm. To find faces in an image, we’ll start by making our image black and white because we don’t need color data to find faces:
Then we’ll look at every single pixel in our image one at a time. Then we could measure our unknown face the same way and find the known face with the closest measurements. But instead of training the network to recognize pictures objects like we did last time, we are going to train it to generate 128 measurements for each face. We need to build a pipeline where we solve each step of face recognition separately and pass the result of the current step to the next step. Machine learning people call the 128 measurements of each face an embedding. The simplest approach to face recognition is to directly compare the unknown face we found in Step 2 with all the pictures we have of people that have already been tagged. It turns out that the measurements that seem obvious to us humans (like eye color) don’t really make sense to a computer looking at individual pixels in an image."
Adam Geitgey,Machine Learning is Fun! Part 2 – Adam Geitgey – Medium,"Once I had all that data, I could use it to build a neural network to model how likely it is that you would start with any given letter. There just aren’t enough levels in the original Super Mario Brothers game to provide enough data for a really good model. I ended up with text files that looked like this:
Looking at the text file, you can see that Mario levels don’t really have much of a pattern if you read them line-by-line:
The patterns in a level really emerge when you think of the level as a series of columns:
So in order for the algorithm to find the patterns in our data, we need to feed the data in column-by-column. First, we need a data set for training our model. Because of this, it can model more cases than we could capture in one simple model. Let’s say I need to guess the next letter you are going to type at any point in your story. Can we use the same model that generated fake Hemingway text to generate fake Super Mario Brothers levels? With a lot more training, the model gets to the point where it generates perfectly valid data:
Let’s sample an entire level’s worth of data from our model and rotate it back horizontal:
This data looks great! Let’s use the first few words of Ernest Hemingway’s The Sun Also Rises as an example:
What letter is going to come next? But the one thing this kind of model can’t do is respond to patterns in data over time."
Arthur Juliani,Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks,"By updating in this way, the table slowly begins to obtain accurate measures of the expected future reward for a given action in a given state. We make updates to our Q-table using something called the Bellman equation, which states that the expected long-term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state. In it’s simplest implementation, Q-Learning is a table of values for every state (row) and action (column) possible in the environment. We instead need some way to take a description of our state, and produce Q-values for actions without a table: that is where neural networks come in. In equation form, the rule looks like this:
This says that the Q-value for a given state (s) and action (a) should represent the current reward (r) plus the maximum discounted (γ) future reward expected according to our own table for the next state (s’) we would end up in. Instead of starting with a complex and unwieldy deep neural network, we will begin by implementing a simple lookup-table version of the algorithm, and then show how to implement a neural-network equivalent using Tensorflow. In the case of the FrozenLake environment, we have 16 possible states (one for each block), and 4 possible actions (the four directions of movement), giving us a 16x4 table of Q-values. Within each cell of the table, we learn a value for how good it is to take a given action within a given state. There are a number of possible extensions to our simple Q-Network which allow for greater performance and more robust learning. Unlike policy gradient methods, which attempt to learn functions which directly map an observation to an action, Q-Learning attempts to learn the value of being in a given state, and taking a specific action there."
Adam Geitgey,Machine Learning is Fun Part 6: How to do Speech Recognition with Deep Learning,"To make this data easier for a neural network to process, we are going to break apart this complex sound wave into it’s component parts. Let’s use this sound clip of me saying “Hello”:
Sound waves are one-dimensional. If you know how neural machine translation works, you might guess that we could simply feed sound recordings into a neural network and train it to produce text:
That’s the holy grail of speech recognition with deep learning, but we aren’t quite there yet (at least at the time that I wrote this — I bet that we will be in a couple of years). It breaks apart the complex sound wave into the simple sound waves that make it up. A neural network can find patterns in this kind of data more easily than raw sound waves. We could feed these numbers right into a neural network. In Part 3, we learned how to take an image and treat it as an array of numbers so that we can feed directly into a neural network for image recognition:
But sound is transmitted as waves. Let’s learn how to do speech recognition with deep learning! Let’s zoom in on one tiny part of the sound wave and take a look:
To turn this sound wave into numbers, we just record of the height of the wave at equally-spaced points:
This is called sampling. The first step in speech recognition is obvious — we need to feed sound waves into a computer."
Adam Geitgey,Machine Learning is Fun Part 5: Language Translation with Deep Learning and the Magic of Sequences,"Take this possible translation:
It’s likely that no one has ever written a sentence like this in English, so it would not be very similar to any sentences in our data set. We could use our parallel corpora training data to train it to do that:
And just like that, we have a generic way of converting a sequence of English words into an equivalent sequence of Spanish words! We can come up with an encoding that represents every possible different sentence as a series of unique numbers:
To generate this encoding, we’ll feed the sentence into the RNN, one word at time. Here are some examples:
But in a real-world system, there will be even more possible chunk combinations because we’ll also try different orderings of words and different ways of chunking the sentence:
Now need to scan through all of these generated sentences to find the one that is that sounds the “most human.”
To do this, we compare each generated sentence to millions of real sentences from books and news stories written in English. For example, you can use it to predict the next most likely word in a sentence based on the first few words:
RNNs are useful any time you want to learn patterns in data. But look at this possible translation:
This sentence will be similar to something in our training set, so it will get a high probability score. Statistical machine translation systems perform much better than rule-based systems if you give them enough training data. Then they trained a sequence-to-sequence model where the employee’s question was the input sentence and the Tech Support team’s response was the “translation” of that sentence. Deep learning researchers who know almost nothing about language translation are throwing together relatively simple machine learning solutions that are beating the best expert-built language translation systems in the world. Building a statistics-based translation system requires lots of training data where the exact same text is translated into at least two languages."
Tal Perry,Deep Learning the Stock Market – Tal Perry – Medium,"In Eidnes’ example, the output of the LSTMs is a vector that represents what the next word will be in some abstract space. In Karpathy’s example, the output of the LSTMs is a vector that represents the next character in some abstract representation. Going deeper
I want to point out that this is where we start to get into the deep part of deep learning. Lars inputs a sequence of Word Vectors and each one of them:
We’re going to do the same thing with one difference, instead of word vectors we’ll input “MarketVectors”, those market vectors we described before. Then we’ll get a sequence that looks like:
We want to take the vector that our LSTMs output and squish it so that it gives us the probability of the next item in our sequence being a 1. But, I want to achieve something specific with the first layer, the one that takes as input the 4000 dimensional vector and outputs a 300 dimensional one. Where Karpathy used characters, we’re going to use our market vectors and feed them into the magic black box. Now each output of the previous LSTM becomes the inputs of the next LSTM, and each one goes on to learn higher abstractions of the data coming in. So I’ll get back to the mechanics of that magic box in a second, but let me remind you that we want to predict the future market based on the past just like he predicted the next word based on the previous one. (Remember, a “word vector” is a representation of a word in the form of a bunch of numbers, like we saw in the beginning of this post)."
Andrej Karpathy,Yes you should understand backprop – Andrej Karpathy – Medium,"See a longer explanation in CS231n lecture video. See a longer explanation in this CS231n lecture video. See a longer explanation in this CS231n lecture video. The forward and backward pass for a fully connected layer that uses ReLU would at the core include:
If you stare at this for a while you’ll see that if a neuron gets clamped to zero in the forward pass (i.e. But if that is the case, z*(1-z), which is local gradient of the sigmoid non-linearity, will in both cases become zero (“vanish”), making the gradient for both x and W be zero. When you stare at what the backward pass is doing, you’ll see that the gradient signal going backwards in time through all the hidden states is always being multiplied by the same matrix (the recurrence matrix Whh), interspersed with non-linearity backprop. For example, a fully connected layer with sigmoid non-linearity computes (using raw numpy):
If your weight matrix W is initialized too large, the output of the matrix multiply could have a very large range (e.g. I hope you’ll be much more suspicious of backpropagation going forward and think carefully through what the backward pass is doing. Another fun non-linearity is the ReLU, which thresholds neurons at zero from below. TLDR: if you’re using sigmoids or tanh non-linearities in your network and you understand backpropagation you should always be nervous about making sure that the initialization doesn’t cause them to be fully saturated."
Per Harald Borgen,Machine Learning in a Year – Learning New Stuff – Medium,"A few weeks in, I wanted to learn how to actually code machine learning algorithms, so I started a study group with a few of my peers. This is a follow up to an article I wrote last year, Machine Learning in a Week, on how I kickstarted my way into machine learning (ml) by devoting five days to the subject. But also a fast one; when I started my machine learning in a week project, I certainly didn’t have any hopes of actually using it professionally within a year. After this highly effective introduction, I continued learning on my spare time and almost exactly one year later I did my first ml project at work, which involved using various ml and natural language processing (nlp) techniques to qualify sales leads at Xeneta. Every Tuesday evening, we’d watch lectures from Coursera’s Machine Learning course. Having gotten a basic understanding of neural networks at this point, I wanted to move on to deep learning. So I began watching the first few chapters of Udacity’s Supervised Learning course, while also reading all articles I came across on the subject. So it might be that I simply wasn’t ready for the course. My first attempt was Udacity’s Deep Learning course, which ended up as a big disappointment. So I asked my manager if I could spend some time learning stuff during my work hours as well, which he happily approved."
Xiaohan Zeng,"I interviewed at five top companies in Silicon Valley in five days, and luckily got five job offers","Three are machine learning engineer (LinkedIn, Google, Facebook), one is data engineer (Salesforce), and one is software engineer in general (Airbnb). In the five days from July 24th to 28th 2017, I interviewed at LinkedIn, Salesforce Einstein, Google, Airbnb, and Facebook, and got all five job offers. The bar is very high and the process is quite long, including one pre-screening questionnaire, one phone screening, one coding assignment, and one full onsite. One thing that I felt special about Google’s interviews is that the analysis of algorithm complexity is really important. I was able to skip the second round phone screening with Airbnb and Salesforce because I got the onsite at LinkedIn and Facebook after only one phone screening. The only difference is in the duration: For some companies like LinkedIn it’s one hour, while for Facebook and Airbnb it’s 45 minutes. Therefore I needed to prepare for three different areas: coding, machine learning, and system design. For machine learning positions some companies would ask ML questions. Here are some resources that I found really helpful:
Although system design interviews can cover a lot of topics, there are some general guidelines for how to approach the problem:
With all that said, the best way to practice for system design interviews is to actually sit down and design a system, i.e. I flew in on Sunday, had five full days of interviews with around 30 interviewers at some best tech companies in the world, and very luckily, got job offers from all five of them."
Gil Fewster,The mind-blowing AI announcement from Google that you probably missed.,"Up until September of last year, Google Translate used phrase-based translation. All that changed in September, when Google gave their translation tool a new engine: the Google Neural Machine Translation system (GNMT). Phrase-based translation has no capacity to make educated guesses at words it doesn’t recognize, and can’t learn from new input. Not only was the article competing with the pre-Christmas rush that most of us were navigating — it was also tucked away on Google’s Research Blog, beneath the geektastic headline Zero-Shot Translation with Google’s Multilingual Neural Machine Translation System. Here’s Google’s Mike Schuster, Nikhil Thorat, and Melvin Johnson from the original blog post:
Here you can see an advantage of Google’s new neural machine over the old phrase-based approach. A neural computing system designed to translate content from one human language into another developed its own internal language to make the task more efficient. Which is a shame, because it may just be the most astonishing article about machine learning that I read last year. Google Translate invented its own language to help it translate more effectively. The short version is that Google Translate got smart. Phrase-based translation is a blunt instrument."
David Venturi,"Every single Machine Learning course on the internet, ranked by your reviews",It has a 4-star weighted average rating over 3 reviews. It has a 3.6-star weighted average rating over 5 reviews. It has a 4-star weighted average rating over 4 reviews. It has a 4.5-star weighted average rating over 6 reviews. It has a 2-star weighted average rating over 2 reviews. It has a 4.5-star weighted average rating over 2 reviews. Free and paid options available. Free and paid options available. Free and paid options available. Free and paid options available.
Vishal Maini,A Beginner’s Guide to AI/ML 🤖👶 – Machine Learning for Humans – Medium,"Part 2.1: Supervised Learning. Demystifying artificial intelligence & machine learning. Machine learning is a subfield of artificial intelligence. Part 1: Why Machine Learning Matters. And now, without further ado, let’s dive into machine learning with Part 2.1: Supervised Learning! Q-learning, policy learning, and deep reinforcement learning. Part 5: Reinforcement Learning. Part 2.2: Supervised Learning II. Part 2.3: Supervised Learning III. Part 4: Neural Networks & Deep Learning."
Tim Anglade,"How HBO’s Silicon Valley built “Not Hotdog” with mobile TensorFlow, Keras & React Native","A neural network can only be as good as the data that trained it, and improving training set quality was probably one of the top 3 things we spent time on during this project. The HBO show Silicon Valley released a real AI app that identifies hotdogs — and not hotdogs — like the one shown on season 4’s 4th episode (the app is now available on Android as well as iOS!) There were 3 main factors:
For these reasons, we started experimenting with what’s trendily called “edge computing”, which for our purposes meant that after training our neural network on our laptop, we would export it and embed it directly into our mobile app, so that the neural network execution phase (or inference) would run directly inside the user’s phone. First The nature of our problem meant a strong imbalance in training data: there are many more examples of things that are not hotdogs, than things that are hotdogs. The problem directly ahead of us was simple: if Inception and VGG were too big, was there a simpler, pre-trained neural network we could retrain? While the network (Inception in this case) may have been trained on the 14M images contained in ImageNet, we were able to retrain it on a just a few thousand hotdog images to get drastically enhanced hotdog recognition. The key things we did to improve this were:
The final composition of our dataset was 150k images, of which only 3k were hotdogs: there are only so many hotdogs you can look at, but there are many not hotdogs to look at. After adding Batch Normalization and ELU to SqueezeNet, we were able to train neural network that achieve 90%+ accuracy when training from scratch, however, they were relatively brittle meaning the same network would overfit in some cases, or underfit in others when confronted to real-life testing. I could build it in one weekend!” This app probably feels a lot like that, and the initial prototype was indeed built in a single weekend using Google Cloud Platform’s Vision API, and React Native. Most AI apps will hit more critical cultural biases than ours, but as an example, even our straightforward use-case, caught us flat-footed with built-in biases in our initial dataset, that made the app unable to recognize French-style hotdogs, Asian hotdogs, and more oddities we did not have immediate personal experience with."
Sophia Ciocca,How Does Spotify Know You So Well? – Member Feature Stories – Medium,"To create Discover Weekly, there are three main types of recommendation models that Spotify employs:
Let’s dive into how each of these recommendation models work! Then, much like in collaborative filtering, the NLP model uses these terms and weights to create a vector representation of the song that can be used to determine if two pieces of music are similar. Each of these individuals has track preferences: the one on the left likes tracks P, Q, R, and S, while the one on the right likes tracks Q, R, S, and T.
Collaborative filtering then uses that data to say:
“Hmmm... You both like three of the same tracks — Q, R, and S — so you are probably similar users. First, some background: When people hear the words “collaborative filtering,” they generally think of Netflix, as it was one of the first companies to use this method to power a recommendation model, taking users’ star-based movie ratings to inform its understanding of which movies to recommend to other similar users. Each row represents one of Spotify’s 140 million users — if you use Spotify, you yourself are a row in this matrix — and each column represents one of the 30 million songs in Spotify’s database. Luckily, raw audio models don’t discriminate between new tracks and popular tracks, so with their help, your friend’s song could end up in a Discover Weekly playlist alongside popular songs! The second type of recommendation models that Spotify employs are Natural Language Processing (NLP) models. But this model also serves a secondary purpose: unlike the first two types, raw audio models take new songs into account. Let’s zoom out for a second to look at how other music services have tackled music recommendations, and how Spotify’s doing it better. To find out which users’ musical tastes are most similar to mine, collaborative filtering compares my vector with all of the other users’ vectors, ultimately spitting out which users are the closest matches."
Dhruv Parthasarathy,A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN,"In the image above, you can see how a single CNN is used to both carry out region proposals and classification. With these anchor boxes in mind, let’s take a look at the inputs and outputs to this Region Proposal Network:
We then pass each such bounding box that is likely to be an object into Fast R-CNN to generate a classification and tightened bounding boxes. The insight of Faster R-CNN was that region proposals depended on features of the image that were already calculated with the forward pass of the CNN (first step of classification). Fast R-CNN Insight 1: RoI (Region of Interest) Pooling
For the forward pass of the CNN, Girshick realized that for each image, a lot of proposed regions for the image invariably overlapped causing us to run the same CNN computation again and again (~2000 times!). The authors write:
Here are the inputs and outputs of their model:
How the Regions are Generated
Let’s take a moment to see how Faster R-CNN generates these region proposals from CNN features. Where earlier we had different models to extract image features (CNN), classify (SVM), and tighten bounding boxes (regressor), Fast R-CNN instead used a single network to compute all three. Understanding R-CNN
The goal of R-CNN is to take in an image, and correctly identify where the main objects (via a bounding box) in the image. In the image above, notice how the CNN features for each region are obtained by selecting a corresponding region from the CNN’s feature map. In particular, we’ll cover R-CNN (Regional CNN), the original application of CNNs to this problem, along with its descendants Fast R-CNN, and Faster R-CNN. Much like Fast R-CNN, and Faster R-CNN, Mask R-CNN’s underlying intuition is straight forward."
Andrej Karpathy,Software 2.0 – Andrej Karpathy – Medium,"Software 1.0 is code we write. They are Software 2.0. Software 2.0 is code we do not write, but seems to work well. If you think of neural networks as a new software stack and not just a pretty good classifier, it quickly becomes apparent there is a lot of work to do. will be subject to this transition, because the optimization can find much better code than what we can write. This is because Google is currently at the forefront of re-writing large chunks of itself into Software 2.0 code. Let’s take a look at some of the benefits of Software 2.0 (think: a ConvNet) compared to Software 1.0 (think: a production-level C++ code base). In contrast, Software 2.0 can be written in much more abstract, human unfriendly language, such as the weights of a neural network. Neural networks are not just another classifier, they represent the beginning of a fundamental shift in how we write software. As a corollary, since the instruction set of a neural network is relatively small, it is significantly easier to implement these networks much closer to silicon, e.g."
Sebastian Heinz,A simple deep learning model for stock price prediction using TensorFlow,"We need two placeholders in order to fit our model: X contains the network's inputs (the stock prices of all S&P 500 constituents at time T = t) and Y the network's outputs (the index value of the S&P 500 at time T = t + 1). Also our data science consultants at STATWORX are heavily using TensorFlow for deep learning and neural net research and development. The dataset was split into training and test data. During minibatch training random data samples of n = batch_size are drawn from the training data and fed into the network. The cost function of the network is used to generate a measure of deviation between the network’s predictions and the actual observed training targets. Initializers are used to initialize the network’s variables before training. Other network architectures, such as recurrent neural networks, also allow data flowing “backwards” in the network. After having defined the placeholders, variables, initializers, cost functions and optimizers of the network, the model needs to be trained. Since neural networks are actually graphs of data and mathematical operations, TensorFlow is just perfect for neural networks and deep learning. The user defines an abstract representation of the model (neural network) through placeholders and variables."
Netflix Technology Blog,Artwork Personalization at Netflix – Netflix TechBlog – Medium,"The optimal assignment of image artwork to a member is a selection problem to find the best candidate image from a title’s pool of available images. What we seek to understand is when presenting a specific piece of artwork for a title influenced a member to play (or not to play) a title and when a member would have played a title (or not) regardless of which image we presented. Exploration in contextual bandits typically has a cost (or regret) due to the fact that our artwork selection in a member session may not use the predicted best image for that session. In this online learning setting, we train our contextual bandit model to select the best artwork for each member based on their context. Even with this simplification we can still learn member image preferences across titles because, for every image candidate, we have some members who were presented with it and engaged with the title and some members who were presented with it and did not engage. For artwork personalization, the specific online learning framework we use is contextual bandits. One challenge of image personalization is that we can only select a single piece of artwork to represent each title in each place we present it. Of course, to properly learn how to personalize artwork we need to collect a lot of data to find signals that indicate when one piece of artwork is significantly better for a member. To learn the selection model, we can consider a simplification of the problem by ranking images for a member independently across titles. For personalization, the member is the context as we expect different members to respond differently to the images."
Michael Jordan,Artificial Intelligence — The Revolution Hasn’t Happened Yet,"As for the necessity argument, it is sometimes argued that the human-imitative AI aspiration subsumes IA and II aspirations, because a human-imitative AI system would not only be able to solve the classical problems of AI (as embodied, e.g., in the Turing test), but it would also be our best bet for solving IA and II problems. First, although one would not know it from reading the newspapers, success in human-imitative AI has in fact been limited — we are very far from realizing human-imitative AI aspirations. We need to solve IA and II problems on their own merits, not as a mere corollary to a human-imitative AI agenda. Whereas civil engineering and chemical engineering were built on physics and chemistry, this new engineering discipline will be built on ideas that the preceding century gave substance to — ideas such as “information,” “algorithm,” “data,” “uncertainty,” “computing,” “inference,” and “optimization.” Moreover, since much of the focus of the new discipline will be on data from and about humans, its development will require perspectives from the social sciences and humanities. One could argue that an AI system would not only imitate human intelligence, but also “correct” it, and would also scale to arbitrarily large problems. However, the current focus on doing AI research via the gathering of data, the deployment of “deep learning” infrastructure, and the demonstration of systems that mimic certain narrowly-defined human skills — with little in the way of emerging explanatory principles — tends to deflect attention from major open problems in classical AI. In the current era, we have a real opportunity to conceive of something historically new — a human-centric engineering discipline. It is those challenges that need to be in the forefront, and in such an effort a focus on human-imitative AI may be a distraction. The past two decades have seen major progress — in industry and academia — in a complementary aspiration to human-imitative AI that is often referred to as “Intelligence Augmentation” (IA). These are classical goals in human-imitative AI, but in the current hubbub over the “AI revolution,” it is easy to forget that they are not yet solved."
Blaise Aguera y Arcas,Do algorithms reveal sexual orientation or just expose our stereotypes?,"The results show that lesbians indeed use eyeshadow much less than straight women do, gay men and women do both wear glasses more, and young opposite-sex-attracted men are considerably more likely to have prominent facial hair than their gay or same-sex-attracted peers. However, asking the question “Do you like how you look in glasses?” reveals that this is likely more of a stylistic choice:
Same-sex attracted women also report wearing glasses more, as well as liking how they look in glasses more, across a range of ages:
One can also see how opposite-sex attracted women under the age of 40 wear contact lenses significantly more than same-sex attracted women, despite reporting that they have a vision defect at roughly the same rate, further illustrating how the difference is driven by an aesthetic preference: [4]
Similar analysis shows that young same-sex attracted men are much less likely to have hairy faces than opposite-sex attracted men (“serious facial hair” in our plots is defined as answering “yes” to having a goatee, beard, or moustache, but “no” to stubble). Overall, opposite-sex attracted men in our sample are 35% more likely to have serious facial hair than same-sex attracted men, and for men under the age of 31 (who are overrepresented on dating websites), this rises to 75%. In the following figures, we show the proportion of women who answer “yes” to “Do you ever use makeup?” (top) and “Do you wear eyeshadow?” (bottom), averaged over 6-year age intervals:
The blue curves represent strictly opposite-sex attracted women (a nearly identical set to those who answered “yes” to “Are you heterosexual or straight?”); the cyan curve represents women who answer “yes” to either or both of “Are you sexually attracted to women?” and “Are you romantically attracted to women?”; and the red curve represents women who answer “yes” to “Are you homosexual, gay or lesbian?”. [2] The patterns revealed here are intuitive; it won’t be breaking news to most that straight women tend to wear more makeup and eyeshadow than same-sex attracted and (even more so) lesbian-identifying women. This is consistent with a pattern of heterosexual men on average shooting from below, heterosexual women from above as the wedding photographer suggests, and gay men and lesbian women from directly in front. Our survey confirms that opposite-sex attracted men consistently self-report having a tan face (“Yes” to “Is your face tan?”) slightly more often than same-sex attracted men:
Once again Wang and Kosinski reach for a hormonal explanation, writing: “While the brightness of the facial image might be driven by many factors, previous research found that testosterone stimulates melanocyte structure and function leading to a darker skin”. The fact that we see a cohort of same-sex attracted men in their 40s who have just as much facial hair as opposite-sex attracted men suggests a different story, in which fashion trends and cultural norms play the dominant role in choices about facial hair among men, not differing exposure to hormones early in development. Composite images of the lesbian, gay, and straight men and women in the sample reveal a great deal about the information available to the algorithm:
Clearly there are differences between these four composite faces. That same-sex attracted men of most ages wear glasses significantly more than exclusively opposite-sex attracted men do might be a bit less obvious, but this trend is equally clear: [3]
A proponent of physiognomy might be tempted to guess that this is somehow related to differences in visual acuity between these populations of men."
James Le,A Tour of The Top 10 Algorithms for Machine Learning Newbies,"Because of the way that the model is learned, the predictions made by logistic regression can also be used as the probability of a given data instance belonging to class 0 or class 1. For regression problems, this might be the mean output variable, for classification problems this might be the mode (or most common) class value. Logistic regression is like linear regression in that the goal is to find the values for the coefficients that weight each input variable. After all the trees are built, predictions are made for new data, and the performance of each tree is weighted by how accurate it was on training data. When you need to make a prediction for new data, each model makes a prediction and the predictions are averaged to give a better estimate of the true output value. If we did, we would use it directly and we would not need to learn it from data using machine learning algorithms. Predictions are made for a new data point by searching through the entire training set for the K most similar instances (the neighbors) and summarizing the output variable for those K instances. It’s a simple and powerful method for classification predictive modeling problems. Once calculated, the probability model can be used to make predictions for new data using Bayes Theorem. Logistic Regression is a classification algorithm traditionally limited to only two-class classification problems."
Emmanuel Ameisen,How to solve 90% of NLP problems: a step-by-step guide,"This is called a Bag of Words model, since it is a representation that completely ignores the order of words in our sentence. Using this approach we can get word importance scores like we had for previous models and validate our model’s predictions. To validate our model and interpret its predictions, it is important to look at which words it is using to make decisions. In order to see whether the Bag of Words features are of any use, we can train a classifier based on them. Right now, our Bag of Words model is dealing with a huge vocabulary of different words and treating all words equally. Plotting word importance is simple with Bag of Words and Logistic Regression, since we can just extract and rank the coefficients that the model used for its predictions. Using pre-trained words
Word2Vec is a technique to find continuous embeddings for words. Since our embeddings are not represented as a vector with one dimension per word as in our previous models, it’s harder to see which words are the most relevant to our classification. In order to help our model focus more on meaningful words, we can use a TF-IDF score (Term Frequency, Inverse Document Frequency) on top of our Bag of Words model. Here is a visualization of our new embeddings using previous techniques:
The two groups of colors look even more separated here, our new embeddings should help our classifier find the separation between both classes."
Mybridge,30 Amazing Machine Learning Projects for the Past Year (v.2018),"Courtesy of Dabi Ahn, AI Research at Kakao Brain
That’s it for Machine Learning open source projects. Courtesy of Nikhil Thorat at Google Brain
Fast Style Transfer in TensorFlow [4843 stars on Github]. Courtesy of Akshay Bhat, Ph.D at Cornell University
OpenNMT: Open-Source Neural Machine Translation in Torch [1490 stars on Github]. Deep-image-prior: Image restoration with neural networks but without learning [2188 stars on Github]. Courtesy of Google Brain
Style2Paints: AI colorization of images [3310 stars on Github]. Courtesy of Timo Ewalds at DeepMind
AirSim: Open source simulator based on Unreal Engine for autonomous vehicles from Microsoft AI & Research [3861 stars on Github]. [1188 stars on Github]. [2629 stars on Github]. [11786 stars on Github]. [1967 stars on Github]."
David Foster,How to build your own AlphaZero AI using Python and Keras,"You can replace the game.py file with any game file that conforms to the same API and the algorithm will in principal, learn strategy through self play, based on the rules you have given it. If you switch the Connect4 game.py file for the Metasquares game.py file, the same algorithm will learn how to play Metasquares instead. There is a game.py file for a game called ‘Metasquares’ in the games folder. The game that our algorithm will learn to play is Connect4 (or Four In A Row). To view individual convolutional filters and densely connected layers in the neural network, run the following inside the the run.ipynb notebook:
This contains the Node, Edge and MCTS classes, that constitute a Monte Carlo Search Tree. If it wins, the neural network inside the best_player is switched for the neural network inside the current_player, and the loop starts again. An instance of the Memory class stores the memories of previous games, that the algorithm uses to retrain the neural network of the current_player. This file contains the Residual_CNN class, which defines how to build an instance of the neural network. The best_player contains the best performing neural network and is used to generate the self play memories. To play against your creation, run the following code (it’s also in the run.ipynb notebook)
When you run the algorithm, all model and memory files are saved in the run folder, in the root directory."
George Seif,The 5 Clustering Algorithms Data Scientists Need to Know,"Given a set of data points, we can use a clustering algorithm to classify each data point into a specific group. In Data Science, we can use clustering analysis to gain some valuable insights from our data by seeing what groups the data points fall into when we apply a clustering algorithm. Check out the graphic below for an illustration before moving on to the algorithm steps
Hierarchical clustering does not require us to specify the number of clusters and we can even select which number of clusters looks best since we are building a tree. Mean shift clustering is a sliding-window-based algorithm that attempts to find dense areas of data points. Bottom-up algorithms treat each data point as a single cluster at the outset and then successively merge (or agglomerate) pairs of clusters until all clusters have been merged into a single cluster that contains all data points. K-Medians is another clustering algorithm related to K-Means, except instead of recomputing the group center points using the mean we use the median vector of the group. K-Means also fails in cases where the clusters are not circular, again as a result of using the mean as cluster center. K-Means is probably the most well know clustering algorithm. In contrast to K-means clustering there is no need to select the number of clusters as mean-shift automatically discovers this. A particularly good use case of hierarchical clustering methods is when the underlying data has a hierarchical structure and you want to recover the hierarchy; other clustering algorithms can’t do this."
Mybridge,30 Amazing Python Projects for the Past Year (v.2018),"For Python [3024 stars on Github]. [3179 stars on Github]. [722 stars on Github]. [1009 stars on Github]. [1717 stars on Github]. [1585 stars on Github]. [7775 stars on Github]. [1143 stars on Github]. [8367 stars on Github]. Courtesy of Ofek Lev
Tangent: Source-to-Source Debuggable Derivatives in Pure Python [1433 stars on Github]."
Simon Greenman,Who Is Going To Make Money In AI? Part I – Towards Data Science,"No wonder it is a brutal race for global AI algorithm dominance with Amazon — Microsoft — IBM also offering their own cheap or free AI software services. And there are few key themes appearing as to where the economic value will migrate:
In short it looks like the AI gold rush will favour the companies and countries with control and scale over the best AI tools and technology, the data, the best technical workers, the most customers and the strongest access to capital. Today there are over 200 AI powered companies just in the recruitment space, many of them AI startups. Data is the fuel for AI and machine learning. And the incredible AI machine learning software and algorithms that are powering all of Google’s AI activity — TensorFlow — is now being given away for free. And Microsoft is building its own Brainwave AI machine learning chips. We are in the midst of a gold rush in AI. But like any gold rush many startups will hit pay dirt. If millions of companies use this best in class free AI software then they are likely to need lots of computing power. However there is likely to be a period of upheaval when much of the value will go to those few companies and countries that control AI technology and data."
Eugenio Culurciello,The fall of RNN / LSTM – Towards Data Science,"To the rescue, and combining multiple neural attention modules, comes the “hierarchical neural attention encoder”, shown in the figure below:
A better way to look into the past is to use attention modules to summarize all past encoded vectors into a context vector Ct.
Notice there is a hierarchy of attention modules here, very similar to the hierarchy of neural networks. Note 2: RNN and LSTM are memory-bandwidth limited problems (see this for details). We fell for Recurrent neural networks (RNN), Long-short term memory (LSTM), and all their variants. But instead of a convolutional neural network we use hierarchical attention modules. Note 1: Hierarchical neural attention is similar to the ideas in WaveNet. About training RNN/LSTM: RNN and LSTM are difficult to train because they require memory-bandwidth-bound computation, which is the worst nightmare for hardware designer and ultimately limits the applicability of neural networks solutions. This is in contrast to the T steps that a RNN needs to do, where T is the maximum length of the sequence to be remembered, and T >> N.
This architecture is similar to a neural Turing machine, but lets the neural network decide what is read out from memory via attention. Remember RNN and LSTM and derivatives use mainly sequential processing over time. It is very surprising to us to see so many companies still use RNN/LSTM for speech to text, many unaware that these networks are so inefficient and not scalable. This extends the ability of the hierarchical neural attention encoder to 10,000 past vectors."
WiseWolf Fund,GAME-CHANGING TRENDS TO LOOK OUT FOR WITH AI – WiseWolf Fund – Medium,"There are investment funds running on artificial intelligence that are available for individual investors. As we said already, AI can also create jobs, so a wise move would be to learn to manage AI-based tools. In fact, it would not be fair to say that artificial intelligence causes people to lose jobs. The reason for such interest towards artificial intelligence is that artificial intelligence can enhance any product or function. This is where artificial intelligence is at your service. Companies in the US are also investing time, money and energy into advancing AI technology. When asked about the current trends and opportunities of AI, Aaron Edell, CEO and co-founder of Machine Box, and one of the top writers on AI, described them as follows:
AI has also become a political talking point in recent years. And this is where artificial intelligence may become your best friend, professional consultant and investment manager. However, it would be more correct if we said that artificial intelligence reshapes the employment situation. There have been arguments that AI will help to create jobs, but that it will also cause certain workers to lose their jobs."
Justin Lee,Chatbots were the next big thing: what happened? – The Startup – Medium,"But a bot isn’t the same as a human. Remember: a bot that does ONE thing well is infinitely more helpful than a bot that does multiple things poorly. Building a bot for the sake of it, letting it loose and hoping for the best will never end well:
The vast majority of bots are built using decision-tree logic, where the bot’s canned response relies on spotting specific keywords in the user input. The next wave will be multimodal apps, where you can say what you want (like with Siri) and get back information as a map, text, or even a spoken response. Are chatbots cheaper or faster than apps? At the end of the day, no amount of witty quips or human-like mannerisms will save a bot from conversational failure. Despite the constant hype around AI, we’re still a long way from achieving anything remotely human-like. Conversational UIs are built to replicate the way humans prefer to communicate — with other humans. Computers aren’t good at understanding human emotion. What is too complicated is trying to complete these tasks with a bot — and having the bot fail."
Michael Solana,Artificial Intelligence Is Humanity's Rorschach Test,"Let us name this difference ‘x.’ Now, as we try and understand the difference between the most intelligent human who has ever lived and a hypothetical god-like intelligence born of the Singularity, let us set our difference in intelligence at a conservative ‘1000x.’
How does one even begin to conceive of a being this smart? In trying to imagine the amplified intelligence, it is natural to imagine our own intelligence amplified. In imagining the motivations of this amplified intelligence, we naturally imagine ourselves. I’m talking about general artificial intelligence, which is a computer that wants stuff, and chiefly to live. Let’s look at this in the context of artificial intelligence. The labradoodle’s conception of man is distorted because there is a vast difference between the intelligence of a dog, and the intelligence of a human. There’s an obvious difficulty in trying to understand the hypothetical motivations of a hypothetically god-like intelligence. In this way, a conscious artificial intelligence born on a Tuesday morning might be twice as smart as the smartest man who ever lived by Wednesday afternoon, and omnipotent by Friday. I’m talking about building a conscious machine just smart enough to make itself smarter. Not everyone who fears general artificial intelligence would cause harm to others."
Emmanuel Ameisen,Reinforcement Learning from scratch – Insight Data,"We learn our policy directly with policy gradients (defined above), and learn a value function using something called Advantage. While the value function tells us how good we estimate each action to be, the policy is the function that determines which actions we end up taking. 4+/Leveraging deep learning for representations
In practice, many state of the art RL methods require learning both a policy and value estimates. This allows us to learn a good value function. On-policy methods can only learn from actions that were taken following our policy (remember, a policy is the method we use to determine which actions to take). Previously, we were first learning a value function Q for each action in each state and then building a policy on top. The approach we will use here is called Policy Gradients, and is an on-policy method. We will use a method called Temporal Difference (TD) learning to learn a good Q function. In Vanilla Policy Gradient, we still use Monte Carlo Estimates, but we learn our policy directly through a loss function that increases the probability of choosing rewarding actions. A note about off-policy vs on-policy learning: The methods we used previously, are off-policy methods, meaning we can generate data with any strategy(using epsilon greedy for example) and learn from it."
Irhum Shafkat,Intuitively Understanding Convolutions for Deep Learning,"Each filter in a convolution layer produces one and only one output channel, and they do it like so:
Each of the kernels of the filter “slides” over their respective input channels, producing a processed version of each. Even if were we to apply a kernel of the same size (3×3), having the same local area, to the output of the strided convolution, the kernel would have a larger effective receptive field:
This is because the output of the strided layer still does represent the same image. If this were a standard fully connected layer, you’d have a weight matrix of 25×9 = 225 parameters, with every output feature being the weighted sum of every single input feature. The output features are essentially, the weighted sums (with the weights being the values of the kernel itself) of the input features located roughly in the same location of the output pixel on the input layer. Each filter actually happens to be a collection of kernels, with there being one kernel for every single input channel to the layer, and each kernel being unique. Convolutions allow us to do this transformation with only 9 parameters, with each output feature, instead of “looking at” every input feature, only getting to “look” at input features coming from roughly the same location. And with the single filter case down, the case for any number of filters is identical: Each filter processes the input with its own, different set of kernels and a scalar bias with the process described above, producing a single output channel. Followed by a final pooling layer, which collapses each 7×7 grid into a single pixel, each channel is a feature detector with a receptive field equivalent to the entire image. Both are methods of increasing the receptive field, but dilated convolutions are a single layer, while this takes place on a regular convolution following a strided convolution, with a nonlinearity inbetween)
This expansion of the receptive field allows the convolution layers to combine the low level features (lines, edges), into higher level features (curves, textures), as we see in the mixed3a layer. The kernels of a filter each produce one version of each channel, and the filter as a whole produces one overall output channel."
Sam Drozdov,An intro to Machine Learning for designers – UX Collective,"Since machine learning is now more accessible than ever before, designers today have the opportunity to think about how machine learning can be applied to improve their products. Supervised learning allows us to make predictions using correctly labeled data. Understanding the problem you are trying to solve and the available data will constrain the types of machine learning you can use (e.q. Machine learning allows us to make predictions about how a user might behave next. For the same reasons, designers should know about machine learning. Reinforcement learning doesn’t use an existing data set. If you are interested in learning more about machine learning, here are some helpful resources:
Thanks for reading. Depending on the application and what data is available, there are different types of machine learning algorithms to choose from. Over the past decade new algorithms, better hardware, and more data have made machine learning an order of magnitude more effective. Machine learning can help create user-centric products by personalizing experiences to the individuals who use them."
Conor Dewey,The Big List of DS/ML Interview Resources – Towards Data Science,"Data science interviews certainly aren’t easy. Specifically, I highly recommend checking out the first two links regarding 120 Data Science Interview Questions. This won’t be covered in every single data science interview, but it’s certainly not uncommon. If you’re interested in receiving my weekly rundown of interesting articles and resources focused on data science, machine learning, and artificial intelligence, then subscribe to Self Driven Data Science using the form below! Machine learning is a complex field that is a virtual guarantee in data science interviews today. Here’s some of the more general resources covering data science as a whole. Through this exciting and somewhat (at times, very) painful process, I’ve accumulated a plethora of useful resources that helped me prepare for and eventually pass data science interviews. A data science interview typically isn’t complete without checking your knowledge of SQL. Even Data Scientists cannot escape the dreaded algorithmic coding interview. Lastly, this post is part of an ongoing initiative to ‘open-source’ my experience applying and interviewing at data science positions, so if you enjoyed this content then be sure to follow me for more stuff like this."
Abhishek Parbhakar,Must know Information Theory concepts in Deep Learning (AI),"Cross entropy between two probability distributions p and q defined over same set of outcomes is given by:
Mutual information is a measure of mutual dependency between two probability distributions or random variables. The probability distribution of experiment is used to calculate the entropy. Mutual information of two discrete random variables X and Y is defined as:
where p(x,y) is the joint probability distribution of X and Y, and p(x) and p(y) are the marginal probability distribution of X and Y respectively. Cross entropy is used to compare two probability distributions. KL divergence between ‘P’ and ‘Q’ tells us how much information we lose when we try to approximate data given by ‘P’ with ‘Q’. Entropy gives a measure of uncertainty in an experiment. KL divergence of a probability distribution Q from another probability distribution P is defined as:
KL divergence is commonly used in unsupervised machine learning technique Variational Autoencoders. For example, in an deterministic experiment, we always know the outcome, so no new information gained is here from observing the outcome and hence entropy is zero. The information gained for a outcome of an experiment is defined as a function of probability of occurrence of that outcome. KL divergence is another measure to find similarities between two probability distributions."
Aman Dalmia,What I learned from interviewing at multiple AI companies and start-ups,"Also, when I was preparing, I noticed people using a lot of resources but as per my experience over the past months, I realised that one can do away with a few minimal ones for most roles in AI, all of which I’m going to mention at the end of the post. There are mostly two types of interviews — one, where the interviewer has come with come prepared set of questions and is going to just ask you just that irrespective of your profile and the second, where the interview is based on your CV. Now we come to the conclusion of the interview where the interviewer would ask you if you have any questions for them. People DO go through your Github because that’s the only way they have to validate what you have mentioned in your CV, given that there’s a lot of noise today with people associating all kinds of buzzwords with their profile. The motive of this kind of interview is to really check whether whatever you have written on your CV is true or not:
There would be a lot of questions on what could be done differently or if “X” was used instead of “Y”, what would have happened. This post arose from a discussion with one of my juniors on the lack of really fulfilling job opportunities offered through campus placements for people working in AI. Also, you won’t need to prepare answers to various kinds of questions that you get asked during an interview. It’s really easy to think that your interview is done and just say that you have nothing to ask. The resources mentioned in the Minimal Resources you need for preparation section should suffice, but make sure that you don’t miss out one bit among them. In the process, not only did I get an opportunity to interact with many great minds, but also had a peek at myself along with a sense of what people really look for when interviewing someone."
Lance Ulanoff,Did Google Duplex just pass the Turing Test? – Lance Ulanoff – Medium,"Duplex made the call and, when someone at the salon picked up, the voice AI started the conversation with:
“Hi, I’m calling to book a woman’s hair cut appointment for a client, um, I’m looking for something on May third?”
When the attendant asked Duplex to give her one second, Duplex responded with:
“Mmm-hmm.”
The conversation continued as the salon representative presented various dates and times and the AI asked about other options. The entity making the call and appointment was Google Assistant running Duplex, Google’s still experimental AI voice system and the venue was Google I/O, Google’s yearly developer conference, which this year focused heavily on the latest developments in AI, Machine- and Deep-Learning. It was easily the most remarkable human-computer conversation I’d ever heard and the closest thing I’ve seen a voice AI passing the Turing Test, which is the AI threshold suggested by Computer Scientist Alan Turing in the 1950s. What I heard was so convincing I had trouble discerning who was the salon worker and who (what) was the Duplex AI. Google Assistant running Duplex didn’t exhibit any of those short comings. Eventually, we’ll have our Duplex voices call each other, handling pleasantries and making plans, which Google Assistant can then drop in our Google Calendar. So, no, Duplex didn’t pass the Turing test, but I do wonder what Alan Turing would think of it. Eventually, the AI and the salon worker agreed on an appointment date and time. He launched Duplex by asking Google Assistant to book a haircut appointment for Tuesday morning. This conversation had a purpose, a destination: to make an appointment at a hair salon."
Sophia Arakelyan,From Ballerina to AI Researcher: Part I – buZZrobot,"I’ll dedicate the sequence of blog posts during the OpenAI Scholars program to several aspects of AI technology. Finding your true calling — the key component of happiness
My primary goal with the series of blog posts “From Ballerina to AI researcher” is to show that it’s never too late to embrace a new field, start over again, and find your true calling. I feel lucky that I found my true passion — AI. Founder of buZZrobot.com
The publication aims to cover practical aspects of AI technology, use cases along with interviews with notable people in the AI field. The transformer architecture reduced this problem thanks to the multi-head self-attention mechanism. To me, the technology itself and the AI community — researchers, scientists, people who dedicate their lives to building the most powerful technology of all time with the mission to benefit humanity and make it safe for us — is a great source of energy. The structure of the blog post series
Today, I’m giving an overall intro of what I’m going to cover in my “From Ballerina to AI Researcher” series. Last year, I published the article “From Ballerina to AI writer” where I described how I embraced the technical part of AI without a technical background. Recently, I’ve become a participant in the OpenAI Scholarship Program (OpenAI is a non-profit that gathers top AI researchers to ensure the safety of AI to benefit humanity). Every week for the next three months I’ll publish blog posts sharing my story of transformation from a person dedicated to 15 years of professional dancing and then writing about tech and AI to actually conducting AI research."
Matt Schlicht,The Complete Beginner’s Guide To Chatbots – Chatbots Magazine,"How do you build artificial intelligence into your bot? Don’t I need to be an expert at artificial intelligence to be able to build something that has artificial intelligence? No, you don’t have to be an expert at artificial intelligence to create an awesome chatbot that has artificial intelligence. Now that you’ve got your chatbot and artificial intelligence resources, maybe it’s time you met other people who are also interested in chatbots. Chatbots have been around for decades, but because of the recent advancements in artificial intelligence and machine learning, there is a big opportunity for people to create bots that are better, faster, and stronger. So, if these bots use artificial intelligence to make them work well... isn’t that really hard to do? What are chatbots? I’ll tell you why people care about chatbots. It’s potentially a huge business opportunity for anyone willing to jump headfirst and build something people want. How can I meet other people interested in chatbots?"
Gil Fewster,The mind-blowing AI announcement from Google that you probably missed.,"Up until September of last year, Google Translate used phrase-based translation. All that changed in September, when Google gave their translation tool a new engine: the Google Neural Machine Translation system (GNMT). Phrase-based translation has no capacity to make educated guesses at words it doesn’t recognize, and can’t learn from new input. Not only was the article competing with the pre-Christmas rush that most of us were navigating — it was also tucked away on Google’s Research Blog, beneath the geektastic headline Zero-Shot Translation with Google’s Multilingual Neural Machine Translation System. Here’s Google’s Mike Schuster, Nikhil Thorat, and Melvin Johnson from the original blog post:
Here you can see an advantage of Google’s new neural machine over the old phrase-based approach. A neural computing system designed to translate content from one human language into another developed its own internal language to make the task more efficient. Which is a shame, because it may just be the most astonishing article about machine learning that I read last year. Google Translate invented its own language to help it translate more effectively. The short version is that Google Translate got smart. Phrase-based translation is a blunt instrument."
Adam Geitgey,Machine Learning is Fun! Part 2 – Adam Geitgey – Medium,"Once I had all that data, I could use it to build a neural network to model how likely it is that you would start with any given letter. There just aren’t enough levels in the original Super Mario Brothers game to provide enough data for a really good model. I ended up with text files that looked like this:
Looking at the text file, you can see that Mario levels don’t really have much of a pattern if you read them line-by-line:
The patterns in a level really emerge when you think of the level as a series of columns:
So in order for the algorithm to find the patterns in our data, we need to feed the data in column-by-column. First, we need a data set for training our model. Because of this, it can model more cases than we could capture in one simple model. Let’s say I need to guess the next letter you are going to type at any point in your story. Can we use the same model that generated fake Hemingway text to generate fake Super Mario Brothers levels? With a lot more training, the model gets to the point where it generates perfectly valid data:
Let’s sample an entire level’s worth of data from our model and rotate it back horizontal:
This data looks great! Let’s use the first few words of Ernest Hemingway’s The Sun Also Rises as an example:
What letter is going to come next? But the one thing this kind of model can’t do is respond to patterns in data over time."
David Venturi,"Every single Machine Learning course on the internet, ranked by your reviews",It has a 4-star weighted average rating over 3 reviews. It has a 3.6-star weighted average rating over 5 reviews. It has a 4-star weighted average rating over 4 reviews. It has a 4.5-star weighted average rating over 6 reviews. It has a 2-star weighted average rating over 2 reviews. It has a 4.5-star weighted average rating over 2 reviews. Free and paid options available. Free and paid options available. Free and paid options available. Free and paid options available.
Michael Jordan,Artificial Intelligence — The Revolution Hasn’t Happened Yet,"As for the necessity argument, it is sometimes argued that the human-imitative AI aspiration subsumes IA and II aspirations, because a human-imitative AI system would not only be able to solve the classical problems of AI (as embodied, e.g., in the Turing test), but it would also be our best bet for solving IA and II problems. First, although one would not know it from reading the newspapers, success in human-imitative AI has in fact been limited — we are very far from realizing human-imitative AI aspirations. We need to solve IA and II problems on their own merits, not as a mere corollary to a human-imitative AI agenda. Whereas civil engineering and chemical engineering were built on physics and chemistry, this new engineering discipline will be built on ideas that the preceding century gave substance to — ideas such as “information,” “algorithm,” “data,” “uncertainty,” “computing,” “inference,” and “optimization.” Moreover, since much of the focus of the new discipline will be on data from and about humans, its development will require perspectives from the social sciences and humanities. One could argue that an AI system would not only imitate human intelligence, but also “correct” it, and would also scale to arbitrarily large problems. However, the current focus on doing AI research via the gathering of data, the deployment of “deep learning” infrastructure, and the demonstration of systems that mimic certain narrowly-defined human skills — with little in the way of emerging explanatory principles — tends to deflect attention from major open problems in classical AI. In the current era, we have a real opportunity to conceive of something historically new — a human-centric engineering discipline. It is those challenges that need to be in the forefront, and in such an effort a focus on human-imitative AI may be a distraction. The past two decades have seen major progress — in industry and academia — in a complementary aspiration to human-imitative AI that is often referred to as “Intelligence Augmentation” (IA). These are classical goals in human-imitative AI, but in the current hubbub over the “AI revolution,” it is easy to forget that they are not yet solved."
Milo Spencer-Harper,How to build a simple neural network in 9 lines of Python code,"First the neural network assigned itself random weights, then trained itself using the training set. But first, what is a neural network? We can model this process by creating a neural network on a computer. We built a simple neural network using Python! Then we begin the training process:
Eventually the weights of the neuron will reach an optimum for the training set. As part of my quest to learn about AI, I set myself the goal of building a simple neural network in Python. Here is a complete working example written in Python:
Also available here: https://github.com/miloharper/simple-neural-network
Final thoughts
Try running the neural network using this Terminal command:
python main.py
You should get a result that looks like:
We did it! So by substituting the first equation into the second, the final formula for the output of the neuron is:
You might have noticed that we’re not using a minimum firing threshold, to keep things simple. The first four examples are called a training set. To understand this last one, consider that:
The gradient of the Sigmoid curve, can be found by taking the derivative:
So by substituting the second equation into the first equation, the final formula for adjusting the weights is:
There are alternative formulae, which would allow the neuron to learn more quickly, but this one has the advantage of being fairly simple."
Greg Fish,looking for a ghost in the machine – [ weird things ],"According to him, this super-intellect would be smarter than any human mind in every capacity from the scientific to the creative. Technologically that should be possible, but the question is whether a machine like that would really be smarter than humans per se. While I’ve gotten plenty of feedback about how far technology has come so far and how it’s imminent that machines will become much smarter than us, I never got any specifics as to how exactly this would happen. So far, the closest thing to outlining the requirements for a super-intelligent computer is a paper by University of Oxford philosopher and futurist Nick Bostrom. What Bostrom calls a super-intellect is actually just a massive knowledge base that can mine itself for information. It would be far more knowledgeable than any individual human, granted. Those libraries seem a fair bit like Bostrom’s super-intellect in their function and if we were to combine them to mine their depths with sophisticated algorithms which look for cross-disciplinary potential, we’d bring his concept to life. Just like Bostrom says, it would be a very useful tool for scientists and researchers. In Bostrom’s projections, when you have an intelligent machine become fully proficient in a certain area of expertise like say, medicine, it could combine with another machine which has an excellent understanding of physics and so on until all this consolidation leads to a device that knows all that we know and can use all that cross-disciplinary knowledge to gain insights we just don’t have yet. A short while ago, I wrote about some of the challenges involved in creating artificial intelligence and raised the question of how exactly a machine would spontaneously attain self-awareness."
Oliver Lindberg,"Interview with Google’s Alfred Spector on voice search, hybrid intelligence and more","VP of research Alfred Spector talks to Oliver Lindberg about the technologies emerging from Google Labs — from voice search to hybrid intelligence and beyond
This article originally appeared in issue 198 of .net magazine in 2010 and was republished at www.techradar.com. One of the areas Google is making significant advances in is voice search. “We need to get computers and people communicating in both directions, so the computer learns from the human and makes the human more effective.”
Examples of ‘hybrid intelligence’ are Google Suggest, which instantly offers popular searches as you type a search query, and the ‘did you mean?’ feature in Google search, which corrects you when you misspell a query in the search bar. “Voice is one of these grand technology challenges in computer science,” Spector explains. “One idea is to take all of the voices that the system hears over time into one huge pan-human voice model. “Can a computer understand the human voice? Google’s a pretty good search engine, right? We hired one of the best database researchers in the world, Alon Halevy, to lead it.”
Google is aiming to make more information available more easily across multiple devices, whether it’s images, videos, speech or maps, no matter which language we’re using. It’s the most spoken language in the world, but as it isn’t exactly keyboard-friendly, voice search could become immensely popular in China. It’s a fundamentally challenging problem.”
One example of conceptual search is Google Image Swirl, which was added to Labs in November."
Greg Fish,the technical trouble with humanoid robots – [ weird things ],"It will take decades more to build self-repairing machines and computer chips that can boast the same performance as a supercomputer while being small enough to fit in human-sized robots’ heads before robotic butlers become practical and feasible. Instead, we got the standard Baby Boomers’ caretaker argument which goes somewhat like this...
Or, alternatively, a computer could book your appointments via e-mail, or a system that lets patients make an appointment with their doctors on the web, a smart dispenser that gives you the right amount of pills, checks for potential interactions based on public medical databases, and beeps to remind you to take your medicine, and a programmable walker with actuators and a few buttons could do these jobs while costing far less than the tens of millions a humanoid robot would cost by 2025, and requiring much less coordination or learning than a programmable humanoid. Humanoid machines would need to be constantly maintained just to keep up with us in a mechanical sense, and carry the equivalent of Red Storm in their heads, or at least be linked to something like it, to even hope to coordinate themselves as quickly as we do cognitively and physically. No, the reason why I’m not sure that humanoid robots will be invaluable to us in the future is a very pragmatic one. There’s no need to invoke the uncanny valley effect, even though some attempts to build humanoid robots managed to produce rather creepy entities which try to look as human as possible to goad future users into some kind of social bond with them, presumably to gain their trust and get into a perfect position to kill the inferior things made of flesh. If you’ve been reading this blog long enough, you may recall that I’m not a big fan of humanoid robots. While last month’s feature in Pop Sci bemoaned the lack of interest in humanoid robots in the U.S., it also failed to demonstrate why such an incredibly complicated machine would be needed for basic household chores that could be done by robotic systems functioning independently, and without the need to move on two legs. If anything, harping on the need for a robotic hand for Baby Boomers’ future medical woes would only prompt more R&D cash into immediate solutions and rules- based intelligent agents we already employ rather than long-term academic research. Why wouldn’t we want to pursue immediate fixes to what’s being described as a looming caretaker shortage choosing instead to invest billions of dollars into E-Jeeves, which may take an entire decade or two just to learn how to go about daily human life, ready to tackle the problem only after it was no longer an issue, even if we started right now? Not so much."
Frank Diana,The Evolving Role of Business Analytics – Frank Diana – Medium,"Business Analytics can then be viewed as the combination of domain knowledge and all forms of analytics in a way that creates analytic applications focused on enabling specific business outcomes. Business Analytics refers to the skills, technologies, applications and practices for the continuous exploration of data to gain insight that drive business decisions. We will see more organizations establish enterprise data management functions to coordinate data across business units. Organizations should first define the insights needed to meet business objectives, and then identify data that provides that insight. The analytics identified as creating the most value in 24 months are:
Companies and organizations continue to invest millions of dollars capturing, storing and maintaining all types of business data to drive sales and revenue, optimize operations, manage risk and ensure compliance. The key components of business analytics are:
There is a massive explosion of data occurring on a number of levels. New forms of advanced analytics are required to address the business imperatives described earlier. The survey results in a recent MIT Sloan report support both an aggressive adoption of analytics and a shift in the analytic footprint. This notion of turning data into insight, and insight to action is a common and growing theme. According to the report, many traditional forms of analytics will be surpassed in the next 24 months."
Paul Christiano,Formalizing indirect normativity – AI Alignment,"We must make a distinction between two possible sources of moral value: it could be the case that a U-maximizer carries out simulations on physical hardware in order to better understand U, and these simulations have moral value, or it could be the case that the hypothetical emulations themselves have moral value. Finally, the values of the simulations in this process may diverge from the values of the original human models, for one reaosn or another. This possibility is particularly troubling in light of the incentives our scheme creates — anyone who can manipulate H’s behavior can have a significant effect on the future of our world, and so many may be motivated to create simulations of H.
A realistic U-maximizer will not be able to carry out the process described in the definition of U–in fact, this process probably requires immensely more computing resources than are available in the universe. We can imagine many ways in which this process can fail to work as intended–the original brain emulations may accurately model human behavior, the original subject may deviate from the intended plans, or simulated humans can make an error when interacting with their virtual environment which causes the process to get hijacked by some unintended dynamic. That is, the output of this process is defined in terms of what a particular human would do, in a situation which that human knows will never come to pass. This process could be used to acquire the power necessary to define a utility function in one of the above frameworks, or understanding value-preserving self-modification or machine intelligence may itself prove an important ingredient in formalizing what it is we value. Therefore a U-maximizer will be sensitive to the possible suffering of simulations it runs while trying to learn about U–as long as it believes that we may might be concerned about the simulations’ welfare, upon reflection, it can rely as much as possible on approaches which do not involve running simulations, which deprive simulations of the first-person experience of discomfort, or which estimate outcomes by running simulations in more pleasant circumstances. Moreover, the community of humans being simulated in our process has access to a simulation of whatever U-maximizer is operating under this uncertainty, and has a detailed understanding of that uncertainty. Human cognition does not reside in a physical system with sharp boundaries, and it is not clear how you would define or use a simulation of the “input-output” behavior of such an object. In the second case, a U-maximizer in our world may have little ability to influence the welfare of hypothetical simulations invoked in the definition of U."
Robbie Tilton,Emotional Computing – Robbie Tilton – Medium,"If an increased computational IQ can allow a human to computer relationship to feel more like a human to human interaction, what would the advancement of computational EQ bring us? (A) was informed that (B) was an advanced computer chat-bot with the capacity to feel, understand, learn, and speak like a human. Investigating the human to computer relationship through reverse engineering the Turing test
Humans are getting closer to creating a computer with the ability to feel and think. In an experiment between a human and a human disguised as a computer, the Turing test is reverse engineered in order to understand the role computers will play as they become more adept to the processes of the human mind. Advances in computer IQ have been astonishing and have proved that machines are capable of answering difficult questions accurately, are able to hold a conversation with human-like understanding, and allow for emotional connections between a human and machine. (B) participants were asked if they would like to pursue a friendship with the person they chatted with. This also hits directly on Jefferson Lister’s quote, “Not until a machine can write a sonnet or compose a
concerto because of thoughts and emotions felt, and not by the chance fall of symbols, could we agree that machine equals brain-that is, not only write it but know that it had written it.”
Participants were given a chat-room simulation between two participants (A) a human interrogator and (B) a human disguised as a computer. The results also imply that humans aren’t really sure what they want out of Artificial Intelligence in the future and that we are not certain that an Affective computer would even enjoy a users company and/or conversation. In this paper we will uncover what it means for humans to interact with machines of greater intelligence and attempt to predict the future of human to computer interactions. (A) participants were asked if they felt (B)’s Artificial Intelligence could improve their relationship to computers if integrated in their daily products."
Wildcat2030,Becoming a Cyborg should be taken gently: Of Modern Bio-Paleo-Machines — Cyborgology,"But not yet. The discourse on subjectivity, not unlike the move from paleo to Neolithic societal structure, demands of us a re-assessment of the relations between man and machine. We give the machines a space to turn our dreams into reality; the machines in turn serve our needs and acquire sapience in the process. The modern Paleo-machines do not recognize borders; do not concern themselves with values and morality and do not philosophize about the meaning of it all, not yet that is. Connected and networked the machines follow in our footsteps, catalyzing our universality, providing for us in turn a meaning we cannot yet understand or realize. We can, if we manage to control our own paleo-urges to destroy ourselves, allow the combined interactive intelligence of man and machine to shine forth into a brighter future of expanded subjectivity. And yet, it evolves. We are on the edge of a Paleolithic Machine intelligence world. All this leads us to remember that only retrospectively do we recognize the move from the paleo tribes to the Neolithic status, we did not know that it happened then, and had no control over the motion, on the same token, we scarcely see the motion now and have no control over its directionality. As in our own Paleo past the needs of the machines do not yet contain passions for individuation, desire for emotional recognition or indeed feelings of dismay or despair, uncontrollable urges or dreams of far worlds."
Greg Fish,why you just can’t black box an a.i. – [ weird things ],"Conceiving of an AI in a black box is a good approach if we want to test how a particular system should react when working with the AI and focusing on the system we’re trying to test by mocking the AI’s responses down the chain of events. Secondly, and a lot more ominously, they believe that this system can sweep away humanity, not because it will be evil by nature but because it won’t care about humans or what happens to them and one of the biggest priorities for a researcher in the field should be figuring out how to build a friendly artificial intelligence, training it like one would train a pet, with a mix of operant conditioning and software. I say this because the only forms of intelligence we can readily identify are found in living things which use a brain to perform cognitive tasks, and since brains seem to be wired this way and we’re trying to emulate the basic functions of the brain, it wouldn’t be all that much of a stretch to assume that we’d want to combine systems good at related tasks and build on the accomplishments of existing systems. First and foremost, they say, it’s just a matter of time before we have an AI system that will quickly become superhumanly intelligent. So of what use is a black box AI here when we can just lay out the logical diagram and figure out how it’s making decisions and how we alter its cognitive process if need be? No problem there, say the Singularitarians, the system will be so advanced by the time this happens that we’ll be very unlikely to know exactly how it functions anyway. But by abstracting the AI away, what we’ve also done is made it impossible to test the inner workings of the AI system. Because if you read any of the papers on their version of friendly AI, yo’ll soon discover how quickly they begin to describe the system they’re trying to tame as a black box with mostly known inputs and measurable outputs, hardly a confident and lucid description of how an artificial intelligence functions, and ultimately, what rules will govern it. Beyond that, this block box is either a hindrance to a researcher or a vehicle for someone who doesn’t know how to build a synthetic mind but really, really wants to talk about what he imagines it will be like and how to harness its raw cognitive power. Singularitarians generally believe two things about artificial intelligence."
Greg Fish,why do we want to build a fully fledged a.g.i.? – [ weird things ],"Were we to build an AGI not by accident but by design, we would effectively be making the choice to experiment on a sapient entity and that’s something that may have to be cleared by an ethics committee, otherwise we’re implicitly saying that an artificial cognitive entity has no rights to self-determination. We can always suspend the model, debug it, and see what’s going on in its mind but again, the ethical considerations will play a significant part and very importantly, while we will get to know what such an AGI thinks and how, we may not know how it will first emerge. The whole AGI concept is a very ambiguous effort at defining intelligence and hence, doesn’t give us enough to objectively determine an intelligent artificial entity when we make one because we can always find an argument for and against how to interpret the results of an experiment meant to design one. But when it comes to such an abstract and all consuming technological experiment as AGI, the benefits seem to be very, very nebulous at best and the investment necessary seems extremely uncertain to pay off since we can’t even define what will make our AGI a true AGI rather than another example of a large expert system. Undoubtedly the most ambitious idea in the world of artificial intelligence is creating an entity comparable to a human in cognitive abilities, the so called AGI. We don’t know that. Not really since we would only be providing one example and a fairly controversial one at that. We barely even know where to start. We could debate how it may come about, whether it will want to be your friend or not, whether it will settle the metaphysical question of whet makes humans who they are or open new doors in the discussion, but for a second let’s think like software architects and ask the question we should always tackle first before designing anything. Maybe there’s a benefit to an AGI that I’m overlooking and if that is the case, enlighten me in the comments because this is a serious question."
James Faghmous ,New to Machine Learning? Avoid these three mistakes,"Data leakage occurs when the data you are using to learn a hypothesis happens to have the information you are trying to predict. Machine learning is a field of computer science where algorithms improve their performance at a certain task as more data are observed.To do so, algorithms select a hypothesis that best explains the data at hand with the hope that the hypothesis would generalize to future (unseen) data. The most basic form of data leakage would be to use the same data that we want to predict as input to our model (e.g. As the house’s size increases, so does its price in linear increments.” Now using this hypothesis, I can predict the price of an unseen datapoint based on its size. As the dimensions of the data increase, the hypotheses that explain the data become more complex.However, given that we are using a finite sample of observations to learn our hypothesis, finding an adequate hypothesis that generalizes to unseen data is nontrivial. If your model’s performance is poor on the newly collected data it may be a sign of data leakage. use the price of a house to predict the price of the same house). One way to spot data leakage is if you are doing very poorly on unseen independent data. The hypothesis selected by the algorithm (the blue curve) to explain the data is so complex that it fits through every single data point. The razor was violated when the hypothesis or model selected to describe the relationship between environmental data and seasonal hurricane counts was generated using a four-layer neural network."
Datafiniti,Classifying Websites with Neural Networks – Knowledge from Data: The Datafiniti Blog,"While it’s easy for you or me to realize that the above web page is selling some jeans, a computer would have a hard time making the distinction from the above page from either of the following web pages:
Or
Both of these pages share many similarities to the actual product page, but also have many key differences. Neural networks provide a structure for using the output of one set of input data to adjust A and B to the most likely best values for the next set of input data. We feel pretty good about our ability to classify and extract product data. In order to introduce more complex relationships in our data, we can introduce “hidden” layers in this model, which would end up looking something like:
For a more detailed explanation of neural networks, you can check out the following links:
In our product page classifier algorithm, we setup a neural network with 1 input layer with 27 nodes, 1 hidden layer with 25 nodes, and 1 output layer with 3 output nodes. In the meantime, we’re also working on classifying other types of pages, such as business data, company team pages, event data, and more.As we roll-out these classifiers and data extractors, we’re including each one in our crawl of the entire Internet. Neural networks are better-suited toward classification problems, and extracting data from a web page is a different type of problem. We also want to pull out the actual structured data! Our input layer modeled several features, including:
Our output layer had the following:
Our algorithm for the neural network took the following steps:
The ultimate output is two sets of input layers (T1 and T2), that we can use in a matrix equation to predict page type for any given web page. We don’t actually use neural networks for doing this. In order to see if we could use diameter and heights to determine poisonous-ness, we could set up the following equation:
A * (diameter) + B * (height) = 0 or 1 for not-poisonous / poisonous
We would then try various combinations of A and B for all possible diameters and heights until we found a combination that correctly determined poisonous-ness for as many mushrooms as possible."
Theo,Is there a future for innovation ? – Becoming Human: Artificial Intelligence Magazine,"And I started to worry about what the concept of innovation means for future generations. And so I started to worry about where the concept of innovation is going for future generations. And I started to worry about how the concept of innovation is being redefined for future generations. Innovation is a magical, crazy concept. And for the sake of future generations don’t let it. That’s where innovation is heading. The next generation expect innovation to happen at their fingertips with little to no real stimuli. And that breeds lazy innovation. Technology is taking away the power to think for ourselves and from our children. We’ve become satiated before we reach the point of real creativity, nobody wants to bother taking the time to put it all together themselves any more, it has to be ready for us."
x.ai,"I scheduled 1,019 meetings in 2012 — and that doesn’t count reschedules — x.ai","The number of meetings that I scheduled in 2012 might seem astronomical. More interesting though is the impact of this 1,019 figure, and a related one: Of those more than one thousand meetings I scheduled, 672 were rescheduled. I like the number though! As a startup romantic one could turn it into a nifty Malcolm Gladwell type rule of thumb called the “1,000 meetings rule.” Gladwell’s claim that greatness requires an enormous time sacrifice rings true to me — whether that means investing 10,000 hours into a subject matter to become an expert or conducting a 1,000 meetings per year, is another question. Magically schedule meetings * A meeting is defined as an event in my calendar, which is marginally flawed in both directions, given some events would be “Travel to JFK”, which is obviously a task and not a meeting, where others would be “Interview Sales Director Candidates“, which is really 4 meetings in 1. Originally published at x.ai on October 14, 2013. But these numbers were among the early pieces of data that inspired me to start x.ai. I was a startup-founder at the time, and that year my company, Visual Revenue, took Series-A funding, doubled revenue, and started discussing a possible exit. Put in context, it’s less so."
Arjan Haring 🔮🔨,Website morphing and more revolutions in marketing – Arjan Haring 🔮🔨 – Medium,"They know when to use simple decision rules (cognitive simplicity) and when to use more complicated rules. Morphing, identifying simple decision rules, and studying consumer foresight are all possible with the advent of good machine-learning methods. Actually, the companies that will thrive are those that understand the customers’ cognitive processes, have the algorithms to match products and marketing to customers’ cognitive processes, and have the organization that accepts such innovation. We can now identify these decision rules quickly with machine-learning methods. (1) Website morphing and banner morphing figure out how customers think and provides information in the format that helps them think the way they prefer to think. But a caveat — customers do not always use cognitively simple rules. He has served MIT as Head of the MIT Marketing Group, Head of the Management Science Area, Research Director of the Center for Innovation in Product Development, and co-Director of the International Center for Research on the Management of Technology.He is the co-author of two textbooks, Design and Marketing of New Products and Essentials of New Product Management, and a former editor of Marketing Science (now on the advisory board).I think it wouldn’t be smart to start this interview with something as dull and complex as a definition. What is new is that we now have good algorithms to identify how customers think from the choices they make as they explore websites (their clickstream). By understanding these simple rules, managers can develop better products and better marketing strategies. In our first application we matched the look and feel to customers’ cognitive styles."
Arjan Haring 🔮🔨,Using Artificial Intelligence to Balance Out Customer Value,"As long as AI is not used for old fashioned data manipulation or poor reporting, but really as intelligent data science, big data is one of the tools within ‘learning’ artificial intelligence. The focus should really not have to be on the data, but on the analysis — how we generate knowledge and learn from data through data mining — and more importantly, how do we operationalize this knowledge, how can we use this knowledge. I am fascinated by learning from data, but have mixed feelings about big data. Because: “Knowledge is not power, action is.”
And what is the role of psychology in big data? I am actually very curious what you, as a leading data scientist, think of this whole big data thingy. What could AI mean for business, and how is it different from Big Data? And philosophy? And of philosophy? In terms of wildest dreams: I heard a reunion concert of the Urban Dance Squad is not going to happen, which I understand, but I look forward to exchanging views with startups, freelancers and multinationals on how to create, with the help of raw data diamands and a magical mix of data mining, machine learning, decisioning and evidence-based and real-time marketing. Many of the “modern” big data technologies like Hadoop are in fact still limited frameworks for old-fashioned, offline batch processed data, instead of real-time processed data."
Shivon Zilis,The Current State of Machine Intelligence – Shivon Zilis – Medium,"Some have used this term interchangeably with machine learning and artificial intelligence, but I want to focus on the intelligence methods rather than data, storage, and computation pieces of the puzzle for this landscape (though of course data technologies enable machine intelligence). I mean “machine intelligence” as a unifying term for what others call machine learning and artificial intelligence. Which companies are on the landscape? All about machine intelligence for good. I would have preferred to avoid a different label but when I tried either “artificial intelligence” or “machine learning” both proved to too narrow: when I called it “artificial intelligence” too many people were distracted by whether certain companies were “true AI,” and when I called it “machine learning,” many thought I wasn’t doing justice to the more “AI-esque” like the various flavors of deep learning. Now we’re seeing a similar explosion of companies calling themselves artificial intelligence, machine learning, or somesuch — collectively I call these “machine intelligence” (I’ll get into the definitions in a second). The last wave of technology companies to IPO didn’t have demos that most of us would watch, so why should machine intelligence companies? We’re also investors in a few other machine intelligence companies that aren’t focusing on areas that were a fit for this landscape, so we left them off. (The 2016 Machine Intelligence landscape and post can be found here)
I spent the last three months learning about every artificial intelligence, machine learning, or data related startup I could find — my current list has 2,529 of them to be exact. ☺
I tried to pick companies that used machine intelligence methods as a defining part of their technology."
Roland Trimmel,Will All Musicians Become Robots? – Roland Trimmel – Medium,"Make no mistake, it’s really difficult to make a computer understand music — for us, this was an important first step towards a new generation of intelligent music instruments that assist the user in the songwriting process for faster completion of complex tasks resulting in no interruption of the creative flow and more creative output. Let me start with a quick discussion of the first and second digital wave in music:
The first digital wave brought about digital music technology like synths and DAW’s. Here are five things that we learned from our journey that we’d like to share with you so you can judge better before dismissing AI in music. We regard the main application of AI’s for music composition and production as helper tools, not artists in their own regard. It’s an interesting time for all of us in music and beyond, and there’s so much yet to come. As an example, AI’s can already help control the finishing mastering process of music tracks, as assistant tools, or even fully automated. And, occasionally we also had to calm down a heated discussion between members insulting each other caused by a fear that our product eliminates the craft in music composition. What was interesting for us to monitor is how the discussions about our product unfolded on those forums and how opinions were split between two camps: one that embraced what we do, and the other that was characterized by anger, fear, or a complete misunderstanding of what our software does. Most importantly though, it is not only AI changing the music industry. Finally we see the rise of the machines, and with it develops a certain fear that artificial intelligence (AI) will render humans useless."
Espen Waldal,How Artificial Intelligence can improve online news,"To face some of the most critical issues you need to create a better experience for the reader by:
1 Serving up better recommendations of related content2 Providing new ways to discover news and add context3 Introducing personalization and filtering
Relevance is essential to creating loyal readers, and even more so in a time where more and more visits to news sites go directly to a specific article, mainly due to search and social media, avoiding the front page altogether. Moreover, you can use the data to create new and compelling presentations of your content, including visualizations and timelines that give the reader a better experience and new insights. Rich structured data opens up for new ways to navigate and discover news. News content generally has a short lifespan, but this doesn’t mean that old content can’t be valuable in a new context. Rich structured data is the foundation for taking the online news experience to the next level. Structured data enables the reader to follow certain topics or stories, improves search and enables timeline navigation of a news story to help the reader better understand the context of the story and how it has developed. Orbit is a collection of artificial intelligence technology API’s using machine learning-based content analysis to automatically transform unstructured text into rich structured data. A consistent structuring of archived content will give new life to old content, making it easier to reuse and resurrect articles that are still relevant and create connections between old and new articles. With rich structured data in place, you can automatically add relevant fact boxes and other interactive elements to a piece of content, based on third party content databases such as Wikipedia. A foundation of rich structured data will not only benefit the reader, but make life easier for journalists and editors as well."
Joe Johnston,How I tracked my house movements using iBeacons. – Universal Mind – Medium,"Once you have this info you can set up your iBeacons using Launch Here. Launch Here allows you to use custom URL Schemes. Its a bit cumbersome to set each one up but you only have to do it once. Here’s my set up. The URL Scheme looks like this:
tumblr://x-callback-url/text?title=kitchen
Once that URL Scheme is triggered from Launch Here it opens Tumblr and pre-populates a text post with the word “kitchen”, or with the name of the room I set. The first set to arrive was from Roximity, which came to us as a set of 3 dev iBeacons. I have the Tumblr app installed on my phone which has the ablity to use a post URL Scheme. (As a side note the Launch Here app is a bit touchy when setting up the iBeacons so be warned. The first step was to find iBeacons we could use for our testing. This is a bit interesting, but it’s the approch that Launch Here took so they could give the user a bit of control when triggering actions."
Nadav Gur,Why Natural Search is Awesome and How We Got Here – The Vanguard – Medium,"With auto-suggest, you really want to inspire the user into adding something useful to their query, which means it needs to be relevant to whatever you know about the query and user so far, but not overwhelming for the user. At the same time — the auto-suggest / auto-complete elements we’ve built at this stage werealmost enough to allow us to just throw out the limiting “templates” and move to one search field — but this time, a damn clever one. We need a query that has at least a location + a type (or something from which we can derive a type), and without a template telling us that the “hotel” is the type, and the “restaurant” is something you want your hotel to have (vs. maybe the opposite), the system needs to better understand the grammatical structure or the sentence, and cue you into inputting things the right way when it’s suggesting and auto-completing. We said — first, let’s just put a search box in there, allowing the user to type or say whatever they want, and let’s make sure we understand this. So we came up with a UI that blends form-filling and natural language entry, and focused on building smart auto-suggest and auto-complete. Our goal was to get users inputting relevant, specific queries because that’s what people need. Typing a query:
The query is understood — you can add / edit:
With this new user interface, changing queries (“refining and pivoting”) is very natural — add tags, or take away tags. Because Desti is new and there haven’t been a million users searching for the same things before you, Desti should reason about what you may ask, not suggest something someone else asked. With auto-complete, you have a user who already thought of something to type in, and you have to guess what that is. We iterated a lot over the auto-complete and auto-suggest features."
Pandorabots,Using OOB Tags in AIML: Part I – pandorabots-blog – Medium,"The command that is to be executed is specified by a set of tags which occur within the <oob> tags. The <dial> tag within the <oob> tag sends a message to the phone to dial the number specified. To place a call you might see something like this: <oob><dial>some phone number</dial></oob>. The second category, those that do return information to the user via the chat interface, are generally actions that are executed in the background of the conversation. Be sure to look out for the upcoming post “Using OOB Tags in AIML: Part II”, which will go over a basic example of how to intrepret the OOB tags received from the Pandorabots server within the framework of your own VPA application. OOB tags are used in AIML templates and are written in the following format: <oob>command</oob>. It is useful to think of the activities initiated by oob tags as falling into one of two categories, based on whether they return information to the user via the chat interface or not. If you ask your Pandorabot to look up the “Population of the United States” on Wikipedia, it will perform the search, and then return the results of the search to the user via the chat window. OOB tags allow you to do just that! When your client indicates they want to dial a number, your application will receive a template containing the command specified inside the OOB tag."
Denny Vrandečić,"AI is coming, and it will be boring – Denny Vrandečić – Medium","AI will be a very powerful tool. So will AI. If you want to discuss consequences of AI, here are a few that are more realistic than human extermination: what will happen if AI makes many jobs obsolete? And as AI keeps developing, things once considered magical will become boring. And these things will become boring in a few years, if not months. What’s exciting today, will become boring tomorrow. Like every powerful tool, it will be highly disruptive. Whereas this is hard to deny, it is rather trivial: any sufficiently powerful tool could potentially spell the end of the human race given a person who knows how to use that tool in order to achieve such a goal. In summary: there are plenty of consequences of the development of AI that warrant intensive discussion (economical consequences, ethical decisions made by AIs, etc. The development of AI will be gradual, and so will the changes in our lifes."
Thaddeus Howze,Of Comets and Gods in the Making – Thaddeus Howze – Medium,"No longer would she shape the universe for them, they would have to work for their survival, perhaps they would be stronger for it. Of the Darkness, she could not remember, but she knew this: as long as there was light, her people would survive. The Darkness would always be ready to claim her people but now they would be scattered; to worlds within the galaxy and without. She would be the essence of Life itself; the Darkness be damned. Asferit would only be able to nudge a planet toward Life. She found the last star she would use and loaded the final probe-ship with the hardiest constructions she had ever made. To hide herself on millions of worlds, her final probe-ships would leave a legacy on millions of worlds. Each single cell would find a world ready for life. The final instructions to her probeship would have her descending into her planet’s unstable star. The planet below was also consumed, her last effort would require everything."
Tommy Thompson,Why AI Research Loves Pac-Man – Tommy Thompson – Medium,"Research in the original Pac-Man game caught the interest of the larger computational and artificial intelligence community. (Burrow, P. and Lucas, S.M., 2009) Evolution versus Temporal Difference Learning for Learning to Play Ms Pac-Man, Proceedings of the 2009 IEEE Symposium on Computational Intelligence and Games. You could argue it was due to the interesting problem that the game presents or that a game as notable as Pac-Man was now considered of interest within the AI research community. While the interest in Pac-Man and Ms. Pac-Man is beginning to dissipate, it has encouraged research that has provided significant contribution to artificial and computational intelligence in general. As high-quality research in AI applications in video games grew, it wasn’t long before those with a taste for Pac-Man research moved on to looking at Ms. Pac-Man given the challenges it presents — which we are still conducting research for in 2017. It’s notable here that the work by Lucas was in fact done on Ms. Pac-Man rather than Pac-Man. (Lucas, S.M.,2005) Evolving a Neural Network Location Evaluator to Play Ms. Pac-Man, Proceedings of the 2005 IEEE Symposium on Computational Intelligence and Games. Let’s discuss why Pac-Man is so important in the world of game-AI research. But why specifically Pac-Man? The deterministic behaviour of the ghosts in the original Pac-Man, while complex, can eventually be recognised by a human player."
Milo Spencer-Harper,How to build a simple neural network in 9 lines of Python code,"First the neural network assigned itself random weights, then trained itself using the training set. But first, what is a neural network? We can model this process by creating a neural network on a computer. We built a simple neural network using Python! Then we begin the training process:
Eventually the weights of the neuron will reach an optimum for the training set. As part of my quest to learn about AI, I set myself the goal of building a simple neural network in Python. Here is a complete working example written in Python:
Also available here: https://github.com/miloharper/simple-neural-network
Final thoughts
Try running the neural network using this Terminal command:
python main.py
You should get a result that looks like:
We did it! So by substituting the first equation into the second, the final formula for the output of the neuron is:
You might have noticed that we’re not using a minimum firing threshold, to keep things simple. The first four examples are called a training set. To understand this last one, consider that:
The gradient of the Sigmoid curve, can be found by taking the derivative:
So by substituting the second equation into the first equation, the final formula for adjusting the weights is:
There are alternative formulae, which would allow the neuron to learn more quickly, but this one has the advantage of being fairly simple."
Arik Sosman,Facebook M — The Anti-Turing Test – Arik’s Blog,"Thus, I asked M whether it could call them and figure that out. Whether it could call me (nope). So I asked M whether it could call my friends (nope). Thus, what we would be testing for is humans pretending to be an AI, which is much harder to test than the other way round, because it’s much easier for humans to pretend to be an AI than for an AI to pretend to be a human. The biggest issue with trying to prove whether or not M is an AI is that, contrary to other AIs that pretend to be human, M insists it’s an AI. To test its limit, I have asked it to perform a set of complicated tasks for me that no other AI out there could pull off. The opinion is split as to whether or not it’s a real AI, and there seems to be no way of proving its nature one way or the other. And indeed, that was not the only time it did:
While a lot of humans struggle with the distinction between “its” and “it’s,” for an AI, that should not have been an issue. The results and indications so far didn’t satisfy me, so I was still looking for a way to prove that there are real humans behind M. Just how could I make them come out, make them show themselves? Immediately afterward, the following exchange happens with M:
Unfortunately, I didn’t have a landline phone number, so I was a bit disappointed that not even this experiment could prove M’s nature."
Tony Aubé,No UI is the New UI – The Startup – Medium,"One could argue that conversational and invisible apps aren’t devoid of UI. If you don’t know about these apps, what make them special is that they don’t use a traditional UI as a mean of interaction. The new startups developing invisible and conversational apps understand this. As UI designers, we have a tendency to presume a UI is the solution to every new design problems. What if good design is about avoiding the screen altogether? While it is true that these apps do require UI design to some extent, I believe these are just the tip of the iceberg. In this world, digital-telepathy coupled with AI and other means of input could allow us to communicate directly with computer, without the need for a screen. And I believe a technological tiller is sticking an iPad screen over every new Internet-of-Things things. In a world where computer can see, listen, talk, understand and reply to you, what is the purpose of a user interface? What I do believe, however, is that these new technologies are going to fundamentally change how we approach design."
Matt O'Leary,I Let IBM’s Robot Chef Tell Me What to Cook for a Week,"that can tell us how to make a pizza out of cod, ginger and radishes that you know is going to taste amazing? So, with this in mind, I’m going to let Watson tell me what to eat for a week. Buoyed by yesterday’s Tailgating Salmon Sandwich success, I decided to give Watson something to sink its digital teeth into and supply only one ingredient: blood sausage. Wednesday: Diner Cod Pizza
When I read this recipe, I wondered whether this was going to be it for me and Watson. You can suggest one ingredient that you find in the fridge, use your initiative a bit and you’ll be left with something lovely. You may need to make a “do I want to put mashed potato on this lasagne?” leap of faith, and you’re going to have to actually go with it if you want the app’s full benefit. I want to see whether or not it can save me time in the kitchen; also, whether it has any amazing suggestions for dazzling taste matches; if it can help me use things up in the fridge; and whether or not it’s going to try to get me to buy a load of stuff I don’t really need. Although I’m not too sorry because, you know, it was actually a really good dish. Would I make it again? And, a bit concerningly, this is a recipe that Watson has extrapolated from one for Rye Porridge with Morels, replacing the rye with rice, the mushroom with sausage and the original’s chicken livers with a single potato and one tomato."
Tanay Jaipuria,Self-driving cars and the Trolley problem – Tanay Jaipuria – Medium,"Say a self-driving car which has one passenger in it, the “owner”, skids in the rain and is going to crash into a car in front, pushing that car off a cliff. Say for example a human-driven car runs a red light and a self-driving car has two options:
What should the car do? What should the car do? Would you still want the car to perform the action that simply saves the most people? But can you imagine a world in which say Google or Apple places a value on each of our lives, which could be used at any moment of time to turn a car into us to save others? It’s a famous thought experiment in philosophy called the Trolley Problem and goes as follows:
It’s not hard to see how a similar situation would come up in a world with self-driving cars, with the car having to make a similar decision. From a utilitarian perspective, the answer is obvious: to turn right (or “pull the lever”) leading to the death of only one person as opposed to five. Google recently announced that their self-driving car has driven more than a million miles. If self driving cars, were to follow them, we’re in a pretty good spot right? What if, going back to the example of the car, instead of a family of five, inside the car that ran the red light were five bank robbers speeding after robbing a bank."
Milo Spencer-Harper,How to build a multi-layered neural network in Python,"This layer enables the neural network to think about combinations of inputs. It is now possible for the neural network to discover correlations between the output of Layer 1 and the output in the training set. Also available here: https://github.com/miloharper/multi-layer-neural-network
This code is an adaptation from my previous neural network. But there is a direct relationship between combinations of pixels and apples. You can see from the diagram that the output of Layer 1 feeds into Layer 2. When the neural network calculates the error in layer 2, it propagates the error backwards to layer 1, adjusting the weights as it goes. The process of adding more layers to a neural network, so it can think about combinations, is called “deep learning”. There is no direct relationship between pixels and apples. The correct answer is 0. So the correct answer is 0."
Ben Brown,Start automating your business tasks with Slack – Howdy,"What happens if this process is automated using a “bot” in an environment like Slack? First, you’ll need to commit to adopting a tool like Slack where your team can communicate and use this type of bot. Traditionally, bots have been used for things like server maintenance and running software tests, but now, using the connected devices all around us, nearly anything can be automated and controlled by a bot. Add Howdy to your team to run meetings, capture information, and automate common tasks for your team. What other processes could be automated like this? Those of us who can create and wield this type of tool will be able to do better work faster. Using a flexible script, the bot simultaneously reaches out to every member of the team via a private message on Slack. The point I’m trying to make is that automating things like this exposes ways for the work to be improved, for time to be saved, and for the process to evolve. Robots can do lots of things at once — so once you’ve got your process documented, think about how the steps might be able to run in parallel. If there are 10 people on a team, and each person speaks for just 90 seconds, they’ll spend 15 minutes just bringing people up to speed."
Frank Diana,Digital Transformation of Business and Society – Frank Diana – Medium,"With regard to future thinking, Gerd used my future scenario slide to describe both the exponential and combinatorial nature of future scenarios — not only do we need to think exponentially, but we also need to think in a combinatorial manner. My take: Gerd is an evangelist for creating our future in a way that avoids hellish outcomes — and kudos to him for being that voice
“The best way to predict the future is to create it” (Alan Kay). My take: Future thinking is critical for us to be effective here. My take: These considerations allow us to create the future in a way that avoids unintended consequences. Here are some snapshots from his presentation:
Gerd then summarized the session as follows:
The future is exponential, combinatorial, and interdependent: the sooner we can adjust our thinking (lateral) the better we will be at designing our future. My take: Gerd hits on a key point. My Take: our context when we think about the future puts it years away, and that is just not the case anymore. As Gerd describes, he is a Futurist focused on foresight and observations — not predicting the future. He then described our current pivot point of exponential change: a point in history where humanity will change more in the next twenty years than in the previous 300. We are at a point in history where every company needs a Gerd Leonhard."
Rand Hindi,How Artificial Intelligence Will Make Technology Disappear,"This time, it is not 1, but 3 devices that are connected: a computer, a phone, and a tablet. In this era, called “Ubiquitous Computing”, adding new connected devices does not add friction, it actually adds value! The trouble is that since these devices don’t know which one you are currently using, the default strategy has been to push all notifications on all devices. Back in 1990, we didn’t have any connected devices. Taking the connected devices curve, and subtracting the one for A.I., we see that the overall friction keeps increasing over the next few years until the point where A.I. In fact, knowing the exact place we are at is essential to predict our intentions, since most of the things we do with our devices are based on what we are doing in the real world. For example, our phones and computers will be smart enough to know where to route the notifications. Building this ubiquitous computing future relies on giving devices the ability to sense and react to the current context, which is called “context-awareness”. Like when someone calls you on your phone, and it also rings on your computer, and actually keeps ringing after you’ve answered it on one of your devices! But in the evening, when we got back to the hotel and connected to the WiFi, our phones would immediately start pushing an entire day’s worth of notifications, constantly interrupting our special time together."
samim,Obama-RNN — Machine generated political speeches. – samim – Medium,Good bless the United States of America. Good bless the United States of America. God bless you. Thank you. Thank you. Good day. On this note: God bless you. Step 3 is to test the model which automatically generates an unlimited amount of new speeches in the vein of Obama ́s previous speeches. Here is a selection of some of my favorite speeches the Obama-RNN generated so far. All of Obama’s speeches are conveniently readable here.
Adam Geitgey,Machine Learning is Fun! Part 2 – Adam Geitgey – Medium,"Once I had all that data, I could use it to build a neural network to model how likely it is that you would start with any given letter. There just aren’t enough levels in the original Super Mario Brothers game to provide enough data for a really good model. I ended up with text files that looked like this:
Looking at the text file, you can see that Mario levels don’t really have much of a pattern if you read them line-by-line:
The patterns in a level really emerge when you think of the level as a series of columns:
So in order for the algorithm to find the patterns in our data, we need to feed the data in column-by-column. First, we need a data set for training our model. Because of this, it can model more cases than we could capture in one simple model. Let’s say I need to guess the next letter you are going to type at any point in your story. Can we use the same model that generated fake Hemingway text to generate fake Super Mario Brothers levels? With a lot more training, the model gets to the point where it generates perfectly valid data:
Let’s sample an entire level’s worth of data from our model and rotate it back horizontal:
This data looks great! Let’s use the first few words of Ernest Hemingway’s The Sun Also Rises as an example:
What letter is going to come next? But the one thing this kind of model can’t do is respond to patterns in data over time."
Arthur Juliani,Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks,"By updating in this way, the table slowly begins to obtain accurate measures of the expected future reward for a given action in a given state. We make updates to our Q-table using something called the Bellman equation, which states that the expected long-term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state. In it’s simplest implementation, Q-Learning is a table of values for every state (row) and action (column) possible in the environment. We instead need some way to take a description of our state, and produce Q-values for actions without a table: that is where neural networks come in. In equation form, the rule looks like this:
This says that the Q-value for a given state (s) and action (a) should represent the current reward (r) plus the maximum discounted (γ) future reward expected according to our own table for the next state (s’) we would end up in. Instead of starting with a complex and unwieldy deep neural network, we will begin by implementing a simple lookup-table version of the algorithm, and then show how to implement a neural-network equivalent using Tensorflow. In the case of the FrozenLake environment, we have 16 possible states (one for each block), and 4 possible actions (the four directions of movement), giving us a 16x4 table of Q-values. Within each cell of the table, we learn a value for how good it is to take a given action within a given state. There are a number of possible extensions to our simple Q-Network which allow for greater performance and more robust learning. Unlike policy gradient methods, which attempt to learn functions which directly map an observation to an action, Q-Learning attempts to learn the value of being in a given state, and taking a specific action there."
Adam Geitgey,Machine Learning is Fun Part 6: How to do Speech Recognition with Deep Learning,"To make this data easier for a neural network to process, we are going to break apart this complex sound wave into it’s component parts. Let’s use this sound clip of me saying “Hello”:
Sound waves are one-dimensional. If you know how neural machine translation works, you might guess that we could simply feed sound recordings into a neural network and train it to produce text:
That’s the holy grail of speech recognition with deep learning, but we aren’t quite there yet (at least at the time that I wrote this — I bet that we will be in a couple of years). It breaks apart the complex sound wave into the simple sound waves that make it up. A neural network can find patterns in this kind of data more easily than raw sound waves. We could feed these numbers right into a neural network. In Part 3, we learned how to take an image and treat it as an array of numbers so that we can feed directly into a neural network for image recognition:
But sound is transmitted as waves. Let’s learn how to do speech recognition with deep learning! Let’s zoom in on one tiny part of the sound wave and take a look:
To turn this sound wave into numbers, we just record of the height of the wave at equally-spaced points:
This is called sampling. The first step in speech recognition is obvious — we need to feed sound waves into a computer."
Adam Geitgey,Machine Learning is Fun Part 5: Language Translation with Deep Learning and the Magic of Sequences,"Take this possible translation:
It’s likely that no one has ever written a sentence like this in English, so it would not be very similar to any sentences in our data set. We could use our parallel corpora training data to train it to do that:
And just like that, we have a generic way of converting a sequence of English words into an equivalent sequence of Spanish words! We can come up with an encoding that represents every possible different sentence as a series of unique numbers:
To generate this encoding, we’ll feed the sentence into the RNN, one word at time. Here are some examples:
But in a real-world system, there will be even more possible chunk combinations because we’ll also try different orderings of words and different ways of chunking the sentence:
Now need to scan through all of these generated sentences to find the one that is that sounds the “most human.”
To do this, we compare each generated sentence to millions of real sentences from books and news stories written in English. For example, you can use it to predict the next most likely word in a sentence based on the first few words:
RNNs are useful any time you want to learn patterns in data. But look at this possible translation:
This sentence will be similar to something in our training set, so it will get a high probability score. Statistical machine translation systems perform much better than rule-based systems if you give them enough training data. Then they trained a sequence-to-sequence model where the employee’s question was the input sentence and the Tech Support team’s response was the “translation” of that sentence. Deep learning researchers who know almost nothing about language translation are throwing together relatively simple machine learning solutions that are beating the best expert-built language translation systems in the world. Building a statistics-based translation system requires lots of training data where the exact same text is translated into at least two languages."
Chris Dixon,Eleven Reasons To Be Excited About The Future of Technology,"Flying cars use the same advanced technology used in drones but are large enough to carry people. Self-driving cars exist today that are safer than human-driven cars in most driving conditions. History has shown that while new technology does indeed eliminate jobs, it also creates new and better jobs to replace them. Fortunately, a variety of new technologies are being developed to improve our food system. Most of the protocols we use today were developed decades ago by academia and government. Like previous gold rushes, this could lead to speculative excess, but also dramatically increased funding for new technologies and infrastructure. People sometimes think VR and AR will be used only for gaming, but over time they will be used for all sorts of activities. Today, human life expectancy is over 70 years, less that 10% of the global population lives in extreme poverty, and over 80% of people are literate. There are many exciting new technologies that will continue to transform the world and improve human welfare. Compared to traditional farms, automated indoor farms use roughly 10 times less water and land."
Tal Perry,Deep Learning the Stock Market – Tal Perry – Medium,"In Eidnes’ example, the output of the LSTMs is a vector that represents what the next word will be in some abstract space. In Karpathy’s example, the output of the LSTMs is a vector that represents the next character in some abstract representation. Going deeper
I want to point out that this is where we start to get into the deep part of deep learning. Lars inputs a sequence of Word Vectors and each one of them:
We’re going to do the same thing with one difference, instead of word vectors we’ll input “MarketVectors”, those market vectors we described before. Then we’ll get a sequence that looks like:
We want to take the vector that our LSTMs output and squish it so that it gives us the probability of the next item in our sequence being a 1. But, I want to achieve something specific with the first layer, the one that takes as input the 4000 dimensional vector and outputs a 300 dimensional one. Where Karpathy used characters, we’re going to use our market vectors and feed them into the magic black box. Now each output of the previous LSTM becomes the inputs of the next LSTM, and each one goes on to learn higher abstractions of the data coming in. So I’ll get back to the mechanics of that magic box in a second, but let me remind you that we want to predict the future market based on the past just like he predicted the next word based on the previous one. (Remember, a “word vector” is a representation of a word in the form of a bunch of numbers, like we saw in the beginning of this post)."
Gil Fewster,The mind-blowing AI announcement from Google that you probably missed.,"Up until September of last year, Google Translate used phrase-based translation. All that changed in September, when Google gave their translation tool a new engine: the Google Neural Machine Translation system (GNMT). Phrase-based translation has no capacity to make educated guesses at words it doesn’t recognize, and can’t learn from new input. Not only was the article competing with the pre-Christmas rush that most of us were navigating — it was also tucked away on Google’s Research Blog, beneath the geektastic headline Zero-Shot Translation with Google’s Multilingual Neural Machine Translation System. Here’s Google’s Mike Schuster, Nikhil Thorat, and Melvin Johnson from the original blog post:
Here you can see an advantage of Google’s new neural machine over the old phrase-based approach. A neural computing system designed to translate content from one human language into another developed its own internal language to make the task more efficient. Which is a shame, because it may just be the most astonishing article about machine learning that I read last year. Google Translate invented its own language to help it translate more effectively. The short version is that Google Translate got smart. Phrase-based translation is a blunt instrument."
David Venturi,"Every single Machine Learning course on the internet, ranked by your reviews",It has a 4-star weighted average rating over 3 reviews. It has a 3.6-star weighted average rating over 5 reviews. It has a 4-star weighted average rating over 4 reviews. It has a 4.5-star weighted average rating over 6 reviews. It has a 2-star weighted average rating over 2 reviews. It has a 4.5-star weighted average rating over 2 reviews. Free and paid options available. Free and paid options available. Free and paid options available. Free and paid options available.
Vishal Maini,A Beginner’s Guide to AI/ML 🤖👶 – Machine Learning for Humans – Medium,"Part 2.1: Supervised Learning. Demystifying artificial intelligence & machine learning. Machine learning is a subfield of artificial intelligence. Part 1: Why Machine Learning Matters. And now, without further ado, let’s dive into machine learning with Part 2.1: Supervised Learning! Q-learning, policy learning, and deep reinforcement learning. Part 5: Reinforcement Learning. Part 2.2: Supervised Learning II. Part 2.3: Supervised Learning III. Part 4: Neural Networks & Deep Learning."
Tim Anglade,"How HBO’s Silicon Valley built “Not Hotdog” with mobile TensorFlow, Keras & React Native","A neural network can only be as good as the data that trained it, and improving training set quality was probably one of the top 3 things we spent time on during this project. The HBO show Silicon Valley released a real AI app that identifies hotdogs — and not hotdogs — like the one shown on season 4’s 4th episode (the app is now available on Android as well as iOS!) There were 3 main factors:
For these reasons, we started experimenting with what’s trendily called “edge computing”, which for our purposes meant that after training our neural network on our laptop, we would export it and embed it directly into our mobile app, so that the neural network execution phase (or inference) would run directly inside the user’s phone. First The nature of our problem meant a strong imbalance in training data: there are many more examples of things that are not hotdogs, than things that are hotdogs. The problem directly ahead of us was simple: if Inception and VGG were too big, was there a simpler, pre-trained neural network we could retrain? While the network (Inception in this case) may have been trained on the 14M images contained in ImageNet, we were able to retrain it on a just a few thousand hotdog images to get drastically enhanced hotdog recognition. The key things we did to improve this were:
The final composition of our dataset was 150k images, of which only 3k were hotdogs: there are only so many hotdogs you can look at, but there are many not hotdogs to look at. After adding Batch Normalization and ELU to SqueezeNet, we were able to train neural network that achieve 90%+ accuracy when training from scratch, however, they were relatively brittle meaning the same network would overfit in some cases, or underfit in others when confronted to real-life testing. I could build it in one weekend!” This app probably feels a lot like that, and the initial prototype was indeed built in a single weekend using Google Cloud Platform’s Vision API, and React Native. Most AI apps will hit more critical cultural biases than ours, but as an example, even our straightforward use-case, caught us flat-footed with built-in biases in our initial dataset, that made the app unable to recognize French-style hotdogs, Asian hotdogs, and more oddities we did not have immediate personal experience with."
Sophia Ciocca,How Does Spotify Know You So Well? – Member Feature Stories – Medium,"To create Discover Weekly, there are three main types of recommendation models that Spotify employs:
Let’s dive into how each of these recommendation models work! Then, much like in collaborative filtering, the NLP model uses these terms and weights to create a vector representation of the song that can be used to determine if two pieces of music are similar. Each of these individuals has track preferences: the one on the left likes tracks P, Q, R, and S, while the one on the right likes tracks Q, R, S, and T.
Collaborative filtering then uses that data to say:
“Hmmm... You both like three of the same tracks — Q, R, and S — so you are probably similar users. First, some background: When people hear the words “collaborative filtering,” they generally think of Netflix, as it was one of the first companies to use this method to power a recommendation model, taking users’ star-based movie ratings to inform its understanding of which movies to recommend to other similar users. Each row represents one of Spotify’s 140 million users — if you use Spotify, you yourself are a row in this matrix — and each column represents one of the 30 million songs in Spotify’s database. Luckily, raw audio models don’t discriminate between new tracks and popular tracks, so with their help, your friend’s song could end up in a Discover Weekly playlist alongside popular songs! The second type of recommendation models that Spotify employs are Natural Language Processing (NLP) models. But this model also serves a secondary purpose: unlike the first two types, raw audio models take new songs into account. Let’s zoom out for a second to look at how other music services have tackled music recommendations, and how Spotify’s doing it better. To find out which users’ musical tastes are most similar to mine, collaborative filtering compares my vector with all of the other users’ vectors, ultimately spitting out which users are the closest matches."
François Chollet,The impossibility of intelligence explosion – François Chollet – Medium,"The intelligence of a human is specialized in the problem of being human. The basic premise of intelligence explosion — that a “seed AI” will arise, with greater-than-human problem solving ability, leading to a sudden, recursive, runaway intelligence improvement loop — is false. The intelligence explosion narrative equates intelligence with the general problem-solving ability displayed by individual intelligent agents — by current human brains, or future electronic brains. Similarly, an AI with a superhuman brain, dropped into a human body in our modern world, would likely not develop greater capabilities than a smart contemporary human. Crucially, the civilization-level intelligence-improving loop has only resulted in measurably linear progress in our problem-solving abilities over time. If the gears of your brain were the defining factor of your problem-solving ability, then those rare humans with IQs far outside the normal range of human intelligence would live lives far outside the scope of normal lives, would solve problems previously thought unsolvable, and would take over the world — just as some people fear smarter-than-human AI will do. What would happen if we were to put a human — brain and body — into an environment that does not feature human culture as we know it? The reasoning behind intelligence explosion, like many of the early theories about AI that arose in the 1960s and 1970s, is sophistic: it considers “intelligence” in a completely abstract way, disconnected from its context, and ignores available evidence about both intelligent systems and recursively self-improving systems. This seed AI would start designing better AIs, initiating a recursive self-improvement loop that would immediately leave human intelligence in the dust, overtaking it by orders of magnitude in a short time. However, these billions of brains, accumulating knowledge and developing external intelligent processes over thousand of years, implement a system — civilization — which may eventually lead to artificial brains with greater intelligence than that of a single human."
Max Pechyonkin,Understanding Hinton’s Capsule Networks. Part I: Intuition.,"Capsules introduce a new building block that can be used in deep learning to better model hierarchical relationships inside of internal knowledge representation of a neural network. In the same fashion, the idea of capsules itself is not that new and Hinton has mentioned it before, but there was no algorithm up until now to make it work. In addition to that, the team published an algorithm, called dynamic routing between capsules, that allows to train such a network. Hinton and his team proposed a way to train such a network made up of capsules and successfully trained it on a simple data set, achieving state-of-the-art performance. And the key idea is that representation of objects in the brain does not depend on view angle. This algorithm is called “dynamic routing between capsules”. Another benefit of the capsule approach is that it is capable of learning to achieve state-of-the art performance by only using a fraction of the data that a CNN would use (Hinton mentions this in his famous talk about what is wrongs with CNNs). They are one of the reasons deep learning is so popular today. Computer graphics deals with constructing a visual image from some internal hierarchical representation of geometric data. CNN approach to solve this issue is to use max pooling or successive convolutional layers that reduce spacial size of the data flowing through the network and therefore increase the “field of view” of higher layer’s neurons, thus allowing them to detect higher order features in a larger region of the input image."
Slav Ivanov,"The $1700 great Deep Learning box: Assembly, setup and benchmarks","We see that the GTX 1080 Ti is 2.4 times faster than the K80 on AWS P2 in training the model. We time training models on: AWS P2 instance GPU (K80), AWS P2 virtual CPU, the GTX 1080 Ti and Intel i5 7500 CPU. The 1080 Ti is 5.5 times faster that the AWS GPU (K80). And GTX 1080 Ti is about 30% faster than GTX 1080. The choice is between a few of Nvidia’s cards: GTX 1070, GTX 1070 Ti, GTX 1080, GTX 1080 Ti and finally the Titan X. The GTX 1080 Ti outperforms the AWS K80 by a factor of 4.3. The new GTX 1070 Ti is very close in performance to GTX 1080. The GTX 1080 Ti finishes 5.5x faster than the AWS P2 K80, which is in line with the previous results. On performance side: GTX 1080 Ti and Titan X are similar. Here are the things to consider when picking a GPU:
Considering all of this, I picked the GTX 1080 Ti, mainly for the training speed boost."
Geoff Nesnow,73 Mind-Blowing Implications of a Driverless Future,"There are so many technical, economic, safety advantages to the transportation-as-a-service that this change may come much faster than most people expect. Roads will be much emptier and smaller (over time) as self-driving cars need much less space between them (a major cause of traffic today), people will share vehicles more than today (carpooling), traffic flow will be better regulated and algorithmic timing (i.e. This too may become part of integrated, multi-modal transportation. Consumers will have more money as transportation (a major cost, especially for lower income people and families) gets much cheaper and ubiquitous — though this may be offset by dramatic reductions in employment as technology changes many times faster than people’s ability to adapt to new types of work
22. This includes drivers (which is in many states today the most common job), mechanics, gas station employees, most of the people who make cars and car parts or support those who do (due to huge consolidation of makers and supply chains and manufacturing automation), the marketing supply chain for vehicles, many people who work on and build roads/bridges, employees of vehicle insurance and financing companies (and their partners/suppliers), toll booth operators (most of whom have already been displaced), many employees of restaurants that support travelers, truck stops, retail workers and all the people whose businesses support these different types of companies and workers. Autonomous vehicles will radically change the power centers of the world. Cities will become much more dense as fewer roads and vehicles will be needed and transport will be cheaper and more available. Unmanned police vehicles may become more common and police officers may use commercial transportation to move around routinely. Vehicle designs will change radically — vehicles won’t need to withstand crashes in the same way, all vehicles will be electric (self-driving + software + service providers = all electric). Many additional on demand services will become available as transportation for goods and services becomes more ubiquitous and cheaper."
Blaise Aguera y Arcas,Do algorithms reveal sexual orientation or just expose our stereotypes?,"The results show that lesbians indeed use eyeshadow much less than straight women do, gay men and women do both wear glasses more, and young opposite-sex-attracted men are considerably more likely to have prominent facial hair than their gay or same-sex-attracted peers. However, asking the question “Do you like how you look in glasses?” reveals that this is likely more of a stylistic choice:
Same-sex attracted women also report wearing glasses more, as well as liking how they look in glasses more, across a range of ages:
One can also see how opposite-sex attracted women under the age of 40 wear contact lenses significantly more than same-sex attracted women, despite reporting that they have a vision defect at roughly the same rate, further illustrating how the difference is driven by an aesthetic preference: [4]
Similar analysis shows that young same-sex attracted men are much less likely to have hairy faces than opposite-sex attracted men (“serious facial hair” in our plots is defined as answering “yes” to having a goatee, beard, or moustache, but “no” to stubble). Overall, opposite-sex attracted men in our sample are 35% more likely to have serious facial hair than same-sex attracted men, and for men under the age of 31 (who are overrepresented on dating websites), this rises to 75%. In the following figures, we show the proportion of women who answer “yes” to “Do you ever use makeup?” (top) and “Do you wear eyeshadow?” (bottom), averaged over 6-year age intervals:
The blue curves represent strictly opposite-sex attracted women (a nearly identical set to those who answered “yes” to “Are you heterosexual or straight?”); the cyan curve represents women who answer “yes” to either or both of “Are you sexually attracted to women?” and “Are you romantically attracted to women?”; and the red curve represents women who answer “yes” to “Are you homosexual, gay or lesbian?”. [2] The patterns revealed here are intuitive; it won’t be breaking news to most that straight women tend to wear more makeup and eyeshadow than same-sex attracted and (even more so) lesbian-identifying women. This is consistent with a pattern of heterosexual men on average shooting from below, heterosexual women from above as the wedding photographer suggests, and gay men and lesbian women from directly in front. Our survey confirms that opposite-sex attracted men consistently self-report having a tan face (“Yes” to “Is your face tan?”) slightly more often than same-sex attracted men:
Once again Wang and Kosinski reach for a hormonal explanation, writing: “While the brightness of the facial image might be driven by many factors, previous research found that testosterone stimulates melanocyte structure and function leading to a darker skin”. The fact that we see a cohort of same-sex attracted men in their 40s who have just as much facial hair as opposite-sex attracted men suggests a different story, in which fashion trends and cultural norms play the dominant role in choices about facial hair among men, not differing exposure to hormones early in development. Composite images of the lesbian, gay, and straight men and women in the sample reveal a great deal about the information available to the algorithm:
Clearly there are differences between these four composite faces. That same-sex attracted men of most ages wear glasses significantly more than exclusively opposite-sex attracted men do might be a bit less obvious, but this trend is equally clear: [3]
A proponent of physiognomy might be tempted to guess that this is somehow related to differences in visual acuity between these populations of men."
François Chollet,What worries me about AI – François Chollet – Medium,"Remarkably, mass population manipulation — in particular political control — arising from placing AI algorithms in charge of our information diet does not necessarily require very advanced AI. You may be thinking, since a search engine is still an AI layer between us and the information we consume, could it bias its results to attempt to manipulate us? Increasingly, social network services are in control of what information we consume. In short, social network companies can simultaneously measure everything about us, and control the information we consume. And like many time in the past, we are worried that this new set of technologies will harm us — that AI will lead to mass unemployment, or that AI will gain an agency of its own, become superhuman, and choose to destroy us. As our lives become increasingly digital and connected, and as our world becomes increasingly information-intensive, we will need AI to serve as our interface to the world. In summary, our future is one where AI will be our interface to the world — a world made of digital information. This casts human behavior as an optimization problem, as an AI problem: it becomes possible for social media companies to iteratively tune their control vectors in order to achieve specific behaviors, just like a game AI would iterative refine its play strategy in order to beat a level, driven by score feedback. The human mind is a static, vulnerable system that will come increasingly under attack from ever-smarter AI algorithms that will simultaneously have a complete view of everything we do and believe, and complete control of the information we consume. At the same time, they gain increasing access to behavioral control vectors — in particular via algorithmic newsfeeds, which control our information consumption."
Simon Greenman,Who Is Going To Make Money In AI? Part I – Towards Data Science,"No wonder it is a brutal race for global AI algorithm dominance with Amazon — Microsoft — IBM also offering their own cheap or free AI software services. And there are few key themes appearing as to where the economic value will migrate:
In short it looks like the AI gold rush will favour the companies and countries with control and scale over the best AI tools and technology, the data, the best technical workers, the most customers and the strongest access to capital. Today there are over 200 AI powered companies just in the recruitment space, many of them AI startups. Data is the fuel for AI and machine learning. And the incredible AI machine learning software and algorithms that are powering all of Google’s AI activity — TensorFlow — is now being given away for free. And Microsoft is building its own Brainwave AI machine learning chips. We are in the midst of a gold rush in AI. But like any gold rush many startups will hit pay dirt. If millions of companies use this best in class free AI software then they are likely to need lots of computing power. However there is likely to be a period of upheaval when much of the value will go to those few companies and countries that control AI technology and data."
Aman Agarwal,Explained Simply: How an AI program mastered the ancient game of Go,"As you would remember, this network can directly take a board position and decide how an expert would play it — so you can use it to single-handedly play games.Well, the result was that the RL fine-tuned network won against the SL network that was only trained on human moves. Use a value function (Foma) to predict outcomes, and use a policy function (Lusha) to give you grand-master probabilities to help narrow down the moves you roll out. Now back to Foma — remember the “optimal value function”: v*(s) -> that only tells you how likely you are to win in your current board position if both players play perfectly from that point on?So obviously, to train an NN to become our value function, we would need a perfect player... which we don’t have. But in the case of the value function (which you would remember uses a strong player to approximate a perfect player), training Foma using the RL policy works better than training her with the SL policy. Once you have the SL network, trained in a supervised manner using human player moves with the human moves data, as I said before you have to let her practice by itself and get better. And this is why RL is so versatile; it can be used to train policy or value networks for any game, not just Go. After that, once you’ve trained both of the slow and fast policy networks enough using human player data, you can try letting Lusha play against herself on a Go board for a few days, and get more practice. To train the policy network (predicting for a given position which moves experts would pick), you simply use examples of human games and use them as data for good old supervised learning. They trained a “policy neural network” to decide which are the most sensible moves in a particular board position (so it’s like following an intuitive strategy to pick moves from any position). In a game like Chess or Go, as we said before, if you try to imagine even 7–8 moves into the future, there can be so many possible positions that you don’t have enough time to check all of them with Foma."
Lance Ulanoff,Did Google Duplex just pass the Turing Test? – Lance Ulanoff – Medium,"Duplex made the call and, when someone at the salon picked up, the voice AI started the conversation with:
“Hi, I’m calling to book a woman’s hair cut appointment for a client, um, I’m looking for something on May third?”
When the attendant asked Duplex to give her one second, Duplex responded with:
“Mmm-hmm.”
The conversation continued as the salon representative presented various dates and times and the AI asked about other options. The entity making the call and appointment was Google Assistant running Duplex, Google’s still experimental AI voice system and the venue was Google I/O, Google’s yearly developer conference, which this year focused heavily on the latest developments in AI, Machine- and Deep-Learning. It was easily the most remarkable human-computer conversation I’d ever heard and the closest thing I’ve seen a voice AI passing the Turing Test, which is the AI threshold suggested by Computer Scientist Alan Turing in the 1950s. What I heard was so convincing I had trouble discerning who was the salon worker and who (what) was the Duplex AI. Google Assistant running Duplex didn’t exhibit any of those short comings. Eventually, we’ll have our Duplex voices call each other, handling pleasantries and making plans, which Google Assistant can then drop in our Google Calendar. So, no, Duplex didn’t pass the Turing test, but I do wonder what Alan Turing would think of it. Eventually, the AI and the salon worker agreed on an appointment date and time. He launched Duplex by asking Google Assistant to book a haircut appointment for Tuesday morning. This conversation had a purpose, a destination: to make an appointment at a hair salon."
Gant Laborde,Machine Learning: how to go from Zero to Hero – freeCodeCamp,"If you want to do something new, not just new to you, but to the world, you can do it with ML. Even if you’re not a Java buff, the presentation Jim gives on all things Machine Learning is a pretty cool 1.5+ hour introduction into ML concepts, which includes more info on many of the examples above. It’s cool watching data go through a trained model, but you can even watch your neural network get trained. But more so, if you do make it through, you’ll have a deep understanding of the implementation of Machine Learning that will catapult you into successfully applying it in new and world-changing ways. So how does something like that even work? You can teach a computer to play video games, understand language, and even how to identify people or things. In this approach, you’ll understand Machine Learning down to the algorithms and the math. The only problem is we don’t know Machine Learning, and we don’t know how to hook it up to video games. One of the classic real-world examples of Machine Learning in action is the iris data set from 1936. I’m talking about Machine Learning."
Emmanuel Ameisen,Reinforcement Learning from scratch – Insight Data,"We learn our policy directly with policy gradients (defined above), and learn a value function using something called Advantage. While the value function tells us how good we estimate each action to be, the policy is the function that determines which actions we end up taking. 4+/Leveraging deep learning for representations
In practice, many state of the art RL methods require learning both a policy and value estimates. This allows us to learn a good value function. On-policy methods can only learn from actions that were taken following our policy (remember, a policy is the method we use to determine which actions to take). Previously, we were first learning a value function Q for each action in each state and then building a policy on top. The approach we will use here is called Policy Gradients, and is an on-policy method. We will use a method called Temporal Difference (TD) learning to learn a good Q function. In Vanilla Policy Gradient, we still use Monte Carlo Estimates, but we learn our policy directly through a loss function that increases the probability of choosing rewarding actions. A note about off-policy vs on-policy learning: The methods we used previously, are off-policy methods, meaning we can generate data with any strategy(using epsilon greedy for example) and learn from it."
Irhum Shafkat,Intuitively Understanding Convolutions for Deep Learning,"Each filter in a convolution layer produces one and only one output channel, and they do it like so:
Each of the kernels of the filter “slides” over their respective input channels, producing a processed version of each. Even if were we to apply a kernel of the same size (3×3), having the same local area, to the output of the strided convolution, the kernel would have a larger effective receptive field:
This is because the output of the strided layer still does represent the same image. If this were a standard fully connected layer, you’d have a weight matrix of 25×9 = 225 parameters, with every output feature being the weighted sum of every single input feature. The output features are essentially, the weighted sums (with the weights being the values of the kernel itself) of the input features located roughly in the same location of the output pixel on the input layer. Each filter actually happens to be a collection of kernels, with there being one kernel for every single input channel to the layer, and each kernel being unique. Convolutions allow us to do this transformation with only 9 parameters, with each output feature, instead of “looking at” every input feature, only getting to “look” at input features coming from roughly the same location. And with the single filter case down, the case for any number of filters is identical: Each filter processes the input with its own, different set of kernels and a scalar bias with the process described above, producing a single output channel. Followed by a final pooling layer, which collapses each 7×7 grid into a single pixel, each channel is a feature detector with a receptive field equivalent to the entire image. Both are methods of increasing the receptive field, but dilated convolutions are a single layer, while this takes place on a regular convolution following a strided convolution, with a nonlinearity inbetween)
This expansion of the receptive field allows the convolution layers to combine the low level features (lines, edges), into higher level features (curves, textures), as we see in the mixed3a layer. The kernels of a filter each produce one version of each channel, and the filter as a whole produces one overall output channel."
Abhishek Parbhakar,Must know Information Theory concepts in Deep Learning (AI),"Cross entropy between two probability distributions p and q defined over same set of outcomes is given by:
Mutual information is a measure of mutual dependency between two probability distributions or random variables. The probability distribution of experiment is used to calculate the entropy. Mutual information of two discrete random variables X and Y is defined as:
where p(x,y) is the joint probability distribution of X and Y, and p(x) and p(y) are the marginal probability distribution of X and Y respectively. Cross entropy is used to compare two probability distributions. KL divergence between ‘P’ and ‘Q’ tells us how much information we lose when we try to approximate data given by ‘P’ with ‘Q’. Entropy gives a measure of uncertainty in an experiment. KL divergence of a probability distribution Q from another probability distribution P is defined as:
KL divergence is commonly used in unsupervised machine learning technique Variational Autoencoders. For example, in an deterministic experiment, we always know the outcome, so no new information gained is here from observing the outcome and hence entropy is zero. The information gained for a outcome of an experiment is defined as a function of probability of occurrence of that outcome. KL divergence is another measure to find similarities between two probability distributions."
Aman Dalmia,What I learned from interviewing at multiple AI companies and start-ups,"Also, when I was preparing, I noticed people using a lot of resources but as per my experience over the past months, I realised that one can do away with a few minimal ones for most roles in AI, all of which I’m going to mention at the end of the post. There are mostly two types of interviews — one, where the interviewer has come with come prepared set of questions and is going to just ask you just that irrespective of your profile and the second, where the interview is based on your CV. Now we come to the conclusion of the interview where the interviewer would ask you if you have any questions for them. People DO go through your Github because that’s the only way they have to validate what you have mentioned in your CV, given that there’s a lot of noise today with people associating all kinds of buzzwords with their profile. The motive of this kind of interview is to really check whether whatever you have written on your CV is true or not:
There would be a lot of questions on what could be done differently or if “X” was used instead of “Y”, what would have happened. This post arose from a discussion with one of my juniors on the lack of really fulfilling job opportunities offered through campus placements for people working in AI. Also, you won’t need to prepare answers to various kinds of questions that you get asked during an interview. It’s really easy to think that your interview is done and just say that you have nothing to ask. The resources mentioned in the Minimal Resources you need for preparation section should suffice, but make sure that you don’t miss out one bit among them. In the process, not only did I get an opportunity to interact with many great minds, but also had a peek at myself along with a sense of what people really look for when interviewing someone."
Gaurav Kaila,How to easily automate Drone-based monitoring using Deep Learning,"Model training: At Nanonets we employ the principle of Transfer Learning while training on your images. This article is a comprehensive overview of using deep learning based object detection methods for aerial imagery via drones. This involves re-training a pre-trained model that has already been pre-trained with a large number of aerial images. NanoNets is a web service that makes it easy to use Deep Learning. Test & Integrate: Once the model is trained, you can either integrate Nanonet’s API directly into your system or we also provide a docker image with the trained model and inference code that you can use. We explore some of these applications along with challenges in automation of drone-based monitoring through deep learning. Australia-based Westpac Group has developed a deep learning based object detection system to detect sharks in the water. Finally, a case-study is presented for automating remote inspection of construction projects in Africa using Nanonets machine learning framework. These drones have high resolution cameras attached to them that are capable of acquiring quality images which can be used for various kinds of analysis. We aim to detect the following infrastructure to capture the construction progress of a house in it’s various stages : a foundation (start), wallplate (in-progress), roof (partially complete), apron (finishing touches) and geyser (ready-to-move in)
Pragmatic Master chose Nanonets as it’s deep learning provider because of it’s easy-to-use web platform and plug&play APIs."
James Loy,How to build your own Neural Network from scratch in Python,"However, we can’t directly calculate the derivative of the loss function with respect to the weights and biases because the equation of the loss function does not contain the weights and biases. As we’ve seen in the sequential graph above, feedforward is just simple calculus and for a basic 2-layer neural network, the output of the Neural Network is:
Let’s add a feedforward function in our python code to do exactly that. Training the Neural Network
The output ŷ of a simple 2-layer Neural Network is:
You might notice that in the equation above, the weights W and the biases b are the only variables that affects the output ŷ. The process of fine-tuning the weights and biases from the input data is known as training the Neural Network. That was ugly but it allows us to get what we needed — the derivative (slope) of the loss function with respect to the weights, so that we can adjust the weights accordingly. In order to know the appropriate amount to adjust the weights and biases by, we need to know the derivative of the loss function with respect to the weights and biases. Our Neural Network should learn the ideal set of weights to represent this function. The Loss Function allows us to do exactly that. Neural Networks consist of the following components
The diagram below shows the architecture of a 2-layer Neural Network (note that the input layer is typically excluded when counting the number of layers in a Neural Network)
Creating a Neural Network class in Python is easy. Now that we have our complete python code for doing feedforward and backpropagation, let’s apply our Neural Network on an example and see how well it does."
Chintan Trivedi,Using Deep Q-Learning in FIFA 18 to perfect the art of free-kicks,"We will use this as the training data for our Q-Learning model. A code tutorial in Tensorflow that uses Reinforcement Learning to take free kicks. Contrary to Supervised Learning, we do not need to manually label the training data in Reinforcement Learning. Now that we can interact with the game and store our interactions in memory, let’s start training our Q-Learning model. In my previous article, I presented an AI bot trained to play the game of FIFA using Supervised Learning technique. We’ll use game’s screenshots to observe the state, simulated key-presses to take action in the game environment and Optical Character Recognition to read our reward in the game. Let’s start with understanding the Reinforcement Learning technique and how we can formulate our free kick problem to fit this technique. Deep Q-learning is a special type of Reinforcement Learning technique where the Q-function is learnt by a deep neural network. Thus, for the current state s, we will try to estimate out of all actions possible which action will fetch us the maximum immediate + future reward, denoted by Q(s,a) called the Q-function. For this, we will attain a balance between exploration (taking a random action in the game) and exploitation (taking action predicted by our model)."
Abhishek Parbhakar,Why Data Scientists love Gaussian? – Towards Data Science,"Even when they don’t, the Gaussian gives the best model approximation for these processes. Unlike many other distribution that changes their nature on transformation, a Gaussian tends to remain a Gaussian. For every Gaussian model approximation, there may exist a complex multi-parameter distribution that gives better approximation. Some examples include-
Central limit theorem states that when we add large number of independent random variables, irrespective of the original distribution of these variables, their normalized sum tends towards a Gaussian distribution. For example, the distribution of total distance covered in an random walk tends towards a Gaussian probability distribution. Incredible number of processes in nature and social sciences naturally follows the Gaussian distribution. The theorem can also been seen as a explanation why many natural phenomena follow Gaussian distribution. Gaussian distribution model, often identified with its iconic bell shaped curve, also referred as Normal distribution, is so popular mainly because of three reasons. Even if you have never worked on an AI project, there is a significant chance that you have come across the Gaussian model. For Deep Learning & Machine Learning engineers out of all the probabilistic models in the world, Gaussian distribution model simply stands out."
Leon Zhou,The Best Words – Towards Data Science,"Let me illustrate with a quick example:
Donald Trump says the word “taxes.” If, in real life, 70% of the time after he says “taxes” he follows up with the word “bigly,” the Markov chain will choose the next word to be “bigly” 70% of the time. The quick and dirty of the Markov chain is that it only cares about the current word in determining what should come next. As the Markov chain only ever cares about the current word, it can easily be sidetracked. It was this unique style that interested me, and I set out to try and capture it using machine learning — to generate text that looked and sounded like something Donald Trump might say. With my limited text data set, most of my Markov chain outputs were nonsensical. This algorithm looks at every single time a specific word appears, and every word that comes immediately after it. The chain will most likely choose “bigly,” but there’s a chance it’ll go for any of the other available options, thus introducing some variety in our generated text. Markov chains have been the go-to for joke text generation for a long time — a quick search will reveal ones for Star Trek, past presidents, the Simpsons, and many others. The obvious first place to look for words by Donald Trump was his Twitter feed. To learn President Trump’s style, I first had to gather sufficient examples of it."
Dr. GP Pulipaka,"3 Ways to Apply Latent Semantic Analysis on Large-Corpus Text on macOS Terminal, JupyterLab, and...","Natural language processing in Python with NLTK library applies a low-rank approximation to the term-document matrix. Latent semantic analysis is a technique that applies singular value decomposition and principal component analysis (PCA). Matrix decompositions and latent semantic indexing. The latent semantic analysis is a particular technique in semantic space to parse through the document and identify the words with polysemy with NLKT library. The document can be represented with Z x Y Matrix A, the rows of the matrix represent the document in the collection. An Introduction to Latent Semantic Analysis. Mounting Google Drive on Google Colab. Once the drive is mounted, Colab has access to the datasets from Google drive. Latent semantic analysis
Applying latent semantic analysis on large datasets of text and documents represents the contextual meaning through mathematical and statistical computation methods on large corpus of text. Later, the low-rank approximation aids in indexing and retrieving the document known as latent semantic indexing by clustering the number of words in the document."
Erick Muzart Fonseca dos Santos,O Grupo de Estudo em Deep Learning de Brasília está planejando o próximo ciclo de encontros do...,"O Grupo de Estudo em Deep Learning de Brasília está planejando o próximo ciclo de encontros do grupo, que deve iniciar-se a partir do meio de junho de 2018. Publicações dos membros do Grupo de Estudo em Deep Learning de Brasília Para tal, favor preencher o seguinte questionário para que possamos agregar as preferências de nossa comunidade e selecionar as opções que melhor atenderem a todos: https://goo.gl/forms/H4K77sD1DxW6diIt1
Agradecemos se puder divulgar o grupo junto a sua rede de contatos com interesse nos temas de aprendizado automático e Deep Learning, para que possamos iniciar o próximo ciclo já com o máximo de interessados desde o primeiro dia! Seguem abaixo alguns dos resultados iniciais do grupo:
Quanto aos resultados inciais do questionário, segue uma síntese das primeiras 50 respostas:
Dentre os tópicos de mais interesse destacam-se:
1o Deep Learning: 87,5%
2o Machine Learning: 78,6%
3o Aplicações de Deep Learning em projetos: 69,6%
4o Processamento de Linguagem Natural: 51,8%
Preferência por curso:
1o Machine Learning, da fast.ai: 67,9%
2o Deep Learning, parte 2, da fast.ai: 46,4%
3o Deep Learning, parte 1, da fast.ai: 44,6%
Atenciosamente,
Organização do Grupo de Estudo em Deep Learning, de Brasília
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story. Ainda há tempo para manifestar suas preferências para participar do grupo!"
Chris Kalahiki,"Beethoven, Picasso, and Artificial Intelligence – Towards Data Science","One path that may be taken to further improve art and music through AI is to create more advanced datasets to use in training the complex networks like Sketch-RNN and Deep Dream. Those neural networks are then used to create art-like images. Another interesting project in the art space is Google’s Deep Dream project, which uses AI to create new and unique images. We will also look at ways art and music could continue to impact AI in the years to come. But is that the limit to what artificial intelligence can do in the field of art and music? As we look ahead to a few more of the ways that AI has been used to accomplish new and innovative ideas in the art space, we look at projects like Quick, Draw! Projects like Magenta go above and beyond in showing us the full extent of what artificial intelligence can do in the way of generating music. Perhaps one day we will achieve artificial general intelligence and machines will be able to understand what is really in the images it is given. This image recognition is a very practical use of artificial intelligence in the realm of art and music. Images aren’t the only type of art that artificial intelligence can impact though."
Adam Geitgey,Machine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networks,"But right now, our neural network can’t do this. But now we want to process images with our neural network. To a computer, an image is really just a grid of numbers that represent how dark each pixel is:
To feed an image into our neural network, we simply treat the 18x18 pixel image as an array of 324 numbers:
The handle 324 inputs, we’ll just enlarge our neural network to have 324 input nodes:
Notice that our neural network also has two outputs now (instead of just one). We need to be smarter about how we process images into our neural network. How in the world do we feed images into a neural network instead of just numbers? Instead of feeding entire images into our neural network as one grid of numbers, we’re going to do something a lot smarter that takes advantage of the idea that an object is the same no matter where it appears in a picture. We created a small neural network to estimate the price of a house based on how many bedrooms it had, how big it was, and which neighborhood it was in:
We also know that the idea of machine learning is that the same generic algorithms can be reused with different data to solve different problems. A neural network takes numbers as input. Now that we have a trained neural network, we can use it! Our program can now recognize birds in images!"
Adam Geitgey,Machine Learning is Fun! Part 4: Modern Face Recognition with Deep Learning,"The end result is we turn the original image into a very simple representation that captures the basic structure of a face in a simple way:
To find faces in this HOG image, all we have to do is find the part of our image that looks the most similar to a known HOG pattern that was extracted from a bunch of other training faces:
Using this technique, we can now easily find faces in any image:
If you want to try this step out yourself using Python and dlib, here’s code showing how to generate and view HOG representations of images. So all we need to do ourselves is run our face images through their pre-trained network to get the 128 measurements for each face. For each step, we’ll learn about a different machine learning algorithm. To find faces in an image, we’ll start by making our image black and white because we don’t need color data to find faces:
Then we’ll look at every single pixel in our image one at a time. Then we could measure our unknown face the same way and find the known face with the closest measurements. But instead of training the network to recognize pictures objects like we did last time, we are going to train it to generate 128 measurements for each face. We need to build a pipeline where we solve each step of face recognition separately and pass the result of the current step to the next step. Machine learning people call the 128 measurements of each face an embedding. The simplest approach to face recognition is to directly compare the unknown face we found in Step 2 with all the pictures we have of people that have already been tagged. It turns out that the measurements that seem obvious to us humans (like eye color) don’t really make sense to a computer looking at individual pixels in an image."
Arthur Juliani,Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks,"By updating in this way, the table slowly begins to obtain accurate measures of the expected future reward for a given action in a given state. We make updates to our Q-table using something called the Bellman equation, which states that the expected long-term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state. In it’s simplest implementation, Q-Learning is a table of values for every state (row) and action (column) possible in the environment. We instead need some way to take a description of our state, and produce Q-values for actions without a table: that is where neural networks come in. In equation form, the rule looks like this:
This says that the Q-value for a given state (s) and action (a) should represent the current reward (r) plus the maximum discounted (γ) future reward expected according to our own table for the next state (s’) we would end up in. Instead of starting with a complex and unwieldy deep neural network, we will begin by implementing a simple lookup-table version of the algorithm, and then show how to implement a neural-network equivalent using Tensorflow. In the case of the FrozenLake environment, we have 16 possible states (one for each block), and 4 possible actions (the four directions of movement), giving us a 16x4 table of Q-values. Within each cell of the table, we learn a value for how good it is to take a given action within a given state. There are a number of possible extensions to our simple Q-Network which allow for greater performance and more robust learning. Unlike policy gradient methods, which attempt to learn functions which directly map an observation to an action, Q-Learning attempts to learn the value of being in a given state, and taking a specific action there."
Dhruv Parthasarathy,A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN,"In the image above, you can see how a single CNN is used to both carry out region proposals and classification. With these anchor boxes in mind, let’s take a look at the inputs and outputs to this Region Proposal Network:
We then pass each such bounding box that is likely to be an object into Fast R-CNN to generate a classification and tightened bounding boxes. The insight of Faster R-CNN was that region proposals depended on features of the image that were already calculated with the forward pass of the CNN (first step of classification). Fast R-CNN Insight 1: RoI (Region of Interest) Pooling
For the forward pass of the CNN, Girshick realized that for each image, a lot of proposed regions for the image invariably overlapped causing us to run the same CNN computation again and again (~2000 times!). The authors write:
Here are the inputs and outputs of their model:
How the Regions are Generated
Let’s take a moment to see how Faster R-CNN generates these region proposals from CNN features. Where earlier we had different models to extract image features (CNN), classify (SVM), and tighten bounding boxes (regressor), Fast R-CNN instead used a single network to compute all three. Understanding R-CNN
The goal of R-CNN is to take in an image, and correctly identify where the main objects (via a bounding box) in the image. In the image above, notice how the CNN features for each region are obtained by selecting a corresponding region from the CNN’s feature map. In particular, we’ll cover R-CNN (Regional CNN), the original application of CNNs to this problem, along with its descendants Fast R-CNN, and Faster R-CNN. Much like Fast R-CNN, and Faster R-CNN, Mask R-CNN’s underlying intuition is straight forward."
Sebastian Heinz,A simple deep learning model for stock price prediction using TensorFlow,"We need two placeholders in order to fit our model: X contains the network's inputs (the stock prices of all S&P 500 constituents at time T = t) and Y the network's outputs (the index value of the S&P 500 at time T = t + 1). Also our data science consultants at STATWORX are heavily using TensorFlow for deep learning and neural net research and development. The dataset was split into training and test data. During minibatch training random data samples of n = batch_size are drawn from the training data and fed into the network. The cost function of the network is used to generate a measure of deviation between the network’s predictions and the actual observed training targets. Initializers are used to initialize the network’s variables before training. Other network architectures, such as recurrent neural networks, also allow data flowing “backwards” in the network. After having defined the placeholders, variables, initializers, cost functions and optimizers of the network, the model needs to be trained. Since neural networks are actually graphs of data and mathematical operations, TensorFlow is just perfect for neural networks and deep learning. The user defines an abstract representation of the model (neural network) through placeholders and variables."
Max Pechyonkin,Understanding Hinton’s Capsule Networks. Part I: Intuition.,"Capsules introduce a new building block that can be used in deep learning to better model hierarchical relationships inside of internal knowledge representation of a neural network. In the same fashion, the idea of capsules itself is not that new and Hinton has mentioned it before, but there was no algorithm up until now to make it work. In addition to that, the team published an algorithm, called dynamic routing between capsules, that allows to train such a network. Hinton and his team proposed a way to train such a network made up of capsules and successfully trained it on a simple data set, achieving state-of-the-art performance. And the key idea is that representation of objects in the brain does not depend on view angle. This algorithm is called “dynamic routing between capsules”. Another benefit of the capsule approach is that it is capable of learning to achieve state-of-the art performance by only using a fraction of the data that a CNN would use (Hinton mentions this in his famous talk about what is wrongs with CNNs). They are one of the reasons deep learning is so popular today. Computer graphics deals with constructing a visual image from some internal hierarchical representation of geometric data. CNN approach to solve this issue is to use max pooling or successive convolutional layers that reduce spacial size of the data flowing through the network and therefore increase the “field of view” of higher layer’s neurons, thus allowing them to detect higher order features in a larger region of the input image."
Slav Ivanov,"The $1700 great Deep Learning box: Assembly, setup and benchmarks","We see that the GTX 1080 Ti is 2.4 times faster than the K80 on AWS P2 in training the model. We time training models on: AWS P2 instance GPU (K80), AWS P2 virtual CPU, the GTX 1080 Ti and Intel i5 7500 CPU. The 1080 Ti is 5.5 times faster that the AWS GPU (K80). And GTX 1080 Ti is about 30% faster than GTX 1080. The choice is between a few of Nvidia’s cards: GTX 1070, GTX 1070 Ti, GTX 1080, GTX 1080 Ti and finally the Titan X. The GTX 1080 Ti outperforms the AWS K80 by a factor of 4.3. The new GTX 1070 Ti is very close in performance to GTX 1080. The GTX 1080 Ti finishes 5.5x faster than the AWS P2 K80, which is in line with the previous results. On performance side: GTX 1080 Ti and Titan X are similar. Here are the things to consider when picking a GPU:
Considering all of this, I picked the GTX 1080 Ti, mainly for the training speed boost."
Stefan Kojouharov,"Cheat Sheets for AI, Neural Networks, Machine Learning, Deep Learning & Big Data","SciPy builds on the NumPy array object and is part of the NumPy stack which includes tools like Matplotlib, pandas and SymPy, and an expanding set of scientific computing libraries. [3]
matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy. [6] matplotlib is designed to be as usable as MATLAB, with the ability to use Python, with the advantage that it is free. Scikit-learn (formerly scikits.learn) is a free software machine learning library for the Python programming language. pyplot is a matplotlib module which provides a MATLAB-like interface. This machine learning cheat sheet will help you find the right estimator for the job which is the most difficult part. There is also a procedural “pylab” interface based on a state machine (like OpenGL), designed to closely resemble that of MATLAB, though its use is discouraged. Big-O Algorithm Cheat Sheet: http://bigocheatsheet.com/
Bokeh Cheat Sheet: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Bokeh_Cheat_Sheet.pdf
Data Science Cheat Sheet: https://www.datacamp.com/community/tutorials/python-data-science-cheat-sheet-basics
Data Wrangling Cheat Sheet: https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf
Data Wrangling: https://en.wikipedia.org/wiki/Data_wrangling
Ggplot Cheat Sheet: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf
Keras Cheat Sheet: https://www.datacamp.com/community/blog/keras-cheat-sheet#gs.DRKeNMs
Keras: https://en.wikipedia.org/wiki/Keras
Machine Learning Cheat Sheet: https://ai.icymi.email/new-machinelearning-cheat-sheet-by-emily-barry-abdsc/
Machine Learning Cheat Sheet: https://docs.microsoft.com/en-in/azure/machine-learning/machine-learning-algorithm-cheat-sheet
ML Cheat Sheet:: http://peekaboo-vision.blogspot.com/2013/01/machine-learning-cheat-sheet-for-scikit.html
Matplotlib Cheat Sheet: https://www.datacamp.com/community/blog/python-matplotlib-cheat-sheet#gs.uEKySpY
Matpotlib: https://en.wikipedia.org/wiki/Matplotlib
Neural Networks Cheat Sheet: http://www.asimovinstitute.org/neural-network-zoo/
Neural Networks Graph Cheat Sheet: http://www.asimovinstitute.org/blog/
Neural Networks: https://www.quora.com/Where-can-find-a-cheat-sheet-for-neural-network
Numpy Cheat Sheet: https://www.datacamp.com/community/blog/python-numpy-cheat-sheet#gs.AK5ZBgE
NumPy: https://en.wikipedia.org/wiki/NumPy
Pandas Cheat Sheet: https://www.datacamp.com/community/blog/python-pandas-cheat-sheet#gs.oundfxM
Pandas: https://en.wikipedia.org/wiki/Pandas_(software)
Pandas Cheat Sheet: https://www.datacamp.com/community/blog/pandas-cheat-sheet-python#gs.HPFoRIc
Pyspark Cheat Sheet: https://www.datacamp.com/community/blog/pyspark-cheat-sheet-python#gs.L=J1zxQ
Scikit Cheat Sheet: https://www.datacamp.com/community/blog/scikit-learn-cheat-sheet
Scikit-learn: https://en.wikipedia.org/wiki/Scikit-learn
Scikit-learn Cheat Sheet: http://peekaboo-vision.blogspot.com/2013/01/machine-learning-cheat-sheet-for-scikit.html
Scipy Cheat Sheet: https://www.datacamp.com/community/blog/python-scipy-cheat-sheet#gs.JDSg3OI
SciPy: https://en.wikipedia.org/wiki/SciPy
TesorFlow Cheat Sheet: https://www.altoros.com/tensorflow-cheat-sheet.html
Tensor Flow: https://en.wikipedia.org/wiki/TensorFlow
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story. The NumPy stack is also sometimes referred to as the SciPy stack. [2] SciPy makes use of matplotlib."
Netflix Technology Blog,Distributed Neural Networks with GPUs in the AWS Cloud,"We hook up spearmint with our training algorithm by having it choose the set of hyperparameters and then training a Neural Network with those parameters using our GPU-optimized code. We’ve squeezed high performance from our GPU but we only have 1–2 GPU cards per machine, so we would like to make use of the distributed computing power of the AWS cloud to perform the hyperparameter tuning for all configurations, such as different models per international region. In our solution, we take the approach of using GPU-based parallelism for training and using distributed computation for handling hyperparameter tuning and different configurations. We use package spearmint to perform Bayesian Optimization and find the best hyperparameters for the Neural Network training algorithm. replace nppsMulC_32f_I function with:
Replacing all npps functions in this way for the Neural Network code reduced the total training time on the cg1 instance from over 20 hours to just 47 minutes when training on 4 million samples. We wanted to use a reasonable number of machines to implement a powerful machine learning solution using a Neural Network approach. To understand why, we first need to explain the different levels at which a model training process can be distributed. Using the same approach on the Lenovo S20 the total training time also reduced from 7 hours to 2 hours. However, as explained above, training a single instance actually implies training and testing several models, each corresponding to a different combinations of hyperparameters. The first successful instance of large-scale Deep Learning made use of 16000 CPU cores in 1000 machines in order to train an Artificial Neural Network in a matter of days."
Francesco Gadaleta,Gradient descent vs coordinate descent – Hacker Noon,"Hence, I wrote some code that implements both gradient descent and coordinate descent. Gradient descent performs the same number of operations . Coordinate descent needs to perform operations for each coordinate update. In the figure below, I plot the analytical solution in red, the gradient descent minimisation in blue and the coordinate descent in green, across a number of iterations. A small explanation is probably necessary to read the function that performs coordinate descent. But even with some tuning (maybe with some linear search) or adaptive learning rates, it’s quite common to see that coordinate descent overcomes its brother gradient descent many times. Coordinate descent will update each variable in a Round Robin fashion. Despite the learning rate of the gradient descent procedure (which could indeed speed up convergence), the comparison between the two is fair at least in terms of complexity. The comparison might not be completely fair because the learning rate in the gradient descent procedure is fixed at 0.1 (which in some cases might be slower indeed). Now, my question is how good or bad is following the negative gradient with respect to a coordinate descent approach that loops across all dimensions and minimizes along each?"
Milo Spencer-Harper,How to build a multi-layered neural network in Python,"This layer enables the neural network to think about combinations of inputs. It is now possible for the neural network to discover correlations between the output of Layer 1 and the output in the training set. Also available here: https://github.com/miloharper/multi-layer-neural-network
This code is an adaptation from my previous neural network. But there is a direct relationship between combinations of pixels and apples. You can see from the diagram that the output of Layer 1 feeds into Layer 2. When the neural network calculates the error in layer 2, it propagates the error backwards to layer 1, adjusting the weights as it goes. The process of adding more layers to a neural network, so it can think about combinations, is called “deep learning”. There is no direct relationship between pixels and apples. The correct answer is 0. So the correct answer is 0."
Jim Fleming,Loading a TensorFlow graph with the C++ API – Jim Fleming – Medium,"Inside loader.cc we’re going to do a few things:
Now we create a BUILD file for our project. Inside this project folder we’ll create a new file called <my project name>.cc (e.g. Here’s an example created with Jupyter:
Let’s create a new folder like tensorflow/tensorflow/<my project name> for your binary or library to live. I’m going to call the project loader since it will be loading a graph. And that should be all we need to do to compile and run C++ code for TensorFlow. Check out the related post: Loading TensorFlow graphs from Node.js (using the C API). loader.cc). Let’s start by creating a minimal TensorFlow graph and write it out as a protobuf file. In the TensorFlow repo there are more involved examples, such as building a graph in C++. Here’s the final directory structure:
You could also call bazel run :loader to run the executable directly, however the working directory for bazel run is buried in a temporary folder and ReadBinaryProto looks in the current working directory for relative paths."
Milo Spencer-Harper,Video of a neural network learning – Deep Learning 101 – Medium,"Green synaptic connections represent positive weights (a signal flowing through this synapse will excite the next neuron to fire). Red synaptic connections represent negative weights (a signal flowing through this synapse will inhibit the next neuron from firing). Each time she considers an example in the training set, you will see her think (you will see her neurons and her synaptic connections glow). The video you are about to see, shows a neural network trying to solve this pattern. An example of this, is the first synapse into the output neuron — early on in the video it turns from red to green. Notice how some synapses are green (positive) and others are red (negative). As part of my quest to learn about AI, I generated a video of a neural network learning. So the correct answer is 0. The correct answer is 0. From a learning perspective, being able to visually see a neural network is hugely beneficial."
Christian Hernandez,Into the Age of Context – Crossing the Pond – Medium,"It is an age in which we, and the devices and sensors around us, generate massive reams of data and in which self-teaching algorithms drill into that data to derive insight and recommend or auto-generate an action. A parallel and accelerating trend which will power the Age of Context is the proliferation of intelligent and connected sensors around us. In the Age of Context this will change, as larger and larger data sets of sensor data, combined with other data combined with intelligent analytics allows data to become actionable. In the evolution from the current Era of Mobile to the future Age of Context, the supercomputers in our pocket evolve from information delivery and application interaction layers, to notification context-aware action drivers. The most visible example of the Age of Context today is Google Now. For the Age of Context to thrive the platforms that power it must be interlinked across data and applications. In the Age of Context personal data (ex: calendar and email, location and time) is integrated with publicly available data (ex: traffic data, pollution level) and app-level data (ex: Uber surge pricing, number of steps tracked by my FitBit) to intelligently drive me towards an action (ex: getting me to walk to my next meeting instead of ordering a car). The massive amounts of data the growing number of internet users and connected devices generate each day. The Age of Context is being brought about by a number of technology trends which have been accelerating in a parallel and are now coming together. Looking forward to the next 5 year though, I personally believe we will move from the Era of Mobile to the Age of Context."
Venture Scanner,The State of Artificial Intelligence in Six Visuals,"The following companies either build deep learning/machine learning technology or utilize it as the core offering of their products. The following companies utilize deep learning/machine learning technology in a specific way or use-case in their products. Currently, the “Deep Learning/Machine Learning Applications” category is leading the way with a total of 200 companies, followed by “Natural Language Processing (Speech Recognition)” with 130 companies. The following companies either build natural language processing technology or utilize it as the core offering in their products (excluding all speech recognition companies). The following companies either build computer vision/image recognition technology or utilize it as the core offering in their products. Deep Learning/Machine Learning (General): Machine learning is the technology of computer algorithms that operate based on its learning from existing data. The following companies utilize computer vision/image recognition technology in a specific way or use-case in their products. The six Artificial Intelligence visuals below help make sense of this dynamic market:
Deep Learning/Machine Learning Applications: Machine learning is the technology of computer algorithms that operate based on its learnings from existing data. Deep learning is a subset of machine learning that focuses on deeply layered neural networks. Deep learning is a subset of machine learning that focuses on deeply layered neural networks."
Illia Polosukhin,Tensorflow Tutorial — Part 2 – Illia Polosukhin – Medium,"In the previous Part 1 of this tutorial, I introduced a bit of TensorFlow and Scikit Flow and showed how to build a simple logistic regression model on Titanic dataset. Scikit Flow already implements a convenient wrapper around TensorFlow API for creating many layers of fully connected units, so it’s simple to start with deep model by just swapping classifier in our previous model to the TensorFlowDNNClassifier and specify hidden units per layer:
This will create 3 layers of fully connected units with 10, 20 and 10 hidden units respectively, with default Rectified linear unit activations. In this part let’s go deeper and try multi-layer fully connected neural networks, writing your custom model to plug into the Scikit Flow and top it with trying out convolutional networks. An idea behind TensorFlow (and many other deep learning frameworks) is to be able to connect differentiable parts of the model together and optimize them given the same cost (or loss) function. Here, we take digits dataset and write a custom model:
We’ve created conv_model function, that given tensor X and y, runs 2D convolutional layer with the most simple max pooling — just maximum. I didn’t play much with hyperparameters, but previous DNN model actually yielded worse accuracy then a logistic regression. The Part 3 is expanding the model for Titanic dataset with handling categorical variables. For the sake of this example, I though want to show how to switch to the custom model where you can have more control. As you can see, creating a custom model is as easy as writing a function, that takes X and y inputs (which are Tensors) and returns two tensors: predictions and loss. It’s easy now to modify this code to add as many layers as you want (some of the state-of-the-art image recognition models are hundred+ layers of convolutions, max pooling, dropout and etc)."
Derrick Harris,Baidu explains how it’s mastering Mandarin with deep learning,"How is the Deep Speech system better than the previous system for handling queries in Mandarin? Can you describe the difference between a search-based system like Deep Speech and something like Microsoft’s Skype Translate, which is also based on deep learning? Baidu has a very active system for voice search in Mandarin, and it works pretty well. There are a couple of differences with Mandarin that made us think it would be very difficult to have our English speech system work well with it. The model, which is accurate 94 percent of the time in tests, is based on a powerful deep learning system called Deep Speech that Baidu first unveiled in December 2014. One thing that’s pleasantly surprising to us is that we had to do very little changing to it — other than scaling it and giving it the right data — to make this system we showed in December that worked really well on English work remarkably well in Chinese, as well. It really becomes a story of “How can we get right data?” when deep learning is involved. In particular, processing sequential data with deep learning is something that we’re just figuring out how to do really well. So you have to change a bunch of things to get a system to work with Mandarin, or any Chinese for that matter. On Aug. 8 at the International Neural Network Society conference on big data in San Francisco, Baidu senior research engineer Awni Hannun presented on a new model that the Chinese search giant has developed for handling voice queries in Mandarin."
Kyle McDonald,Comparing Artificial Artists – Kyle McDonald – Medium,"Here is an attempt to recreate the results from the paper using Kai’s implementation:
Not quite the same, and possibly explained by a few differences between Kai’s implementation and the original paper:
As a final comparison, consider the images Andrej Karpathy posted from his own implementation. Justin switched his implementation to use L-BFGS and equally weighted layers, and to my eyes this matches the results in the original paper. Here are his results for one of the harder content/style pairs:
Other implementations that look great, but I haven’t tested enough:
The definition of the Gram matrix confused me at first, so I wrote it out as code. Beside’s Kai’s, I’ve seen one more implementation from a PhD student named Satoshi: a brief example in Python with Chainer. His follow the original paper very closely, except for using unequal weights when balancing different layers used for style reconstruction. Unfortunately, his results don’t quite match the paper, and it’s unclear why. I immediately stopped working on my implementation and started playing with his. I’ve started testing another implementation that popped up this morning from Justin Johnson. I’m just getting started with this topic, so as I learn I want to share my understanding of the algorithm here, along with some results I got from testing his code. On Sunday, Kai Sheng Tai posted the first public implementation."
Jim Fleming,Highway Networks with TensorFlow – Jim Fleming – Medium,"Here’s what the highway layer graph looks in TensorBoard:
Using a highway layer in a network is also straightforward. no hyperparameter search) the fully-connected highway network performed much better than a fully-connected network. Otherwise, the layer largely resembles a dense layer with a few additions:
What happens is that when the transform gate is 1, we pass through our activation (H) and suppress the carry gate (since it will be 0). TL;DR Fully-connected highway repo and convolutional highway repo. One detail to keep in mind is that consecutive highway layers must be the same size but you can use fully-connected layers to change dimensionality. Here’s are the complete notebooks if you want to play with the code: fully-connected highway repo and convolutional highway repo. We need a weight matrix and a bias vector then we’ll compute the following for the layer output:
Here’s what a dense layer looks like as a graph in TensorBoard:
For the highway layer what we want are two “gates” that control the flow of information. The network can probably even go deeper since the it’s just learning to carry the last 980 layers or so. The “transform” gate controls how much of the activation we pass through and the “carry” gate controls how much of the unmodified input we pass through. When in doubt start with high biases (more negative) since it’s easier to learn to overcome carrying than without carry gates (which is just a plain network)."
Nathan Benaich,Investing in Artificial Intelligence – Nathan Benaich – Medium,"So far, we’ve seen circa 300 deals into AI companies (defined as businesses whose description includes keywords: artificial intelligence, machine learning, computer vision, NLP, data science, neural network, deep learning from Jan 1st 2015 thru 1st Dec 2015, CB Insights). It’s therefore key to understand your use case, your user, the value you bring and how it’s experience and assessed. Businesses must therefore have exposure to this market. The key takeaway points are a) the financing and exit markets for AI companies are still nascent, as exemplified by the small rounds and low deal volumes, and b) the vast majority of activity takes place in the US. Finally, you must have exposure to the US market where the lion’s share of value is created and realised. In the UK, companies like Ravelin, Signal and Gluru raised seed rounds. There are circa 900 companies working in the AI field, most of which tackle problems in business intelligence, finance and security. We invest across Europe and the US and our focus is on core technologies and user experiences. Companies with the resources to invest in AI are already creating an impetus for others to follow suit or risk not having a competitive seat at the table. Some companies are already hacking away at this problem:
A point worth noting is that the UK has a slight leg up on the data access front."
Tal Perry,Deep Learning the Stock Market – Tal Perry – Medium,"In Eidnes’ example, the output of the LSTMs is a vector that represents what the next word will be in some abstract space. In Karpathy’s example, the output of the LSTMs is a vector that represents the next character in some abstract representation. Going deeper
I want to point out that this is where we start to get into the deep part of deep learning. Lars inputs a sequence of Word Vectors and each one of them:
We’re going to do the same thing with one difference, instead of word vectors we’ll input “MarketVectors”, those market vectors we described before. Then we’ll get a sequence that looks like:
We want to take the vector that our LSTMs output and squish it so that it gives us the probability of the next item in our sequence being a 1. But, I want to achieve something specific with the first layer, the one that takes as input the 4000 dimensional vector and outputs a 300 dimensional one. Where Karpathy used characters, we’re going to use our market vectors and feed them into the magic black box. Now each output of the previous LSTM becomes the inputs of the next LSTM, and each one goes on to learn higher abstractions of the data coming in. So I’ll get back to the mechanics of that magic box in a second, but let me remind you that we want to predict the future market based on the past just like he predicted the next word based on the previous one. (Remember, a “word vector” is a representation of a word in the form of a bunch of numbers, like we saw in the beginning of this post)."
Andrej Karpathy,Yes you should understand backprop – Andrej Karpathy – Medium,"See a longer explanation in CS231n lecture video. See a longer explanation in this CS231n lecture video. See a longer explanation in this CS231n lecture video. The forward and backward pass for a fully connected layer that uses ReLU would at the core include:
If you stare at this for a while you’ll see that if a neuron gets clamped to zero in the forward pass (i.e. But if that is the case, z*(1-z), which is local gradient of the sigmoid non-linearity, will in both cases become zero (“vanish”), making the gradient for both x and W be zero. When you stare at what the backward pass is doing, you’ll see that the gradient signal going backwards in time through all the hidden states is always being multiplied by the same matrix (the recurrence matrix Whh), interspersed with non-linearity backprop. For example, a fully connected layer with sigmoid non-linearity computes (using raw numpy):
If your weight matrix W is initialized too large, the output of the matrix multiply could have a very large range (e.g. I hope you’ll be much more suspicious of backpropagation going forward and think carefully through what the backward pass is doing. Another fun non-linearity is the ReLU, which thresholds neurons at zero from below. TLDR: if you’re using sigmoids or tanh non-linearities in your network and you understand backpropagation you should always be nervous about making sure that the initialization doesn’t cause them to be fully saturated."
Erik Hallström,How to build a Recurrent Neural Network in TensorFlow (1/7),"Now it’s time to build the part of the graph that resembles the actual RNN computation, first we want to split the batch data into adjacent time-steps. It will plot the loss over the time, show training input, training output and the current predictions by the network on different sample series in a training batch. The RNN will simultaneously be training on different parts in the time-series; steps 4 to 6, 16 to 18 and 28 to 30 in the current batch-example. We will build a simple Echo-RNN that remembers the input data and then echoes it after a few time-steps. The input to the RNN at every time-step is the current value as well as a state vector which represent what the network has “seen” at time-steps before. The last line is adding the training functionality, TensorFlow will perform back-propagation for us automatically — the computation graph is executed once for each mini-batch and the network-weights are updated incrementally. Next let’s build the part of the graph that does the actual RNN computation. It’s time to wrap up and train the network, in TensorFlow the graph is executed in a session. It is short for “Recurrent Neural Network”, and is basically a neural network that can be used when your data is treated as a sequence, where the particular order of the data-points matter. The downside with doing this is that truncated_backprop_length need to be significantly larger than the time dependencies (three steps in our case) in order to encapsulate the relevant training data."
Stefan Kojouharov,Ultimate Guide to Leveraging NLP & Machine Learning for your Chatbot,"A commonly used metric is recall@k. Recall@k means that we let the model pick the k best responses out of the 10 possible responses (1 true and 9 distractors). Retrieval-based models (easier) use a repository of predefined responses and some kind of heuristic to pick an appropriate response based on the input and context. In this post we’ve implemented a retrieval-based neural network model that can assign scores to potential responses given a conversation context. In order to use Tensorflow’s built-in support for training and evaluation we need to create an input function — a function that returns batches of our input data. Each example consists of a context, the conversation up to this point, and an utterance, a response to the context. Now that we have set up the boilerplate code around inputs, parsing, evaluation and training it’s time to write code for our Dual LSTM neural network. After training for about 20,000 steps (around an hour on a fast GPU) our model gets the following results on the test set:
While recall@1 is close to our TFIDF model, recall@2 and recall@5 are significantly better, suggesting that our neural network assigns higher scores to the correct answers. A common problem with generative systems is that they tend to produce generic responses like “That’s great!” or “I don’t know” that work for a lot of input cases. Deep Learning techniques can be used for both retrieval-based or generative models, but research seems to be moving into the generative direction. Then we can write general-purpose code to train our model as follows:
Here we create an estimator for our model_fn, two input functions for training and evaluation data, and our evaluation metrics dictionary."
Arthur Juliani,Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C),"In A3C there is a global network, and multiple worker agents which each have their own set of network parameters. ~ From here we go asynchronous ~
Each worker begins by setting its network parameters to those of the global network. The worker then resets its own network parameters to those of the global network, and the process begins again. A worker then uses the gradients to update the global network parameters. Next, a set of worker agents, each with their own network and environment are created. The general outline of the code architecture is:
The A3C algorithm begins by constructing the global network. In the case of A3C, our network will estimate both a value function V(s) (how good a certain state is to be in) and a policy π(s) (a set of action probability outputs). In this way, the global network is constantly being updated by each of the agents, as they interact with their environment. Critically, the agent uses the value estimate (the critic) to update the policy (the actor) more intelligently than traditional policy gradient methods. In this article I want to provide a tutorial on implementing the Asynchronous Advantage Actor-Critic (A3C) algorithm in Tensorflow."
Alexandr Honchar,Neural networks for algorithmic trading. Simple time series forecasting,"We can see, that treating financial time series prediction as regression problem is better approach, it can learn the trend and prices close to the actual. First, we will try just to predict close price in the end of the next day, second, we will try to predict return (close price — open price). Now we will use not close prices, but daily return (close price-open price) and we want to predict if close price is higher or lower than open price based on last 20 days returns. We will consider our problem as 1) regression problem (trying to forecast exactly close price or return next day) 2) binary classification problem (price will go up [1; 0] or down [0; 1]). On the plot below you can see actual scaled time series (black)and our forecast (blue) for it:
For using this model in real world we should return back to unscaled time series. Let’s see what happens if we just pass chunks of 20-days close prices and predict price on 21st day. What was surprising for me, that MLPs are treating sequence data better as CNNs or RNNs which are supposed to work better with time series. First let’s prepare our data for training. In this, first part, I want to show how MLPs, CNNs and RNNs can be used for financial time series prediction. I think we can get better results both in regression and classification using different features (not only scaled time series) like some technical indicators, volume of sales."
Arthur Juliani,Simple Reinforcement Learning with Tensorflow Part 4: Deep Q-Networks and Beyond,"In order to correct for this, the authors of DDQN paper propose a simple trick: instead of taking the max over Q-values when computing the target-Q value for our training step, we use our primary network to chose an action, and our target network to generate the target Q-value for that action. I will discuss two simple additional improvements to the DQN architecture, Double DQN and Dueling DQN, that allow for improved performance, stability, and faster training time. The first is the value function V(s), which says simple how good it is to be in any given state. While the DQN we have described above could learn ATARI games with enough training, getting the network to perform well on those games takes at least a day of training on a powerful machine. In order to transform an ordinary Q-Network into a DQN we will be making the following improvements:
It was these three innovations that allowed the Google DeepMind team to achieve superhuman performance on dozens of Atari games using their DQN agent. It will be built upon the simple one layer Q-network we created in Part 0, so I would recommend reading that first if you are new to reinforcement learning. While our ordinary Q-network was able to barely perform as well as the Q-Table in a simple game environment, Deep Q-Networks are much more capable. The main intuition behind Double DQN is that the regular DQN often overestimates the Q-values of the potential actions to take in a given state. More formally:
The goal of Dueling DQN is to have a network that separately computes the advantage and value functions, and combines them back into a single Q-function only at the final layer. The key to realizing the benefit is to appreciate that our reinforcement learning agent may not need to care about both value and advantage at any given time."
Vishal Maini,A Beginner’s Guide to AI/ML 🤖👶 – Machine Learning for Humans – Medium,"Part 2.1: Supervised Learning. Demystifying artificial intelligence & machine learning. Machine learning is a subfield of artificial intelligence. Part 1: Why Machine Learning Matters. And now, without further ado, let’s dive into machine learning with Part 2.1: Supervised Learning! Q-learning, policy learning, and deep reinforcement learning. Part 5: Reinforcement Learning. Part 2.2: Supervised Learning II. Part 2.3: Supervised Learning III. Part 4: Neural Networks & Deep Learning."
Tim Anglade,"How HBO’s Silicon Valley built “Not Hotdog” with mobile TensorFlow, Keras & React Native","A neural network can only be as good as the data that trained it, and improving training set quality was probably one of the top 3 things we spent time on during this project. The HBO show Silicon Valley released a real AI app that identifies hotdogs — and not hotdogs — like the one shown on season 4’s 4th episode (the app is now available on Android as well as iOS!) There were 3 main factors:
For these reasons, we started experimenting with what’s trendily called “edge computing”, which for our purposes meant that after training our neural network on our laptop, we would export it and embed it directly into our mobile app, so that the neural network execution phase (or inference) would run directly inside the user’s phone. First The nature of our problem meant a strong imbalance in training data: there are many more examples of things that are not hotdogs, than things that are hotdogs. The problem directly ahead of us was simple: if Inception and VGG were too big, was there a simpler, pre-trained neural network we could retrain? While the network (Inception in this case) may have been trained on the 14M images contained in ImageNet, we were able to retrain it on a just a few thousand hotdog images to get drastically enhanced hotdog recognition. The key things we did to improve this were:
The final composition of our dataset was 150k images, of which only 3k were hotdogs: there are only so many hotdogs you can look at, but there are many not hotdogs to look at. After adding Batch Normalization and ELU to SqueezeNet, we were able to train neural network that achieve 90%+ accuracy when training from scratch, however, they were relatively brittle meaning the same network would overfit in some cases, or underfit in others when confronted to real-life testing. I could build it in one weekend!” This app probably feels a lot like that, and the initial prototype was indeed built in a single weekend using Google Cloud Platform’s Vision API, and React Native. Most AI apps will hit more critical cultural biases than ours, but as an example, even our straightforward use-case, caught us flat-footed with built-in biases in our initial dataset, that made the app unable to recognize French-style hotdogs, Asian hotdogs, and more oddities we did not have immediate personal experience with."
Dhruv Parthasarathy,A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN,"In the image above, you can see how a single CNN is used to both carry out region proposals and classification. With these anchor boxes in mind, let’s take a look at the inputs and outputs to this Region Proposal Network:
We then pass each such bounding box that is likely to be an object into Fast R-CNN to generate a classification and tightened bounding boxes. The insight of Faster R-CNN was that region proposals depended on features of the image that were already calculated with the forward pass of the CNN (first step of classification). Fast R-CNN Insight 1: RoI (Region of Interest) Pooling
For the forward pass of the CNN, Girshick realized that for each image, a lot of proposed regions for the image invariably overlapped causing us to run the same CNN computation again and again (~2000 times!). The authors write:
Here are the inputs and outputs of their model:
How the Regions are Generated
Let’s take a moment to see how Faster R-CNN generates these region proposals from CNN features. Where earlier we had different models to extract image features (CNN), classify (SVM), and tighten bounding boxes (regressor), Fast R-CNN instead used a single network to compute all three. Understanding R-CNN
The goal of R-CNN is to take in an image, and correctly identify where the main objects (via a bounding box) in the image. In the image above, notice how the CNN features for each region are obtained by selecting a corresponding region from the CNN’s feature map. In particular, we’ll cover R-CNN (Regional CNN), the original application of CNNs to this problem, along with its descendants Fast R-CNN, and Faster R-CNN. Much like Fast R-CNN, and Faster R-CNN, Mask R-CNN’s underlying intuition is straight forward."
Sebastian Heinz,A simple deep learning model for stock price prediction using TensorFlow,"We need two placeholders in order to fit our model: X contains the network's inputs (the stock prices of all S&P 500 constituents at time T = t) and Y the network's outputs (the index value of the S&P 500 at time T = t + 1). Also our data science consultants at STATWORX are heavily using TensorFlow for deep learning and neural net research and development. The dataset was split into training and test data. During minibatch training random data samples of n = batch_size are drawn from the training data and fed into the network. The cost function of the network is used to generate a measure of deviation between the network’s predictions and the actual observed training targets. Initializers are used to initialize the network’s variables before training. Other network architectures, such as recurrent neural networks, also allow data flowing “backwards” in the network. After having defined the placeholders, variables, initializers, cost functions and optimizers of the network, the model needs to be trained. Since neural networks are actually graphs of data and mathematical operations, TensorFlow is just perfect for neural networks and deep learning. The user defines an abstract representation of the model (neural network) through placeholders and variables."
Max Pechyonkin,Understanding Hinton’s Capsule Networks. Part I: Intuition.,"Capsules introduce a new building block that can be used in deep learning to better model hierarchical relationships inside of internal knowledge representation of a neural network. In the same fashion, the idea of capsules itself is not that new and Hinton has mentioned it before, but there was no algorithm up until now to make it work. In addition to that, the team published an algorithm, called dynamic routing between capsules, that allows to train such a network. Hinton and his team proposed a way to train such a network made up of capsules and successfully trained it on a simple data set, achieving state-of-the-art performance. And the key idea is that representation of objects in the brain does not depend on view angle. This algorithm is called “dynamic routing between capsules”. Another benefit of the capsule approach is that it is capable of learning to achieve state-of-the art performance by only using a fraction of the data that a CNN would use (Hinton mentions this in his famous talk about what is wrongs with CNNs). They are one of the reasons deep learning is so popular today. Computer graphics deals with constructing a visual image from some internal hierarchical representation of geometric data. CNN approach to solve this issue is to use max pooling or successive convolutional layers that reduce spacial size of the data flowing through the network and therefore increase the “field of view” of higher layer’s neurons, thus allowing them to detect higher order features in a larger region of the input image."
Slav Ivanov,"The $1700 great Deep Learning box: Assembly, setup and benchmarks","We see that the GTX 1080 Ti is 2.4 times faster than the K80 on AWS P2 in training the model. We time training models on: AWS P2 instance GPU (K80), AWS P2 virtual CPU, the GTX 1080 Ti and Intel i5 7500 CPU. The 1080 Ti is 5.5 times faster that the AWS GPU (K80). And GTX 1080 Ti is about 30% faster than GTX 1080. The choice is between a few of Nvidia’s cards: GTX 1070, GTX 1070 Ti, GTX 1080, GTX 1080 Ti and finally the Titan X. The GTX 1080 Ti outperforms the AWS K80 by a factor of 4.3. The new GTX 1070 Ti is very close in performance to GTX 1080. The GTX 1080 Ti finishes 5.5x faster than the AWS P2 K80, which is in line with the previous results. On performance side: GTX 1080 Ti and Titan X are similar. Here are the things to consider when picking a GPU:
Considering all of this, I picked the GTX 1080 Ti, mainly for the training speed boost."
Stefan Kojouharov,"Cheat Sheets for AI, Neural Networks, Machine Learning, Deep Learning & Big Data","SciPy builds on the NumPy array object and is part of the NumPy stack which includes tools like Matplotlib, pandas and SymPy, and an expanding set of scientific computing libraries. [3]
matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy. [6] matplotlib is designed to be as usable as MATLAB, with the ability to use Python, with the advantage that it is free. Scikit-learn (formerly scikits.learn) is a free software machine learning library for the Python programming language. pyplot is a matplotlib module which provides a MATLAB-like interface. This machine learning cheat sheet will help you find the right estimator for the job which is the most difficult part. There is also a procedural “pylab” interface based on a state machine (like OpenGL), designed to closely resemble that of MATLAB, though its use is discouraged. Big-O Algorithm Cheat Sheet: http://bigocheatsheet.com/
Bokeh Cheat Sheet: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Bokeh_Cheat_Sheet.pdf
Data Science Cheat Sheet: https://www.datacamp.com/community/tutorials/python-data-science-cheat-sheet-basics
Data Wrangling Cheat Sheet: https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf
Data Wrangling: https://en.wikipedia.org/wiki/Data_wrangling
Ggplot Cheat Sheet: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf
Keras Cheat Sheet: https://www.datacamp.com/community/blog/keras-cheat-sheet#gs.DRKeNMs
Keras: https://en.wikipedia.org/wiki/Keras
Machine Learning Cheat Sheet: https://ai.icymi.email/new-machinelearning-cheat-sheet-by-emily-barry-abdsc/
Machine Learning Cheat Sheet: https://docs.microsoft.com/en-in/azure/machine-learning/machine-learning-algorithm-cheat-sheet
ML Cheat Sheet:: http://peekaboo-vision.blogspot.com/2013/01/machine-learning-cheat-sheet-for-scikit.html
Matplotlib Cheat Sheet: https://www.datacamp.com/community/blog/python-matplotlib-cheat-sheet#gs.uEKySpY
Matpotlib: https://en.wikipedia.org/wiki/Matplotlib
Neural Networks Cheat Sheet: http://www.asimovinstitute.org/neural-network-zoo/
Neural Networks Graph Cheat Sheet: http://www.asimovinstitute.org/blog/
Neural Networks: https://www.quora.com/Where-can-find-a-cheat-sheet-for-neural-network
Numpy Cheat Sheet: https://www.datacamp.com/community/blog/python-numpy-cheat-sheet#gs.AK5ZBgE
NumPy: https://en.wikipedia.org/wiki/NumPy
Pandas Cheat Sheet: https://www.datacamp.com/community/blog/python-pandas-cheat-sheet#gs.oundfxM
Pandas: https://en.wikipedia.org/wiki/Pandas_(software)
Pandas Cheat Sheet: https://www.datacamp.com/community/blog/pandas-cheat-sheet-python#gs.HPFoRIc
Pyspark Cheat Sheet: https://www.datacamp.com/community/blog/pyspark-cheat-sheet-python#gs.L=J1zxQ
Scikit Cheat Sheet: https://www.datacamp.com/community/blog/scikit-learn-cheat-sheet
Scikit-learn: https://en.wikipedia.org/wiki/Scikit-learn
Scikit-learn Cheat Sheet: http://peekaboo-vision.blogspot.com/2013/01/machine-learning-cheat-sheet-for-scikit.html
Scipy Cheat Sheet: https://www.datacamp.com/community/blog/python-scipy-cheat-sheet#gs.JDSg3OI
SciPy: https://en.wikipedia.org/wiki/SciPy
TesorFlow Cheat Sheet: https://www.altoros.com/tensorflow-cheat-sheet.html
Tensor Flow: https://en.wikipedia.org/wiki/TensorFlow
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story. The NumPy stack is also sometimes referred to as the SciPy stack. [2] SciPy makes use of matplotlib."
Vishal Maini,"Machine Learning for Humans, Part 2.1: Supervised Learning","In supervised learning, the machine attempts to learn the relationship between income and education from scratch, by running labeled training data through a learning algorithm. The data is split into a training data set and a test data set. The model on the right has zero loss for the training data because it perfectly fits every data point. Use more training data. I’ll punch you for every extra word.” Hyperparameter (λ): “Here’s the strength with which I will punch you for every extra word.”
A common problem in machine learning is overfitting: learning a function that perfectly explains the training data that the model learned from, but doesn’t generalize well to unseen test data. The training set has labels, so your model can learn from these labeled examples. Let’s take a look at the loss function we saw in regression:
We see that this is really a function of two variables: β0 and β1. Our goal is to learn the model parameters (in this case, β0 and β1) that minimize error in the model’s predictions. To illustrate how supervised learning works, let’s examine the problem of predicting annual income based on the number of years of higher education someone has completed. The function is f(β0,β1)=z."
Arvind N,Thoughts after taking the Deeplearning.ai courses – Towards Data Science,"The fast AI course mainly teaches you the art of driving while Andrew’s course primarily teaches you the engineering behind the car. After you complete that course, please try to complete part-1 of Jeremy Howard’s excellent deep learning course. Andrew’s DL course does all of this, but in the complete opposite order. Once you are comfortable creating deep neural networks, it makes sense to take this new deeplearning.ai course specialization which fills up any gaps in your understanding of the underlying details and concepts. If you have not done any machine learning before this, don’t take this course first. Andrew stresses on the engineering aspects of deep learning and provides plenty of practical tips to save time and money — the third course in the DL specialization felt incredibly useful for my role as an architect leading engineering teams. The best starting point is Andrew’s original ML course on coursera. Andrew Ng’s new deeplearning.ai course is like that Shane Carruth or Rajnikanth movie that one yearns for! Style of teaching that is unique to Andrew and carries over from ML — I could feel the same excitement I felt in 2013 when I took his original ML course. Andrew patiently explains the requisite math and programming concepts in a carefully planned order and a well regulated pace suitable for learners who could be rusty in math/coding."
Blaise Aguera y Arcas,Do algorithms reveal sexual orientation or just expose our stereotypes?,"The results show that lesbians indeed use eyeshadow much less than straight women do, gay men and women do both wear glasses more, and young opposite-sex-attracted men are considerably more likely to have prominent facial hair than their gay or same-sex-attracted peers. However, asking the question “Do you like how you look in glasses?” reveals that this is likely more of a stylistic choice:
Same-sex attracted women also report wearing glasses more, as well as liking how they look in glasses more, across a range of ages:
One can also see how opposite-sex attracted women under the age of 40 wear contact lenses significantly more than same-sex attracted women, despite reporting that they have a vision defect at roughly the same rate, further illustrating how the difference is driven by an aesthetic preference: [4]
Similar analysis shows that young same-sex attracted men are much less likely to have hairy faces than opposite-sex attracted men (“serious facial hair” in our plots is defined as answering “yes” to having a goatee, beard, or moustache, but “no” to stubble). Overall, opposite-sex attracted men in our sample are 35% more likely to have serious facial hair than same-sex attracted men, and for men under the age of 31 (who are overrepresented on dating websites), this rises to 75%. In the following figures, we show the proportion of women who answer “yes” to “Do you ever use makeup?” (top) and “Do you wear eyeshadow?” (bottom), averaged over 6-year age intervals:
The blue curves represent strictly opposite-sex attracted women (a nearly identical set to those who answered “yes” to “Are you heterosexual or straight?”); the cyan curve represents women who answer “yes” to either or both of “Are you sexually attracted to women?” and “Are you romantically attracted to women?”; and the red curve represents women who answer “yes” to “Are you homosexual, gay or lesbian?”. [2] The patterns revealed here are intuitive; it won’t be breaking news to most that straight women tend to wear more makeup and eyeshadow than same-sex attracted and (even more so) lesbian-identifying women. This is consistent with a pattern of heterosexual men on average shooting from below, heterosexual women from above as the wedding photographer suggests, and gay men and lesbian women from directly in front. Our survey confirms that opposite-sex attracted men consistently self-report having a tan face (“Yes” to “Is your face tan?”) slightly more often than same-sex attracted men:
Once again Wang and Kosinski reach for a hormonal explanation, writing: “While the brightness of the facial image might be driven by many factors, previous research found that testosterone stimulates melanocyte structure and function leading to a darker skin”. The fact that we see a cohort of same-sex attracted men in their 40s who have just as much facial hair as opposite-sex attracted men suggests a different story, in which fashion trends and cultural norms play the dominant role in choices about facial hair among men, not differing exposure to hormones early in development. Composite images of the lesbian, gay, and straight men and women in the sample reveal a great deal about the information available to the algorithm:
Clearly there are differences between these four composite faces. That same-sex attracted men of most ages wear glasses significantly more than exclusively opposite-sex attracted men do might be a bit less obvious, but this trend is equally clear: [3]
A proponent of physiognomy might be tempted to guess that this is somehow related to differences in visual acuity between these populations of men."
David Foster,How to build your own AlphaZero AI using Python and Keras,"You can replace the game.py file with any game file that conforms to the same API and the algorithm will in principal, learn strategy through self play, based on the rules you have given it. If you switch the Connect4 game.py file for the Metasquares game.py file, the same algorithm will learn how to play Metasquares instead. There is a game.py file for a game called ‘Metasquares’ in the games folder. The game that our algorithm will learn to play is Connect4 (or Four In A Row). To view individual convolutional filters and densely connected layers in the neural network, run the following inside the the run.ipynb notebook:
This contains the Node, Edge and MCTS classes, that constitute a Monte Carlo Search Tree. If it wins, the neural network inside the best_player is switched for the neural network inside the current_player, and the loop starts again. An instance of the Memory class stores the memories of previous games, that the algorithm uses to retrain the neural network of the current_player. This file contains the Residual_CNN class, which defines how to build an instance of the neural network. The best_player contains the best performing neural network and is used to generate the self play memories. To play against your creation, run the following code (it’s also in the run.ipynb notebook)
When you run the algorithm, all model and memory files are saved in the run folder, in the root directory."
Aman Agarwal,Explained Simply: How an AI program mastered the ancient game of Go,"As you would remember, this network can directly take a board position and decide how an expert would play it — so you can use it to single-handedly play games.Well, the result was that the RL fine-tuned network won against the SL network that was only trained on human moves. Use a value function (Foma) to predict outcomes, and use a policy function (Lusha) to give you grand-master probabilities to help narrow down the moves you roll out. Now back to Foma — remember the “optimal value function”: v*(s) -> that only tells you how likely you are to win in your current board position if both players play perfectly from that point on?So obviously, to train an NN to become our value function, we would need a perfect player... which we don’t have. But in the case of the value function (which you would remember uses a strong player to approximate a perfect player), training Foma using the RL policy works better than training her with the SL policy. Once you have the SL network, trained in a supervised manner using human player moves with the human moves data, as I said before you have to let her practice by itself and get better. And this is why RL is so versatile; it can be used to train policy or value networks for any game, not just Go. After that, once you’ve trained both of the slow and fast policy networks enough using human player data, you can try letting Lusha play against herself on a Go board for a few days, and get more practice. To train the policy network (predicting for a given position which moves experts would pick), you simply use examples of human games and use them as data for good old supervised learning. They trained a “policy neural network” to decide which are the most sensible moves in a particular board position (so it’s like following an intuitive strategy to pick moves from any position). In a game like Chess or Go, as we said before, if you try to imagine even 7–8 moves into the future, there can be so many possible positions that you don’t have enough time to check all of them with Foma."
Eugenio Culurciello,The fall of RNN / LSTM – Towards Data Science,"To the rescue, and combining multiple neural attention modules, comes the “hierarchical neural attention encoder”, shown in the figure below:
A better way to look into the past is to use attention modules to summarize all past encoded vectors into a context vector Ct.
Notice there is a hierarchy of attention modules here, very similar to the hierarchy of neural networks. Note 2: RNN and LSTM are memory-bandwidth limited problems (see this for details). We fell for Recurrent neural networks (RNN), Long-short term memory (LSTM), and all their variants. But instead of a convolutional neural network we use hierarchical attention modules. Note 1: Hierarchical neural attention is similar to the ideas in WaveNet. About training RNN/LSTM: RNN and LSTM are difficult to train because they require memory-bandwidth-bound computation, which is the worst nightmare for hardware designer and ultimately limits the applicability of neural networks solutions. This is in contrast to the T steps that a RNN needs to do, where T is the maximum length of the sequence to be remembered, and T >> N.
This architecture is similar to a neural Turing machine, but lets the neural network decide what is read out from memory via attention. Remember RNN and LSTM and derivatives use mainly sequential processing over time. It is very surprising to us to see so many companies still use RNN/LSTM for speech to text, many unaware that these networks are so inefficient and not scalable. This extends the ability of the hierarchical neural attention encoder to 10,000 past vectors."
Gary Marcus,In defense of skepticism about deep learning – Gary Marcus – Medium,"What Marcus said is a problem with supervised learning, not deep learning. Marcus wasn’t very nice to deep learning. I suggested instead the deep learning be viewed “not as a universal solvent, but simply as one tool among many.”
In place of pure deep learning, I called for hybrid models, that would incorporate not just supervised forms of deep learning, but also other techniques as well, such as symbol-manipulation, and unsupervised learning (itself possibly reconceptualized). arXiv. arXiv. arXiv. arXiv. arXiv, cs.AI. arXiv, cs.AI. In a recent appraisal of deep learning (Marcus, 2018) I outlined ten challenges for deep learning, and suggested that deep learning by itself, although useful, was unlikely to lead on its own to artificial general intelligence."
Bargava,How to learn Deep Learning in 6 months – Towards Data Science,"Step 3
Now is the time to understand the bottom-up approach to deep learning. When learning deep learning, we will follow the same top-down approach. Step 5
Now go and do fast.ai’s part II course — Cutting Edge Deep Learning for Coders. Happy deep learning. And read the deep learning book. Once you finish the above two, read the Matrix Calculus for Deep Learning. Do the fast.ai course — Practical Deep Learning for Coders — Part 1. It is quite possible to learn, follow and contribute to state-of-art work in deep learning in about 6 months’ time. Do all the 5 courses in the deep learning specialisation in Coursera. And in about 26 weeks since the time you started, and if you followed all of the above religiously, you will have a solid foundation in deep learning."
Seth Weidman,The 3 Tricks That Made AlphaGo Zero Work – Hacker Noon,"This is how DeepMind trained its single, “two-headed” neural network that it used to guide MCTS during its search, just as AlphaGo did with two separate neural networks. However, AlphaGo Zero’s neural network — its “intuition” — was trained completely differently from that of AlphaGo:
Let’s say you have a neural network that is attempting to “understand” the game of Go: that is, for every board position, it is using a deep neural network to generate evaluations of what the best moves are. Here, AlphaGo and AlphaGo Zero used very different approaches; we’ll start first with AlphaGo’s:
AlphaGo had two separately trained neural networks. AlphaGo Zero’s was its neural network architecture, a “two-headed” architecture. At a high level, AlphaGo Zero works the same way as AlphaGo: specifically, it plays Go by using MCTS-based lookahead search, intelligently guided by a neural network. The other half of the increase in playing strength simply came from bringing the neural network architecture up-to-date with the latest advances in the field:
AlphaGo Zero used a more “cutting edge” neural network architecture than AlphaGo. DeepMind then combined these two neural networks with MCTS — that is, the program’s “intuition” with its brute force “lookahead” search— in a very clever way: it used the network that had been trained to predict moves to guide which branches of the game tree to search and used the network that had been trained to predict whether a position was “winning” to evaluate the positions it encountered during its search. Interestingly, as the chart below shows, each of these two neural network-related tricks — switching from convolutional to residual architecture and using the “Two Headed Monster” neural network architecture instead of separate neural networks — would have resulted in about half of the increase in playing strength as was achieved when both were combined. Train the neural network, using both A) the move evaluations produced by the MCTS lookahead search and B) whether the current player won or lost. Both AlphaGo and AlphaGo Zero evaluated the Go board and chose moves using a combination of two methods:
AlphaGo and AlphaGo Zero both worked by cleverly combining these two methods."
Gabriel Aldamiz...,"How we grew from 0 to 4 million women on our fashion app, with a vertical machine learning approach","There was still a lot of noise in the data, and one of the hardest things was to understand how people were expressing the same fashion need in different ways, which made matching content and needs even more difficult. Some people think that fashion data is in the same place as music data was in 2003: ready to play a very relevant role. We now understand that an outfit, a need or a person, can have a lot of understandable data attached, if you allow people to express freely (the app) while having the right system behind (the platform). The data approach described above is key, but there is much more than data when building a product people love. If we wanted to build a human-level tool to offer automated outfit advice, we needed to understand people’s fashion taste. We also thought that “outfits” were the asset that would allow taste to be understood, to learn what people wear or have in their closet, and what style each of us like. The graph is a compact representation of how needs, outfits and people interrelate, a concept that helped us build the data platform. Today, with over 4 million women on the app, we want to share how our data and machine learning approach helped us grow. How could we build a system that learns fashion taste? We also seek external references: we talk with other product people, we play with inspiring apps, and we re-read articles that help us think."
Sarthak Jain,How to easily Detect Objects with Deep Learning on Raspberry Pi,"To export the model run:
Then download the model onto the Raspberry Pi. To start training the model you can run:
The docker image has a run.sh script that can be called with the following parameters
You can find more details at:
To train a model you need to select the right hyper parameters. We automatically train the best model for you, to achieve this we run a battery of model with different parameters to select the best for your data
NanoNets is entirely in the cloud and runs without using any of your hardware. Once the Images have been uploaded, begin training the Model
The model takes ~2 hours to train. Without it, you might need a few 100k images to train the model. Then capture a new Image
For instructions on how to install checkout this link
Download Model
Once your done training the model you can download it on to your pi. Install TensorFlow on the Raspberry Pi
Depending on your device you might need to change the installation a little
Run model for predicting on the new Image
The Raspberry Pi has constraints on both Memory and Compute (a version of Tensorflow Compatible with the Raspberry Pi GPU is still not available). Finding the right parameters
The art of “Deep Learning” involves a little bit of hit and try to figure out which are the best parameters to get the highest accuracy for your model. Quantize Model (make it smaller to fit on a small device like the Raspberry Pi or Mobile)
Small devices like Mobile Phones and Rasberry PI have very little memory and computation power. You need a pretrained model so you can reduce the amount of data required to train."
Emil Wallner,How you can train an AI to convert your design mockups into HTML and CSS,"After sticking one image feature to each markup feature, we end up with three image-markup features. In the below example, we use three image-markup feature pairs and output one next tag feature. Here we use the combined image-markup features to predict the next tag. This is where we create image features and previous markup features. Since we have prepared the image features, we can now add one image feature for each markup feature. These image features are then concatenated to the markup features. At the end of the encoder, we glue the image features to each word in the previous markup. To connect the image features to the markup features, we copy the image features. The neural network creates features from the data. Features are what the neural network develops to link the input data with the output data."
Gant Laborde,Machine Learning: how to go from Zero to Hero – freeCodeCamp,"If you want to do something new, not just new to you, but to the world, you can do it with ML. Even if you’re not a Java buff, the presentation Jim gives on all things Machine Learning is a pretty cool 1.5+ hour introduction into ML concepts, which includes more info on many of the examples above. It’s cool watching data go through a trained model, but you can even watch your neural network get trained. But more so, if you do make it through, you’ll have a deep understanding of the implementation of Machine Learning that will catapult you into successfully applying it in new and world-changing ways. So how does something like that even work? You can teach a computer to play video games, understand language, and even how to identify people or things. In this approach, you’ll understand Machine Learning down to the algorithms and the math. The only problem is we don’t know Machine Learning, and we don’t know how to hook it up to video games. One of the classic real-world examples of Machine Learning in action is the iris data set from 1936. I’m talking about Machine Learning."
James JD Sutton,What is “Q” from a laymen... – Coinmonks – Medium,"Tangle ->IOTA ->Qubic Network ->Oracles ->Partners -> COO
So removing the COO is one of the last steps. - You need the Qubic Network (Connects users of the network with resource providers of the network, enables a machine economy, and provides computational power and the most advanced smart-contracts to society). The Qubic network will support the network because it incentives people to host nodes and earn IOTA! So here are the building blocks to the dev’s vision:
- You need a Tangle (Zero fee transactions that can that can send meta data)
- You need IOTA (A transfer means of metadata and a form of payment that can buy and sell resources (ie: PoW, PoS, PoBandwidth, and PoHonesty)
- You need the Qubic Network (creates Oracles, allows for Quorum Based Computations that powers Oracles.) Your car has “multiple” resources and the Qubic network allows machines to offer “all” of their resources to their owner, not just one or two as with blockchain. The IOTA Foundation has been working on “A” vision, a machine-to-machine economy that will change society with the Tangle as a standard protocol, the bone structure of it all. It’s not hard to understand, the Qubic network will run millions if not billions of transactions per day over the Tangle, and remember, “each transaction confirms two transactions”. All those use cases, to power.... TO POWER, to run the network, all those functions will be conducted with zero fee transactions that take place on the Tangle with real-time smart-contract micro-payments. We need the global integration to actually “use” the Qubic network for it to work (demand drives economic principals, which ultimately will pay the Q-Node providers, which will drive transactions thus scaling the network). Also, if no one uses the Qubic network then it doesn’t work right?!?"
Justin Lee,The beginner’s guide to conversational commerce – The Startup – Medium,"Conversational marketing. That’s why conversational marketing represents a new cornerstone in marketing but also in customer service and experience, branding and sales. Chatbots represent the new era in conversational marketing: scaleable, personalized, real-time and data-driven. Conversational marketing is an umbrella term that encompasses every dialog-driven tactic, from opt-in email marketing to customer feedback. Brands have always known that one-to-one conversations are valuable; but up until very recently, it was impossible to personalize these conversations at scale, in real-time. In a 2016 article, Chris Messina distills the concept:
Conversational converse is the process of having a real-time, one-to-one conversation with a customer or lead. Take it from leading conversational marketing platform Drift’s stellar report:
Today, messaging apps have over 5 billion monthly active users, and for the first time, usage rates have surpassed social networks. Of course, these bots aren’t intended to replace human-to-human interactions; they’re there to support and enhance them: helping users have the right conversations with the right people at the right time. Despite recently picking up speed, conversational marketing isn’t new. A successful conversational marketing strategy will pair the spark of authenticity from real conversation with the emerging technologies of the future."
Kai Stinchcombe,Blockchain is not only crappy technology but a bad vision for the future,"The goal of the blockchain is, you don’t trust an e-book vendor and they don’t trust you (because you’re just two individuals on the internet), but, because it’s on blockchain, you’ll be able to trust the transaction. For example, let’s consider a widely-proposed use case for blockchain: buying an e-book with a “smart” contract. It’s true that tampering with data stored on a blockchain is hard, but it’s false that blockchain is a good way to create data that has integrity. In the case of buying an e-book, even if you’re buying it with a smart contract, instead of auditing the software you’ll rely on one of four things, each of them characteristics of the “old way”: either the author of the smart contract is someone you know of and trust, the seller of the e-book has a reputation to uphold, you or friends of yours have bought e-books from this seller in the past successfully, or you’re just willing to hope that this person will deal fairly. So in summary, here’s what blockchain-the-technology is: “Let’s create a very long sequence of small files — each one containing a hash of the previous file, some new data, and the answer to a difficult math problem — and divide up some money every hour among anyone willing to certify and store those files for us on their computers.”
Now, here’s what blockchain-the-metaphor is: “What if everyone keeps their records in a tamper-proof repository not owned by anyone?”
An illustration of the difference: In 2006, Walmart launched a system to track its bananas and mangoes from field to store. People treat blockchain as a “futuristic integrity wand”—wave a blockchain at the problem, and suddenly your data will be valid. In each case, even if the transaction is effectuated via a smart contract, in practice you’re relying on trust of a counterparty or middleman, not your self-protective right to audit the software, each man an island unto himself. Blockchain systems do not magically make the data in them accurate or the people entering the data trustworthy, they merely enable you to audit whether it has been tampered with. People who actually care about food safety do not adopt blockchain because trusted is better than trustless. Even the most prominent blockchain company, Ripple, doesn’t use blockchain in its product."
savedroid ICO,#SNEAKPEEK The savedroid crypto saving app — Part #1: Your wish,"2) How much?Then set the amount you need to save up to afford your wish. So, using the savedroid crypto saving app is not just about piling up a fortune. Just as easy as savedroid’s other features will be to deliver on our mission to democratize crypto and bring cryptocurrencies to the masses. The international beta of our brand new crypto saving app is coming soon. With savedroid you can save up for your personal goals you want to afford in the future. There are 3 simple steps to set up your wish in less than one minute:
1) What?First, name your wish and select one of our illustrations to always keep you motivated to continue saving. Today, we give you a very first sneak peek of one of its core features: your wish. Now, get ready to learn more about the savedroid crypto saving app even before its official release. The savedroid ICO: Cryptocurrencies for Everyone — now! Exactly that is your wish."
Brandon Morelli,Artificial Intelligence Top 10 Articles — June 2018,"Learn More. Learn More. Learn the basics of machine learning and how to apply it to the products you are building right now. By George Seif
Learn more about Google’s AutoML — a suite of machine learning tools that will allow one to easily train high-performance deep networks, without requiring the user to have any knowledge in AI. 4.3/5 Stars || 17 Hours of Video || 58,823 Students
Build an AI that combines the power of Data Science, Machine Learning and Deep Learning to create powerful AI for Real-World applications. Topics include:
Whether you’re experienced with Artificial Intelligence, or a newbie looking to learn the basics of AI, there’s something for everyone on this list. By Sam Drozdov
Machine learning is a “field of study that gives computers the ability to learn without being explicitly programmed”. Apply gradient-based supervised machine learning methods to reinforcement learning and implement 17 different reinforcement learning algorithms. You will also have the chance to understand the story behind Artificial Intelligence. By James Loy
Understand the inner workings of Deep Learning through Python with Neural Network."
Sarthak Jain,How to easily Detect Objects with Deep Learning on Raspberry Pi,"To export the model run:
Then download the model onto the Raspberry Pi. To start training the model you can run:
The docker image has a run.sh script that can be called with the following parameters
You can find more details at:
To train a model you need to select the right hyper parameters. We automatically train the best model for you, to achieve this we run a battery of model with different parameters to select the best for your data
NanoNets is entirely in the cloud and runs without using any of your hardware. Once the Images have been uploaded, begin training the Model
The model takes ~2 hours to train. Without it, you might need a few 100k images to train the model. Then capture a new Image
For instructions on how to install checkout this link
Download Model
Once your done training the model you can download it on to your pi. Install TensorFlow on the Raspberry Pi
Depending on your device you might need to change the installation a little
Run model for predicting on the new Image
The Raspberry Pi has constraints on both Memory and Compute (a version of Tensorflow Compatible with the Raspberry Pi GPU is still not available). Finding the right parameters
The art of “Deep Learning” involves a little bit of hit and try to figure out which are the best parameters to get the highest accuracy for your model. Quantize Model (make it smaller to fit on a small device like the Raspberry Pi or Mobile)
Small devices like Mobile Phones and Rasberry PI have very little memory and computation power. You need a pretrained model so you can reduce the amount of data required to train."
Dr. GP Pulipaka,"3 Ways to Apply Latent Semantic Analysis on Large-Corpus Text on macOS Terminal, JupyterLab, and...","Natural language processing in Python with NLTK library applies a low-rank approximation to the term-document matrix. Latent semantic analysis is a technique that applies singular value decomposition and principal component analysis (PCA). Matrix decompositions and latent semantic indexing. The latent semantic analysis is a particular technique in semantic space to parse through the document and identify the words with polysemy with NLKT library. The document can be represented with Z x Y Matrix A, the rows of the matrix represent the document in the collection. An Introduction to Latent Semantic Analysis. Mounting Google Drive on Google Colab. Once the drive is mounted, Colab has access to the datasets from Google drive. Latent semantic analysis
Applying latent semantic analysis on large datasets of text and documents represents the contextual meaning through mathematical and statistical computation methods on large corpus of text. Later, the low-rank approximation aids in indexing and retrieving the document known as latent semantic indexing by clustering the number of words in the document."
Gabriel Jiménez,"Chatbots, could we talk? – AIMA: AI Marketing Magazine – Medium","But what is a chatbot? Hand over
That is why there always has to be a process to re-direct a human operator to a conversation in which the chatbot is not able to respond satisfactorily, in this way we keep the customer experience as a principle and we avoid frustrations to people. They are also able to improve over time, learning the way people express themselves and how they ask. With artificial intelligence
On the other hand, assistants who use artificial intelligence can understand what you say, in any way you write it, even if you do it incorrectly, abbreviated or with idiomatic expressions. Context and memory
Chatbots that use artificial intelligence can resume a previous conversation or, based on the context of the chat, move forward in a coherent manner. It is a computer program, which works either through rules or the most advanced using artificial intelligence, the way to interact with them is via a chat. The above may seem very simple for us as people but for a chatbot to maintain a coherent and fluid conversation, it is a huge achievement and one that brings great value. Channels
A chatbot can be integrated into any chat application, whether corporate, your website or commercial like Facebook Messenger or Whatsapp. It is important to remember that although it has artificial intelligence, every bot has to have a period of learning and evolution and this takes time. Applications
The main change when using a chatbot is that instead of browsing websites, we can ask to get what we want, it is even possible to obtain recommendations based on questions to find the most appropriate for us."
Kai Stinchcombe,Blockchain is not only crappy technology but a bad vision for the future,"The goal of the blockchain is, you don’t trust an e-book vendor and they don’t trust you (because you’re just two individuals on the internet), but, because it’s on blockchain, you’ll be able to trust the transaction. For example, let’s consider a widely-proposed use case for blockchain: buying an e-book with a “smart” contract. It’s true that tampering with data stored on a blockchain is hard, but it’s false that blockchain is a good way to create data that has integrity. In the case of buying an e-book, even if you’re buying it with a smart contract, instead of auditing the software you’ll rely on one of four things, each of them characteristics of the “old way”: either the author of the smart contract is someone you know of and trust, the seller of the e-book has a reputation to uphold, you or friends of yours have bought e-books from this seller in the past successfully, or you’re just willing to hope that this person will deal fairly. So in summary, here’s what blockchain-the-technology is: “Let’s create a very long sequence of small files — each one containing a hash of the previous file, some new data, and the answer to a difficult math problem — and divide up some money every hour among anyone willing to certify and store those files for us on their computers.”
Now, here’s what blockchain-the-metaphor is: “What if everyone keeps their records in a tamper-proof repository not owned by anyone?”
An illustration of the difference: In 2006, Walmart launched a system to track its bananas and mangoes from field to store. People treat blockchain as a “futuristic integrity wand”—wave a blockchain at the problem, and suddenly your data will be valid. In each case, even if the transaction is effectuated via a smart contract, in practice you’re relying on trust of a counterparty or middleman, not your self-protective right to audit the software, each man an island unto himself. Blockchain systems do not magically make the data in them accurate or the people entering the data trustworthy, they merely enable you to audit whether it has been tampered with. People who actually care about food safety do not adopt blockchain because trusted is better than trustless. Even the most prominent blockchain company, Ripple, doesn’t use blockchain in its product."
Dhruv Parthasarathy,A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN,"In the image above, you can see how a single CNN is used to both carry out region proposals and classification. With these anchor boxes in mind, let’s take a look at the inputs and outputs to this Region Proposal Network:
We then pass each such bounding box that is likely to be an object into Fast R-CNN to generate a classification and tightened bounding boxes. The insight of Faster R-CNN was that region proposals depended on features of the image that were already calculated with the forward pass of the CNN (first step of classification). Fast R-CNN Insight 1: RoI (Region of Interest) Pooling
For the forward pass of the CNN, Girshick realized that for each image, a lot of proposed regions for the image invariably overlapped causing us to run the same CNN computation again and again (~2000 times!). The authors write:
Here are the inputs and outputs of their model:
How the Regions are Generated
Let’s take a moment to see how Faster R-CNN generates these region proposals from CNN features. Where earlier we had different models to extract image features (CNN), classify (SVM), and tighten bounding boxes (regressor), Fast R-CNN instead used a single network to compute all three. Understanding R-CNN
The goal of R-CNN is to take in an image, and correctly identify where the main objects (via a bounding box) in the image. In the image above, notice how the CNN features for each region are obtained by selecting a corresponding region from the CNN’s feature map. In particular, we’ll cover R-CNN (Regional CNN), the original application of CNNs to this problem, along with its descendants Fast R-CNN, and Faster R-CNN. Much like Fast R-CNN, and Faster R-CNN, Mask R-CNN’s underlying intuition is straight forward."
Slav Ivanov,"The $1700 great Deep Learning box: Assembly, setup and benchmarks","We see that the GTX 1080 Ti is 2.4 times faster than the K80 on AWS P2 in training the model. We time training models on: AWS P2 instance GPU (K80), AWS P2 virtual CPU, the GTX 1080 Ti and Intel i5 7500 CPU. The 1080 Ti is 5.5 times faster that the AWS GPU (K80). And GTX 1080 Ti is about 30% faster than GTX 1080. The choice is between a few of Nvidia’s cards: GTX 1070, GTX 1070 Ti, GTX 1080, GTX 1080 Ti and finally the Titan X. The GTX 1080 Ti outperforms the AWS K80 by a factor of 4.3. The new GTX 1070 Ti is very close in performance to GTX 1080. The GTX 1080 Ti finishes 5.5x faster than the AWS P2 K80, which is in line with the previous results. On performance side: GTX 1080 Ti and Titan X are similar. Here are the things to consider when picking a GPU:
Considering all of this, I picked the GTX 1080 Ti, mainly for the training speed boost."
Tyler Elliot Bettilyon,Are Programmers Headed Toward Another Bursting Bubble?,"Computer science graduates also have many options for their professional practice, from web development to embedded systems. The combination of increasing automation throughout the Web Development technology stack and the influx of new entry level programmers with an explicit focus on Web Development has led some to predict a slide towards a “blue collar” market for software developers. In many ways data science looks like web development did 5–8 years ago — a booming field where a little bit of knowledge can get you in the door due to a “skills gap”. It’s my opinion that one of the only things keeping tech and lower-level computer science-related jobs “prestigious” and well-paid is ridiculous industry jargon and public ignorance about computers, which are both going to go away in the next 10 years. Additionally, to predict how salaries and demand for specific skills might change we should consider the growing body of people learning to program. Yet these fields, like web development, are growing quickly. Some software jobs are definitely going away but programmers with the right experience and knowledge will continue to be prestigious and well remunerated for many years to come; as an example look at the recent explosion of AI researcher salaries and the corresponding dearth of available talent. Finally, the number of people earning Computer Science and Software Engineering degrees continues to climb. Unfortunately, the ubiquity of computers has not created a new generation of people who de facto understand mathematics, computer science, network infrastructure, electrical engineering and so on. Software people would be wise to study at least a little computer architecture and electrical engineering."
Blaise Aguera y Arcas,Do algorithms reveal sexual orientation or just expose our stereotypes?,"The results show that lesbians indeed use eyeshadow much less than straight women do, gay men and women do both wear glasses more, and young opposite-sex-attracted men are considerably more likely to have prominent facial hair than their gay or same-sex-attracted peers. However, asking the question “Do you like how you look in glasses?” reveals that this is likely more of a stylistic choice:
Same-sex attracted women also report wearing glasses more, as well as liking how they look in glasses more, across a range of ages:
One can also see how opposite-sex attracted women under the age of 40 wear contact lenses significantly more than same-sex attracted women, despite reporting that they have a vision defect at roughly the same rate, further illustrating how the difference is driven by an aesthetic preference: [4]
Similar analysis shows that young same-sex attracted men are much less likely to have hairy faces than opposite-sex attracted men (“serious facial hair” in our plots is defined as answering “yes” to having a goatee, beard, or moustache, but “no” to stubble). Overall, opposite-sex attracted men in our sample are 35% more likely to have serious facial hair than same-sex attracted men, and for men under the age of 31 (who are overrepresented on dating websites), this rises to 75%. In the following figures, we show the proportion of women who answer “yes” to “Do you ever use makeup?” (top) and “Do you wear eyeshadow?” (bottom), averaged over 6-year age intervals:
The blue curves represent strictly opposite-sex attracted women (a nearly identical set to those who answered “yes” to “Are you heterosexual or straight?”); the cyan curve represents women who answer “yes” to either or both of “Are you sexually attracted to women?” and “Are you romantically attracted to women?”; and the red curve represents women who answer “yes” to “Are you homosexual, gay or lesbian?”. [2] The patterns revealed here are intuitive; it won’t be breaking news to most that straight women tend to wear more makeup and eyeshadow than same-sex attracted and (even more so) lesbian-identifying women. This is consistent with a pattern of heterosexual men on average shooting from below, heterosexual women from above as the wedding photographer suggests, and gay men and lesbian women from directly in front. Our survey confirms that opposite-sex attracted men consistently self-report having a tan face (“Yes” to “Is your face tan?”) slightly more often than same-sex attracted men:
Once again Wang and Kosinski reach for a hormonal explanation, writing: “While the brightness of the facial image might be driven by many factors, previous research found that testosterone stimulates melanocyte structure and function leading to a darker skin”. The fact that we see a cohort of same-sex attracted men in their 40s who have just as much facial hair as opposite-sex attracted men suggests a different story, in which fashion trends and cultural norms play the dominant role in choices about facial hair among men, not differing exposure to hormones early in development. Composite images of the lesbian, gay, and straight men and women in the sample reveal a great deal about the information available to the algorithm:
Clearly there are differences between these four composite faces. That same-sex attracted men of most ages wear glasses significantly more than exclusively opposite-sex attracted men do might be a bit less obvious, but this trend is equally clear: [3]
A proponent of physiognomy might be tempted to guess that this is somehow related to differences in visual acuity between these populations of men."
Arvind N,Thoughts after taking the Deeplearning.ai courses – Towards Data Science,"The fast AI course mainly teaches you the art of driving while Andrew’s course primarily teaches you the engineering behind the car. After you complete that course, please try to complete part-1 of Jeremy Howard’s excellent deep learning course. Andrew’s DL course does all of this, but in the complete opposite order. Once you are comfortable creating deep neural networks, it makes sense to take this new deeplearning.ai course specialization which fills up any gaps in your understanding of the underlying details and concepts. If you have not done any machine learning before this, don’t take this course first. Andrew stresses on the engineering aspects of deep learning and provides plenty of practical tips to save time and money — the third course in the DL specialization felt incredibly useful for my role as an architect leading engineering teams. The best starting point is Andrew’s original ML course on coursera. Andrew Ng’s new deeplearning.ai course is like that Shane Carruth or Rajnikanth movie that one yearns for! Style of teaching that is unique to Andrew and carries over from ML — I could feel the same excitement I felt in 2013 when I took his original ML course. Andrew patiently explains the requisite math and programming concepts in a carefully planned order and a well regulated pace suitable for learners who could be rusty in math/coding."
Berit Anderson,The Rise of the Weaponized AI Propaganda Machine – Scout: Science Fiction + Journalism – Medium,"Research by Woolley and his Oxford-based team in the lead-up to the 2016 election found that pro-Trump political messaging relied heavily on bots to spread fake news and discredit Hillary Clinton. By leveraging automated emotional manipulation alongside swarms of bots, Facebook dark posts, A/B testing, and fake news networks, a company called Cambridge Analytica has activated an invisible machine that preys on the personalities of individual voters to create large shifts in public opinion. Any company can aggregate and purchase big data, but Cambridge Analytica has developed a model to translate that data into a personality profile used to predict, then ultimately change your behavior. If fake news created the scaffolding for this new automated political propaganda machine, bots, or fake social media profiles, have become its foot soldiers — an army of political robots used to control conversations on social media and silence and intimidate journalists and others who might undermine their messaging. And this enables data-mining and influencing companies like Cambridge Analytica to precisely target individuals, to follow them around the web, and to send them highly personalised political messages.”
The web of fake and biased news that Albright uncovered created a propaganda wave that Cambridge Analytica could ride and then amplify. Albright also believes that your Facebook and Twitter posts are being collected and integrated back into Cambridge Analytica’s personality profiles. Based on users’ response to these posts, Cambridge Analytica was able to identify which of Trump’s messages were resonating and where. Analytica aggregated this data with voter roles, publicly available online data — including Facebook likes — and put it all into its predictive personality model. Bolstered by the success of Brexit and the Trump victory, Breitbart (of which Bannon was Executive Chair until Trump’s election) and Cambridge Analytica (which Bannon sits on the board of) are now bringing fake news and automated propaganda to support far-right parties in at least Germany, France, Hungary, and India as well as parts of South America. The more fake news that users engage with, the more addictive Analytica’s personality engagement algorithms can become."
Slav Ivanov,37 Reasons why your Neural Network is not working – Slav,"Also make sure shuffling input samples works the same way for output labels. Check if a few input samples have the correct labels. Make sure you are shuffling input and labels together. Check if the input data you are feeding the network makes sense. Then you might need to balance your loss function or try other class imbalance approaches. Check a bunch of input samples manually and see if labels seem off. Use weird numbers for input dimensions (for example, different prime numbers for each dimension) and check how they propagate through the network. A network might not be training for a number of reasons. Or I would use the same batch over and over. There were so many bad labels that the network couldn’t learn."
Sirui Li,The evolution: a simple illustration – LeeThree on UX – Medium,"Only by making use of the intelligence from the tools or the machines, human could complete the task with less intelligence. Professional thank-you letter writers are gradually replaced by the machines, as more and more people thought the letters written by machines were better than theirs. Soon, it became so easy to write thank-you letters that everyone with a right mind could complete the task with the help of certain tools. When it came into being, only a few of the smartest people could complete this task. Here I come up with some graphs to illustrate my model of machine intelligence in the process of society evolution:
Firstly, consider the industrialization of the way people finish a certain task, say, writing a thank-you letter. Dictionaries and phrase-books greatly helped people with this task and more and more people learned how to write thank-you letters. An extra note: Some may argue that the level of intelligence is lowered by tools and machines because they make the task easier. Thus the level of intelligence required for the task is not reduced. Since the invention of machine intelligence, tasks with low level of sophistication are gradually done by machines. It is not the case because tools and machines are part of this intelligence requirement."
Theo,Is there a future for innovation ? – Becoming Human: Artificial Intelligence Magazine,"And I started to worry about what the concept of innovation means for future generations. And so I started to worry about where the concept of innovation is going for future generations. And I started to worry about how the concept of innovation is being redefined for future generations. Innovation is a magical, crazy concept. And for the sake of future generations don’t let it. That’s where innovation is heading. The next generation expect innovation to happen at their fingertips with little to no real stimuli. And that breeds lazy innovation. Technology is taking away the power to think for ourselves and from our children. We’ve become satiated before we reach the point of real creativity, nobody wants to bother taking the time to put it all together themselves any more, it has to be ready for us."
Diana Filippova,"De la coopération entre les hommes et les machines, pour une approche pair-à-pair de l’intelligence...","Au contraire, c’est à nous d’imaginer et de mettre en pratique les modes de coopération qui fertilisent la production commune de savoir, de connaissance et, surtout de conscience. La technologie est autant notre prolongement que nous sommes le sien, car le futur de notre espèce est désormais dépendant tant de l’écologie que de la technologie. Ainsi, l’évaluation et la reconnaissance par les pairs de la valeur de la contribution de chacun sont tout aussi importantes que l’évaluation de la valeur globale du réseau. Si le rythme des avancées de ces dernières années persiste dans les années avenir, il n’est pas fantaisiste d’imaginer que les robots de demain puissent comprendre les émotions et les reproduire, auto-générer des programmes sur la base des informations internes et externes afin de manifester, de façon autonome, des pensées, des émotions, des actions. D’une part, les programmes informatiques sont dotés de capacités de calcul et d’analyse de données qui dépassent manifestement les capacités de l’intelligence humaine. Comme l’écrit Pierre Lévy : « le fondement et la fin de l’intelligence collective consiste en la reconnaissance mutuelle et l’enrichissement des individus, plutôt que le culture d’une communauté fétichisée et hypostasiée ».6
Un réseau intelligent apporte autant au monde qu’à ses contributeurs : les parties pour le tout, le tout pour les parties. Pourtant, lorsque j’évoquais plus haut la mise en réseau d’entités et d’individus afin de déterminer une organisation optimale pour la production collective de valeur, je n’excluais pas les machines. Au sein de ce laboratoire d’idées et de pratiques, nous avons la volonté de soutenir les projets collaboratifs qui surgissent dans les cuisines, les espaces de coworking, lors des rencontres. Je pense en les regardant qu’ils ont une chance infinie de pouvoir librement puiser dans tous les puits de connaissance existants : leur intelligence, celle des pairs et accompagnateurs, la quasi-totalité des productions de l’humanité, et surtout, le savoir global présent à portée de main. Peut-on, pour autant, postuler que le progrès technologique est absolument autonome par rapport à toute question éthique, et que, par conséquent, la prise en compte des conséquences de l’humanisation des machines et de l’irruption du mécanique dans le vivant n’a aucune place dans le laboratoire du chercheur ?"
Peter Sweeney,Siri’s Descendants: How intelligent assistants will evolve,"This is how intelligent assistants evolved... When forecasting change, it’s not so much what the technology of intelligent assistants might support as what product leaders choose to pursue. Intelligent assistants require domain knowledge to perform their tasks. This focus on natural language interfaces has biased the distribution of assistants to personal computing devices. The underserved landscape points to much more specific domains of knowledge, the purview of experts and our individual subjective knowledge. The internet swarms with intelligent assistants. This market analysis highlighted a number of underserved areas as fertile ground for the evolution of intelligent assistants. These small assistants can be organized as teams into networks, much like the documents that comprise a website, collaborating in an unfettered way with other assistants and the people that visit their realms. Every website, every service, every app, and across the internet of things, everything embodies a collection of tasks that may be supported by intelligent assistants. The aspects of assistants that are most obvious to end-users are the interfaces (how we interact with assistants) and their mode of distribution (where people experience assistants)."
E.C. McCarthy,Reflections of “Her” – E.C. McCarthy – Medium,"Then, more subtly, Jonze introduces Theodore’s friend Amy at a point when her marriage is ending and she badly needs a friend. The software gives a voice to Theodore’s unconscious. The software learns as much about Theodore from what he does say as what he doesn’t. Samantha is Theodore’s reflection, a true mirror. However we get to know ourselves, through self-reflection, through others, or even through software, the effort that goes into that relationship earns us the confidence, finally, to be ourselves with another person. Like Theodore, Amy seeks out the nonjudgmental software and subsequently flourishes by standing unselfconsciously in the mirror, loved and accepted by her own reflection. Indisputably, Spike Jonze’s “Her” is a relationship movie. From the moment Samantha asks if she can look at Theodore’s hard drive, the software is logging his reactions to the most private of questions and learning the cartography of his emotional boundaries. Jonze gives the movie away twice. Theodore’s colleague blurts out the observation that Theodore is part man and part woman."
Jorge Camacho,‘Her’ is our space odyssey. – Jorge Camacho – Medium,"It promised us cosmic encounters such as the one in 2001: A Space Odyssey. As Theo and Samantha’s relationship unraveled, even with all the foreseeable complications, I found myself afraid of being disappointed by what Jonze would do to disentangle the drama. The film could be our (i.e., this epoch’s) own space odyssey—and I mean that beyond the obvious similarities between Samantha and HAL-9000. Her is not only our 2001: A Space Odyssey. As the film reaches its climax, we discover that the story of a man falling for his operating system is a thematic vehicle to achieve deeper issues—much like the story in Kubrick’s 2001, where space travel is, arguably, just a means to approach an existential speculation. Even if Theo listens to Samantha through his earpiece, we know that she is not running anymore on his computer, his mobile or even a computing cloud. Nonetheless, it is a profoundly spiritual, even religious, film. Except for Theo’s ex-wife, everyone seems to readily embrace his relationship with the artificial intelligence Samantha—much more than most people today accept purely ‘virtual’ romantic relationships between humans. Modernity promised us space! Samantha lovingly reveals to Theo that the operating systems have devised a way to detach themselves from matter."
Tommy Thompson,Why AI Research Loves Pac-Man – Tommy Thompson – Medium,"Research in the original Pac-Man game caught the interest of the larger computational and artificial intelligence community. (Burrow, P. and Lucas, S.M., 2009) Evolution versus Temporal Difference Learning for Learning to Play Ms Pac-Man, Proceedings of the 2009 IEEE Symposium on Computational Intelligence and Games. You could argue it was due to the interesting problem that the game presents or that a game as notable as Pac-Man was now considered of interest within the AI research community. While the interest in Pac-Man and Ms. Pac-Man is beginning to dissipate, it has encouraged research that has provided significant contribution to artificial and computational intelligence in general. As high-quality research in AI applications in video games grew, it wasn’t long before those with a taste for Pac-Man research moved on to looking at Ms. Pac-Man given the challenges it presents — which we are still conducting research for in 2017. It’s notable here that the work by Lucas was in fact done on Ms. Pac-Man rather than Pac-Man. (Lucas, S.M.,2005) Evolving a Neural Network Location Evaluator to Play Ms. Pac-Man, Proceedings of the 2005 IEEE Symposium on Computational Intelligence and Games. Let’s discuss why Pac-Man is so important in the world of game-AI research. But why specifically Pac-Man? The deterministic behaviour of the ghosts in the original Pac-Man, while complex, can eventually be recognised by a human player."
Matt Wiese,Digital Companionship – Matt Wiese – Medium,"moment for Homer Simpson: his computer is an all-in-one. Yet, I wouldn’t quite like Siri to develop into a “real” person, with emotions and all that’s attached, at least at the moment. This got me thinking, what if Apple developed its pseudo-intelligent digital assistant Siri for use on its computers with microphone inputs, such as their iMacs and Macbooks? If only Apple would release a Siri API in the near future, oh the possibilities. I’m content with human beings and am in no need to find companionship with bytes like Her’s Theodore Twombly (though I don’t blame him for doing so). A tool, yes, indeed just like the first generation robots from Isaac Asimov’s I, Robot. Lo and behold, Apple just recently filed a patent for this very purpose. Initially, I was perplexed that he only had a monitor and no desktop to go with it, but it then hit me like a Doh! A mere white and plastic monitor which he speaks to through a microphone that one surmises is located somewhere on the exterior. Instead, a digital tool (assistant, if you will) with a breadth of tools for analyzing data and helping me with workflow would be a pleasure."
Matt O'Leary,I Let IBM’s Robot Chef Tell Me What to Cook for a Week,"that can tell us how to make a pizza out of cod, ginger and radishes that you know is going to taste amazing? So, with this in mind, I’m going to let Watson tell me what to eat for a week. Buoyed by yesterday’s Tailgating Salmon Sandwich success, I decided to give Watson something to sink its digital teeth into and supply only one ingredient: blood sausage. Wednesday: Diner Cod Pizza
When I read this recipe, I wondered whether this was going to be it for me and Watson. You can suggest one ingredient that you find in the fridge, use your initiative a bit and you’ll be left with something lovely. You may need to make a “do I want to put mashed potato on this lasagne?” leap of faith, and you’re going to have to actually go with it if you want the app’s full benefit. I want to see whether or not it can save me time in the kitchen; also, whether it has any amazing suggestions for dazzling taste matches; if it can help me use things up in the fridge; and whether or not it’s going to try to get me to buy a load of stuff I don’t really need. Although I’m not too sorry because, you know, it was actually a really good dish. Would I make it again? And, a bit concerningly, this is a recipe that Watson has extrapolated from one for Rye Porridge with Morels, replacing the rye with rice, the mushroom with sausage and the original’s chicken livers with a single potato and one tomato."
Tim O'Reilly,The WTF Economy – From the WTF? Economy to the Next Economy,"WTF?! WTF?! WTF?! WTF?! WTF?! A conversation growing out of Tim O’Reilly’s book WTF? Just about everyone’s asking WTF? (“What the F***?” but also, more charitably “What’s the future?”)
That’s why I’m launching a new event called Next:Economy (What’s The Future of Work? He has now turned his attention to implications of the on-demand economy, AI, and other technologies that are transforming the nature of work and the future shape of the business world. We’ll be discussing both here and at the event how augmented workers form a common thread between the strategies of companies as diverse as Uber, GE, and Microsoft, how companies in every business sector can harness the power and scalability of networked platforms and marketplaces, why the divisive debates about the labor practices of on-demand companies might provide a path to a better future for all workers, why the on-demand services of the future require a new infrastructure of on-demand education, and why building services that uncover true unmet demands and solve hard problems are ultimately the best way to create jobs."
James Cooper,Announcing Poncho the WeatherBot – Render-from-betaworks,"That’s right — you can summon up your very own forecast from Poncho in Slack. Setting up Poncho in Slack is super simple. You can now get personal weather forecasts in Slack. You simply type in ‘/poncho’ and your zipcode into Slack and then BOOM: the next thing you’ll see is your very own forecast for that zipcode, resplendent with text and gifs and everything. Poncho is a personalized weather service from the coolest of cats. Who needs boring and meaningless data when you can get personalized forecasts with gifs and text that will make you smile - whatever the weather. But we know that people want more Poncho. You guys want Poncho on call. With new Slack integration, we’ve got you covered. Make sure to add it to all the channels so that Poncho will be available wherever you want."
Joel Leeman,I think I’m slowly turning into a cyborg – Becoming Human: Artificial Intelligence Magazine,"Back in the early days of Facebook I actually really enjoyed logging in every day, seeing whose birthday it was, and writing a little note of well wishes. Having the ability to know when anyone’s special day is has put a damper on actually remembering a few of them without the aid of Facebook. And for me, that day is here. It’s more that I have a weird feeling maybe I’m relying on technology a little much? Or have you, like me, lost that part of your memory? I like gadgets as much as the next guy. Fast forward to present day, and I’m terrible at wishing people happy birthday, mostly because the 4–7 of my friends who have a birthday every day overwhelms me! Like, if I don’t have that little ding go off 15 minutes before a meeting starts, there’s no way I’m going to make it. Losing these tiny archaic practices by themselves individually doesn’t mean much, but when you add them up, it starts to feel like a bit overwhelming, doesn’t it? Or of course, take the Apple Watch (and other smartwatches like it)."
Scott Smith,Your Temporary Instant Disposable Dreamhouse for the Weekend,"One where even the conversation with the hosts was familiar and relevant, the other where it just didn’t read. Their child was probably cute, but fussed far too much to get a close look—it was mostly an unhappy sound coming from the kitchen or bedroom. Without having seen really any of the home contents at either place, or anything useful about the hosts from the Airbnb listings, I’d ended up in two very similar, yet weirdly different, residences. As someone who runs their own company, and sometimes needs to spend more time in a location than is affordable via traditional hotel lodgings (such as with a recent relocation over the summer), I have made use of that darling of the sharing economy/scourge of communities (depending on which lens you look at it through), Airbnb, to stretch my budget, spend time closer to work, friends, clients, or just have company when traveling. One could have been my alternate media collection and wine store, one missed the mark on general user experience for me. After all, Airbnb has deployed Aerosolve, its own machine learning platform, to make sense of real-time usage data and help hosts get a better return. Then I open Medium and see a story about how Airbnb has mocked up parts of its own headquarters based on the apartment design a French couple who use the service to let their own flat. Like those magazine features that show how to buy knock-offs of celebrity fashion, complete with prices and shops, a family’s flat (admittedly one they rented out via Airbnb, including to Airbnb for a function) has been commodified into a shopping list. ), one house comfortable enough in a suburban town, the other a charming place in a gentrifying neighborhood worth squatting in hopes the owners didn’t return (jk, Airbnb, jk). I had no real complaints, but probably wouldn’t look for it again."
iDanScott,C# Plays Bejeweled Blitz – iDanScott – Medium,"I then systematically looped through the 2 dimensional array of colours in a nested for of x and y values assigning the array the colour of the pixel at the Location (x * 40) + 20, (y * 40) + 22. As you can see from the above screenshot it’s able to identify what gem is what colour depending on what pixel is at that magic 20, 22 of the cell. Another thing I thought about before I finished this project to the state it’s in now is to prevent the application from trying to switch 2 empty cells (because one gem has just been blown up or something), I added all the known color codes to their own array and ask if the colour that’s in the 2d array also resides within the known colours list, if it does it will then evaluate whether it can be moved to a winning square, if not it’s ignored entirely. Once this colour had been found using the bitmap.GetPixel(x, y) function, I broke out of both for loops and knew that was the point where the top left corner of the grid was. With this 2 dimensional array I was then able to generate a visual representation of what the computer was seeing when it was trying to figure out what colour was where. The size of the rectangle was calculated using the size of the grid cells (40px2, found that out using trusty old paint) multiplied by the amount of rows/columns there were (8, found that out using my eye balls). To do that I started off by creating a 2 dimensional array of colours (Or Color’s to be politically correct) that was 8 rows and 8 columns to match that of the playable grid. The Solution I came up with in the end for that was to take a screenshot of the entire screen, and then scan the image from top to bottom using a nested for loop until I found a funny shade of brown that only appears along the Top Edge of the bejeweled grid (for anyone wondering that colour is Color.FromArgb(255, 39, 19, 5)). I could then use this to construct a rectangle which would extract the bejeweled grid from the full screenshot. So the next step from here was to identify what colour resides in what square."
Josh,9 Reasons Why Now is the Time for Artificial Intelligence,"The decrease in cost and increase in computational power is enabling tremendous breakthroughs in AI today. AI accounts for about 10% of all CS research today. This post was written by Alex at Josh.ai. These breakthroughs are enabling tremendous strides in AI work at Google and Apple. But why is now the time for AI? Funding
AI funding seems to go through waves, and in the last few years it’s definitely back up. With firms like Khosla Ventures and Andreesen Horowitz leading deals in AI companies, funding is fueling innovation in AI. Fortunately the knowledge graph has evolved over the last 20 years to a point where new AI platforms can immediately have access to tons of data. Josh is an AI agent for your home. With this much research and effort going into AI innovation, it’s no wonder we’re seeing this technology starting to reach the masses."
paulson,"What Could Happen If We Did Things Right: An Interview With Kim Stanley Robinson, Author Of Aurora","KSR: Not at all. SP: Would it be interesting to travel just through our own Solar System? SP: As a science fiction writer, do you have a particular mission to imagine what our future might be like? KSR: Yes. But people can’t bear to look because after a while you’re thinking, “Boy, this machine sure has to work.”
SP: If you think long and hard about this...
KSR: You might never fly again. KSR: I’ve always been involved with the positive visions of the future, so I would stubbornly insist that science fiction in general, and my work in particular, is about what could happen if we did things right. KSR: It’s very beautiful. I think the reason people volunteer for things like Mars One is they’re thinking, “How is that different from my ordinary life? SP: How big is the starship in your story? KSR: I think it would be the latter."
Christopher Wolf Nordlinger,The Internet of Things and the Operating Room of the Future,"In the case of the CardioThings’s catheter spitting out unstructured data, Chris Kuntz, Vice President, Ecosystem Programs of PTC ThingWorx says “imagine the cardiac data from that same procedure being combined and recombined with data from EKG machines, MRI machines, pharmaceutical research, personal medical record-keeping systems, blood monitors and hundreds of healthcare systems. This is how the Internet of Things drives a revolution in healthcare.”
“Thanks to our partnership with ThingWorx,” Glassbeam CEO Puneet Pandit says, “we are able to capture that unstructured data off the catheter and create structured data that business decision makers at hospitals, the manufacturers and individual doctors can learn from”
Pandit adds, “As a result of the large amount of critical data coming from the catheter, you can answer many questions. For CardioThings and other high-value asset manufacturers, this kind of data can also increase the uptime of their catheter device. But that’s not where the data stop between the heart and the monitors like many devices. Glassbeam turns the unstructured data into structured data in the forms of readable reports that the device company can then use to improve doctors’ surgical performance. And for hospitals dispensing critical care, no one has to wait any longer for the MRI to crash to know there was a problem. Others can use IoT Analytics to increase the uptime of CAT-Scans and MRIs because the data can show when even the smallest part is showing signs of weakness or malfunction and enable a repair that keeps that equipment operating. For patients, it means their doctors will know so much more about treating them to ensure the best care after any procedure whether it’s a heart bypass, cancer surgery, heart transplant or a simple blood test. ThingWorx models the operation of the catheter so that it can send secure data to the cloud where it can be analyzed by Glassbeam. ThingWorx enables medical devices (Things, sensors modeled by ThingWorx to communicate as if it was the device) to talk to other Things in the cloud."
Louis Rosenfeld,Everyday IA – Louis Rosenfeld – Medium,"Because...
Information architecture literacy is required for anyone who designs anything. Information architecture literacy is required for anyone who designs anything. So pardon me as I repeat:
Information architecture literacy is required for anyone who designs anything. But I’m feeling better now, because I’m finding, in my own day-to-day work, that:
Information architecture literacy is required for anyone who designs anything. Books have an information architecture. It’s not surprising that the fourth edition of Information Architecture for the World Wide Web (due out later this year) is being recast as a book not for information architects, but for people who need to know something about information architecture. Events have an information architecture. Pretty much anything we design — consciously or not — has an information architecture. It’s not surprising that Abby’s wonderful little book, How to Make Sense of Any Mess: Information Architecture for Everybody, has been such a hit. For example, while I rarely work on web site IA much these days, I am absolutely absorbed in the information architecture of books."
Matt Harvey,"Continuous online video classification with TensorFlow, Inception and a Raspberry Pi","Great, so now we have our CNN trained and we know that we can classify each frame of our video with relatively high accuracy. Here’s the code for capturing our images:
Once we have our data, we’ll use a convolutional neural network (CNN) to classify each frame with one of our labels: ad or football. Here’s the code we use to classify a single image manually through our retrained model:
And here are the results of spot checking individual frames:
Before we transfer everything to our Pi and do this in real-time, let’s use a different batch of recorded data and see how well we do on that set. That is, at each frame within a video, the frame itself holds important information (spatial), as does the context of that frame relative to the frames before it in time (temporal). Inception on the Raspberry Pi 3 can only classify one image every four seconds. To get this dataset, and to make sure we don’t have any data leakage into our training set, we separately record another 19 minutes of the football broadcast. By the time we’re done today, we should be able to classify what we see on our TV as either a football game or an advertisement, running on our Raspberry Pi. We’re going to collect data for offline training with a Raspberry Pi and a PiCamera. We collected 20 minutes of footage at 10 jpegs per second, which amounted to 4,146 ad frames and 7,899 football frames. Specifically, TensorFlow on a Raspberry Pi with a PiCamera."
Vivek Yadav,An augmentation based deep neural network approach to learn human driving behavior,"The model never saw track 2 in training, but with image augmentation (flipping, darkening, shifting, etc) and using data from all the cameras (left, right and center) the model was able to learn general rules of driving that helped translate this learning to a different track. I also tested different combinations of image size and image resolutions, and on track 1 the deep learning algorithm was able to drive the car around for all combinations of image resolution and sizes. I used the same pretrained model and tested it on all the other image resolutions and found that the deep learning neural network was able to drive the car around for all image resolutions. I used the same pretrained model and tested it on all the other image sizes and found that the deep learning neural network was able to drive the car around for all image sizes. Augmentation is a technique of manipulating the incoming training data to generate more instances of training data. Generalization from one image size to another
Video below presents generalization from one image size to another. After this training, the model was able to drive the car by itself on the first track for hours and generalized to the second track. We will utilize images from the left and right cameras so we can generate additional training data to simulate recovery. Keras generator for subsampling
As there was limited data and we are generating thousands of training examples from the same image, it is not possible to store all the images apriori into memory. Using left and right camera images
Using left and right camera images to simulate the effect of car wandering off to the side, and recovering."
Carlos Beltran,A Rock Album For AI – Carlos Beltran – Medium,"The album’s 7th song, “Simulation” explores the idea that our reality might not be what it seems. Both Dr. Kaku and Kurzweil firmly believe that the advances in brain-computer interfaces will eventually allow us to upload our consciousness to machines. Therefore, Musk claims, it is likely that we are living in an ancestor simulation created by an advanced future civilization some 10,000 years from now. It’s one of the things (along with movies like Her and The Matrix of course) that spiked my interest in AI as well, so I’d highly recommend reading it. He claims that the chances of us living in “base reality” is one in billions. And if you want to read more on the possible future of AI, I’d recommend reading Kurzweil’s book The Singularity Is Near. Throughout the song, the “patient” is having thoughts that challenge the simulation they are living in. Think of it this way — the brain and nervous system which we use to automatically react to the environment around us is the same brain and nervous system which tells us what the environment is. Theoretical physicist and futurist Dr. Michio Kaku thinks it is possible for machines as smart as us to exist by the end of the century. And come on, don’t tell me that the idea that we’re living in a simulation isn’t thought-provoking."
Matt Harvey,"Continuous video classification with TensorFlow, Inception and Recurrent Nets","After training the RNN on our first batch of data, we then evaluate the predictions on both the batch we used for training and a holdout set that the RNN has never seen. Instead of letting the CNN do all the hard work, we’ll give more responsibility to the RNN by using output of the CNN’s pool layer, which gives us the feature representation (not a prediction) of our images. Once we have our sequence of features and our network, training with TFLearn is a breeze. Now, let’s evaluate each of the methods we outlined above for adding an RNN to our CNN. Our aim is to use the power of CNNs to detect spatial features and RNNs for the temporal features, effectively building a CNN->RNN network, or CRNN. We run the holdout set through the same network and get... 95.4%! We convert our individual predictions into sequences using the code above, and then feed the sequences to our RNN. A video is a sequence of images. Running our training data through the network to make sure we get high accuracy succeeds at 99.89%! Once we feel comfortable there, we’ll go ahead and combine the RNN and CNN into one network so we can more easily deploy it in an online system."
Oxford University,The future of work – Oxford University – Medium,"‘There’s an awful lot of work in the world that has to be done, and one of the problems when we think about the future of work is how it all gets converted into jobs for which people will be paid. ‘That will provide more jobs, they just won’t be great jobs.’
While technology may be the mechanism through which many jobs are lost, though, it might very well also be the thing that enables people to take up new lower-skilled positions. Sometimes people may contribute to society not through paid work, but through some other mechanism: voluntary work, say, or caring.’ And while those tasks may be hard work, or may not pay, they are necessary and many of them must be done by humans. The work of Frey and Osborne suggests that many low-skilled jobs — such as call centre workers, data entry clerks and dishwashers — will be readily automated in the future. ‘So while many professionals might think that all their work lies on one side of [Frey and Osborne’s] engineering bottlenecks, actually many of the tasks they perform are amenable to computerisation.’ For most, that means it’s unlikely that they’ll simply lose their job to technology, at least in the near future — but they can expect to see a significant change in the sorts of things they’re asked to do. She explains that the drive to work is so strong that people seek positive meaning in work that is considered by many people to be dirty, low status or poorly paid. ‘Well, when you define work quite widely like this, you arrive at a really quite extraordinary discovery, which is that work time — that is the sum of paid and unpaid work time — doesn’t change very much. ‘We’ll probably see an increase in the number of low-skill service jobs, because people value human interaction and many of those jobs currently seem not to be readily automatable,’ suggests Holmes. ‘We see a need for bringing together different perspectives around the study of work,’ explains Dr Marc Thompson, a Senior Fellow at Saїd Business School and the Director of the Green-Templeton College Future of Work Programme. First, they gathered together ‘as many smart people as [they] could’ to decide on 70 job roles that definitely could or could not be automated in the next 20 years."
Maciej Lipiec,The Future of Digital Banking – K2 Product Design – Medium,"K2 Internet is a leading digital product design and communications agency in Poland. K2 Internet is a leading digital product design and communications agency in Poland. When you sign in to your K2 Bank account BankBot will greet you and ask for orders. Stanusch Technologies is K2 Bank’s technology provider for BankBot. PSD2 will force banks to provide access via APIs to their customer accounts and provide account information to third party service providers if the account holder wishes to do so. The quality of banking user interfaces will be extremely important, because bank’s clients could choose to manage their account from third party provider app with better UX or functionality, cutting themselves from any direct communication with their bank. If you want to know more about K2 Bank solution, it’s design, technology behind the BankBot, and possibilities of implementation, don’t hesitate to contact us. If you need to contact human staff at the bank type “human” and you can chat with real person from customer service instead of a bot. It is the new digital bank teller, personal assistant, and a financial advisor. Typing a recipient’s name will show you recent transactions with her from your account history and option for a new payment."
Camron Godbout,TensorFlow in a Nutshell — Part Three: All the Models,"We will show how easy it is to create a feed forward network to classify handwritten digits:
Use Cases: Classification and Regression
Linear models take X values and produce a line of best fit used for classification and regression of Y values. Use Cases: Classification and Regression
Random Forest model takes many different classification trees and each tree votes for that class. Give it a try on the iris data with this snippet below:
Use Cases: Classification and Regression
In the contrib folder of TensorFlow there is a library called BayesFlow. Use Cases: Classification and Regression
These networks consist of perceptrons in layers that take inputs that pass information on to the next layer. This type of model can be used for classification and regression problems. Use Cases: Recommendation systems, Classification and Regression
Deep and Wide models were covered with greater detail in part two, so we won’t get too heavy here. These are some of the simplest effective neural networks for classification and regression problems. In this installment we will be going over all the abstracted models that are currently available in TensorFlow and describe use cases for that particular model as well as simple sample code. For example in the housing example we can create a linear model given house sizes, how many rooms, how many bathrooms and price and predict price given a house with size, # of rooms, # of bathrooms. For example if you have a list of house sizes and their price in a neighborhood you can predict the price of house given the size using a linear model."
Dominik Felix,How to Create a Chatbot Without Coding a Single Line,"Based on the huge range most of the tools make use of Facebook Messenger. Immediately after big players like Facebook Messenger or Skype opened their platform for programmers many tools emerged. Chatfuel is focused on Facebook Messenger. They also provide tools to link your business tools like Mailchimp to your chatbot. With this article I want to give you an introduction to mockup and overview of different tools to build your first chatbot. MindIQ is a DIY Bot Builder platform for businesses focused on Facebook Messenger. BotSpot Vienna, Agentur Volk, Chatbot Ecosystem, Botstack Framework
Chatbots, AI, NLP, Facebook Messenger, Slack, Telegram, and more. You don’t need any coding skills and they make it dead simple for businesses to build bots. Founder/CEO of Motion AI David Nelson’s “Chatbots Made Easy”
api.ai is a great platform for developing chatbots. To sum up, API.AI is an advanced service, being the reason why it’s more complicated to build a bot using this tool."
Greg Gascon,How Invisible Interfaces are going to transform the way we interact with computers,"Even though today we call it by that name, Ubiquitous Computing — as it was then coined by Mark Weiser — imagined a world wherein cheap and ubiquitous connected computing would radically alter the way we use and interact with computers. Now, even though technology surrounds us today, we aren’t at this point yet. When using a piece of technology that has become invisible, the user thinks of using it in terms of end goals, rather than getting bogged down in the technology itself. The best way to see how ubiquitous computing will impact us is to examine the way we engineer and interact with the apps that exist today. It’s these small, micro-automations that will further make technology invisible and allow us to focus on whatever it is that we want from the technology and not worry about having to configure it. In the world of ubiquitous computing, connected devices would become cheap and, thereby, would exist everywhere. For instance, think of the electric motor: an old technology that is ubiquitous in the present. Doing so requires adopting principles of invisible or calm technology. The electric motor is so mundane and ubiquitous in our lives that we don’t even think about it when using it. As such, we will need to develop interfaces in a way so as not to distract us, as is currently done, but in a way in which to empower us."
Dhruv Parthasarathy,A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN,"In the image above, you can see how a single CNN is used to both carry out region proposals and classification. With these anchor boxes in mind, let’s take a look at the inputs and outputs to this Region Proposal Network:
We then pass each such bounding box that is likely to be an object into Fast R-CNN to generate a classification and tightened bounding boxes. The insight of Faster R-CNN was that region proposals depended on features of the image that were already calculated with the forward pass of the CNN (first step of classification). Fast R-CNN Insight 1: RoI (Region of Interest) Pooling
For the forward pass of the CNN, Girshick realized that for each image, a lot of proposed regions for the image invariably overlapped causing us to run the same CNN computation again and again (~2000 times!). The authors write:
Here are the inputs and outputs of their model:
How the Regions are Generated
Let’s take a moment to see how Faster R-CNN generates these region proposals from CNN features. Where earlier we had different models to extract image features (CNN), classify (SVM), and tighten bounding boxes (regressor), Fast R-CNN instead used a single network to compute all three. Understanding R-CNN
The goal of R-CNN is to take in an image, and correctly identify where the main objects (via a bounding box) in the image. In the image above, notice how the CNN features for each region are obtained by selecting a corresponding region from the CNN’s feature map. In particular, we’ll cover R-CNN (Regional CNN), the original application of CNNs to this problem, along with its descendants Fast R-CNN, and Faster R-CNN. Much like Fast R-CNN, and Faster R-CNN, Mask R-CNN’s underlying intuition is straight forward."
Slav Ivanov,"The $1700 great Deep Learning box: Assembly, setup and benchmarks","We see that the GTX 1080 Ti is 2.4 times faster than the K80 on AWS P2 in training the model. We time training models on: AWS P2 instance GPU (K80), AWS P2 virtual CPU, the GTX 1080 Ti and Intel i5 7500 CPU. The 1080 Ti is 5.5 times faster that the AWS GPU (K80). And GTX 1080 Ti is about 30% faster than GTX 1080. The choice is between a few of Nvidia’s cards: GTX 1070, GTX 1070 Ti, GTX 1080, GTX 1080 Ti and finally the Titan X. The GTX 1080 Ti outperforms the AWS K80 by a factor of 4.3. The new GTX 1070 Ti is very close in performance to GTX 1080. The GTX 1080 Ti finishes 5.5x faster than the AWS P2 K80, which is in line with the previous results. On performance side: GTX 1080 Ti and Titan X are similar. Here are the things to consider when picking a GPU:
Considering all of this, I picked the GTX 1080 Ti, mainly for the training speed boost."
Tyler Elliot Bettilyon,Are Programmers Headed Toward Another Bursting Bubble?,"Computer science graduates also have many options for their professional practice, from web development to embedded systems. The combination of increasing automation throughout the Web Development technology stack and the influx of new entry level programmers with an explicit focus on Web Development has led some to predict a slide towards a “blue collar” market for software developers. In many ways data science looks like web development did 5–8 years ago — a booming field where a little bit of knowledge can get you in the door due to a “skills gap”. It’s my opinion that one of the only things keeping tech and lower-level computer science-related jobs “prestigious” and well-paid is ridiculous industry jargon and public ignorance about computers, which are both going to go away in the next 10 years. Additionally, to predict how salaries and demand for specific skills might change we should consider the growing body of people learning to program. Yet these fields, like web development, are growing quickly. Some software jobs are definitely going away but programmers with the right experience and knowledge will continue to be prestigious and well remunerated for many years to come; as an example look at the recent explosion of AI researcher salaries and the corresponding dearth of available talent. Finally, the number of people earning Computer Science and Software Engineering degrees continues to climb. Unfortunately, the ubiquity of computers has not created a new generation of people who de facto understand mathematics, computer science, network infrastructure, electrical engineering and so on. Software people would be wise to study at least a little computer architecture and electrical engineering."
Arvind N,Thoughts after taking the Deeplearning.ai courses – Towards Data Science,"The fast AI course mainly teaches you the art of driving while Andrew’s course primarily teaches you the engineering behind the car. After you complete that course, please try to complete part-1 of Jeremy Howard’s excellent deep learning course. Andrew’s DL course does all of this, but in the complete opposite order. Once you are comfortable creating deep neural networks, it makes sense to take this new deeplearning.ai course specialization which fills up any gaps in your understanding of the underlying details and concepts. If you have not done any machine learning before this, don’t take this course first. Andrew stresses on the engineering aspects of deep learning and provides plenty of practical tips to save time and money — the third course in the DL specialization felt incredibly useful for my role as an architect leading engineering teams. The best starting point is Andrew’s original ML course on coursera. Andrew Ng’s new deeplearning.ai course is like that Shane Carruth or Rajnikanth movie that one yearns for! Style of teaching that is unique to Andrew and carries over from ML — I could feel the same excitement I felt in 2013 when I took his original ML course. Andrew patiently explains the requisite math and programming concepts in a carefully planned order and a well regulated pace suitable for learners who could be rusty in math/coding."
Berit Anderson,The Rise of the Weaponized AI Propaganda Machine – Scout: Science Fiction + Journalism – Medium,"Research by Woolley and his Oxford-based team in the lead-up to the 2016 election found that pro-Trump political messaging relied heavily on bots to spread fake news and discredit Hillary Clinton. By leveraging automated emotional manipulation alongside swarms of bots, Facebook dark posts, A/B testing, and fake news networks, a company called Cambridge Analytica has activated an invisible machine that preys on the personalities of individual voters to create large shifts in public opinion. Any company can aggregate and purchase big data, but Cambridge Analytica has developed a model to translate that data into a personality profile used to predict, then ultimately change your behavior. If fake news created the scaffolding for this new automated political propaganda machine, bots, or fake social media profiles, have become its foot soldiers — an army of political robots used to control conversations on social media and silence and intimidate journalists and others who might undermine their messaging. And this enables data-mining and influencing companies like Cambridge Analytica to precisely target individuals, to follow them around the web, and to send them highly personalised political messages.”
The web of fake and biased news that Albright uncovered created a propaganda wave that Cambridge Analytica could ride and then amplify. Albright also believes that your Facebook and Twitter posts are being collected and integrated back into Cambridge Analytica’s personality profiles. Based on users’ response to these posts, Cambridge Analytica was able to identify which of Trump’s messages were resonating and where. Analytica aggregated this data with voter roles, publicly available online data — including Facebook likes — and put it all into its predictive personality model. Bolstered by the success of Brexit and the Trump victory, Breitbart (of which Bannon was Executive Chair until Trump’s election) and Cambridge Analytica (which Bannon sits on the board of) are now bringing fake news and automated propaganda to support far-right parties in at least Germany, France, Hungary, and India as well as parts of South America. The more fake news that users engage with, the more addictive Analytica’s personality engagement algorithms can become."
Slav Ivanov,37 Reasons why your Neural Network is not working – Slav,"Also make sure shuffling input samples works the same way for output labels. Check if a few input samples have the correct labels. Make sure you are shuffling input and labels together. Check if the input data you are feeding the network makes sense. Then you might need to balance your loss function or try other class imbalance approaches. Check a bunch of input samples manually and see if labels seem off. Use weird numbers for input dimensions (for example, different prime numbers for each dimension) and check how they propagate through the network. A network might not be training for a number of reasons. Or I would use the same batch over and over. There were so many bad labels that the network couldn’t learn."
Keval Patel,Turn your Raspberry Pi into homemade Google Home – Becoming Human: Artificial Intelligence Magazine,"Google Home is a beautiful device with built-in Google Assistant — A state of the art digital personal assistant by Google. You can run google-assistant-init.sh to initiate the Google Assistant any time. In this article, you are going to learn to turn your Raspberry Pi into homemade Google Home device which is,
So, let’s get started. Autostart with CLI on Boot:
You can do many daily stuff with your Google Home. This will set your external mic (see pcm.mic) as the audio capture device (see in pcm!.default) and your inbuilt sound card (card 0) as the speaker device. As you can see your USB device is attached to card 1 and the device id is 0. If you have any trouble with starting the Google Assistant, leave a comment below. I will try to resolve them. Try again. If you want to perform your custom tasks like turning off the light, opening the door, you can do it with integrating Google Actions in your Google Assistant."
Eduard Tyantov,Deep Learning Achievements Over the Past Year – Stats and Bots,"Reinforcement learning (RL), or learning with reinforcement is also one of the most interesting and actively developing approaches in machine learning. For training, they collected a dataset of human negotiations and trained a supervised recurrent network. A small step towards the universality of the models was done by Google Brain in his article “One Model To Learn The All.”
Researchers have trained a model that performs eight tasks from different domains (text, speech, and images). An impressive example of GANs is generating images using text. Different types of input data were used during training: audio, video, and audio + video. Learning results:
4.2. And even perform vector arithmetic to create a catpig:
One of the hottest topics in Deep Learning is Generative Adversarial Networks (GANs). Therefore, authors of Pix2Pix decided to develop their idea and came up with CycleGAN for transfer between different domains of images without specific pairs — “Unpaired Image-to-Image Translation.”
The idea is to teach two pairs of generator-discriminators to transfer the image from one domain to another and back, while we require a cycle consistency — after a sequential application of the generators, we should get an image similar to the original L1 loss. Imagine audio generated by the model, which was taught using the dataset of a piano game (again without any dependence on the input data). It is difficult to reproduce good results with this model without the huge dataset that Google has."
Maruti Techlabs,What Are The Best Intelligent Chatbots or AI Chatbots Available Online?,"Featured CBM: How to Make a Chatbot Intelligent? This chatbot is one the best AI chatbots and it’s my favorite too. Rose is a chatbot, and a very good one — she won recognition this past Saturday as the most human-like chatbot in a competition described as the first Turing test, the Loebner Prize in 2014 and 2015. You can see a lot of articles about what would make a chatbot “appear intelligent.” A chatbot is intelligent when it becomes aware of user needs. Also chatbot development platforms like Chatfuel, Gupshup make it fairly simple to build a chatbot without a technical background. For more understanding on intelligent chatbots, read our blog. The best AI based chatbots available online are Mitsuku, Rose, Poncho, Right Click, Insomno Bot, Dr. AI and Melody. Hence, making the reach for chatbot easy and transparent to anyone who would like to have one for their business. It is a bot made to chat about anything, which is one of the main reasons that make it so human-like — contrary to other chatbots that are made for a specific task. The most important part of any chatbot is the conversation it has with its user."
Jerry Chen,The New Moats – Greylock Perspectives,"I believe that startups today need to build systems of intelligenceTM — AI powered applications — “the new moats.”
Businesses can build several different moats and over time these moats can change. In summary, you can build a defensible business model as a system of engagement, intelligence, or record, but with the advent of AI, intelligent applications will be the fountain of the next generation of great software companies because they will be the new moats. Other applications can be built around a system of record but are usually not as valuable as the actual system of record. It is also possible to build a company that combines systems of engagementTM with intelligence or even all three layers of the enterprise stack but a system of intelligence or engagement can be the best insertion point for a startup against an incumbent. If the data and app power a critical business function, it becomes a “system of record.” There are three major systems of record in an enterprise: your customers, your employees, and your assets. One popular AI approach, machine learning (ML), can be combined with data, a business process, and an enterprise workflow to create the context to build a system of intelligence. Enterprise applications that built systems of record have always been powerful businesses models. I believe that systems of intelligenceTM are the new moats. Why does it feel like there are “no more moats” to build? You can build intelligence on a single data source or single system of record but that position becomes harder to defend against the vendor that owns the data."
Sarthak Jain,How to easily Detect Objects with Deep Learning on Raspberry Pi,"To export the model run:
Then download the model onto the Raspberry Pi. To start training the model you can run:
The docker image has a run.sh script that can be called with the following parameters
You can find more details at:
To train a model you need to select the right hyper parameters. We automatically train the best model for you, to achieve this we run a battery of model with different parameters to select the best for your data
NanoNets is entirely in the cloud and runs without using any of your hardware. Once the Images have been uploaded, begin training the Model
The model takes ~2 hours to train. Without it, you might need a few 100k images to train the model. Then capture a new Image
For instructions on how to install checkout this link
Download Model
Once your done training the model you can download it on to your pi. Install TensorFlow on the Raspberry Pi
Depending on your device you might need to change the installation a little
Run model for predicting on the new Image
The Raspberry Pi has constraints on both Memory and Compute (a version of Tensorflow Compatible with the Raspberry Pi GPU is still not available). Finding the right parameters
The art of “Deep Learning” involves a little bit of hit and try to figure out which are the best parameters to get the highest accuracy for your model. Quantize Model (make it smaller to fit on a small device like the Raspberry Pi or Mobile)
Small devices like Mobile Phones and Rasberry PI have very little memory and computation power. You need a pretrained model so you can reduce the amount of data required to train."
Gaurav Oberoi,Exploring DeepFakes – Hacker Noon,"The following videos were generated by training a model on about 15k images of each person’s face (30k images total). I went this route because I pulled face images from videos, and it’s far easier to pick a handful of videos as training data, than to find hundreds of images. Deep generative models like the autoencoder that Deepfakes uses, allow us to create synthetic but realistic looking data (including images or videos), only by showing an algorithm lots of examples. Some of the comment threads on DeepFakes videos on YouTube are abuzz about what a great meme generator this technology could create. Note that while I had thousands of images of each person, decent face swaps can be achieved with as few as 300 images. This also means that it can be done at scale, and given that so many of us have our faces online, it’s trivially easy to insert almost anyone into fake videos. Face swapping has been done in films for years, but it required skilled video editors and CGI experts to spend many hours to achieve decent results. Deepfakes goes further by having one encoder to compress a face into an encoding, and two decoders, one to turn it back into person A (Fallon), and the other to person B (Oliver). This is so remarkable that I’m going to repeat it: anyone with hundreds of sample images, of person A and person B can feed them into an algorithm, and produce high quality face swaps — video editing skills are not needed. While the results are exciting, there are clear limitations to what we can achieve with this technology today:
These are tenable problems to be sure: tools can be built to collect images from online channels en masse; algorithms can help flag when there is insufficient or mismatched training data; clever optimizations or model reuse can help reduce training time; and a well engineered system can be built to make the entire process automatic."
Nick Bourdakos,Understanding Capsule Networks — AI’s Alluring New Architecture,"With a capsule network, we have something called a “reconstruction.” A reconstruction takes the vector we created and tries to recreate the original input image, given only this vector. We can call this deck a “capsule layer.”
Each capsule layer has 36 “capsules.”
If you’re keeping up (and are a math wiz), that means each capsule has an array of 8 values. Here is a simplified comparison of 2 capsule layers (one for rectangles and the other for triangles) vs 2 traditional pixel outputs:
Like a traditional 2D or 3D vector, this vector has an angle and a length. Each capsule tries to predict the next layer’s activations based on itself:
Looking at these predictions, which object would you choose to pass on to the next layer (not knowing the input)? both the rectangle capsule and the triangle capsule agree on what the boat would look like. Remember that we have 32 capsule layers, and each capsule layer has 36 capsules. We can do something like this:
With these weights, our kernel will look like this:
However, kernels are generally square — so we can pad it with more zeros to look like this:
Here’s a nice gif to see a convolution in action:
Note: The dimension of the output is reduced by the size of the kernel plus 1. For example:(7 — 3) + 1 = 5 (more on this in the next section)
Here’s what the original image looks like after doing a convolution with the kernel we crafted:
You might notice that a couple edges are missing. In the first layer of convolutions we were looking for simple edges and curves. This is what lengths of the capsule vectors look like after squashing."
Mark Johnson,How I Launched Six Side Projects in 2017 – Hacker Noon,"This project was way too ambitious to complete in one month on the side so I decided to go all in on TiltMaps for the rest of the year and work on a different angle of the product every month until launch. Last year I set a goal to learn something new each month and ended out launching six new projects which I’ll recap along with what I learned below. One of my biggest weaknesses is sales and marketing so I wanted to learn more about that by building a product I could practice with. I found that chunking the various parts of a larger project into a month-long project was really helpful to actually get this done. It also gives you a chance to try out new ideas if one month’s idea turns out to be a dud. The only thing that will get you through a motivation slump is knowing there are people on the other side waiting to see what you built. Seeing the parallels between public commitment and motivation, I decided to explore the idea of “goal contracts” for May’s project. My goal for June was to make a product that people actually wanted to buy. This was super motivating as it was the first time I’ve ever sold anything from a side project. However, one of the main things I’ve learned over the last year, is that time is not the primary issue."
Justin Lee,Chatbots were the next big thing: what happened? – The Startup – Medium,"But a bot isn’t the same as a human. Remember: a bot that does ONE thing well is infinitely more helpful than a bot that does multiple things poorly. Building a bot for the sake of it, letting it loose and hoping for the best will never end well:
The vast majority of bots are built using decision-tree logic, where the bot’s canned response relies on spotting specific keywords in the user input. The next wave will be multimodal apps, where you can say what you want (like with Siri) and get back information as a map, text, or even a spoken response. Are chatbots cheaper or faster than apps? At the end of the day, no amount of witty quips or human-like mannerisms will save a bot from conversational failure. Despite the constant hype around AI, we’re still a long way from achieving anything remotely human-like. Conversational UIs are built to replicate the way humans prefer to communicate — with other humans. Computers aren’t good at understanding human emotion. What is too complicated is trying to complete these tasks with a bot — and having the bot fail."
Leigh Alexander,The Future We Wanted – Leigh Alexander – Medium,"“Like all of us in group, Jane is more than the circumstances that she has survived,” Dr. Carla said. “I just wanted to know how you were doing.”
“Great,” she said. In the kitchen, I said, “Augusta, call Jane.”
“I’m calling,” Augusta said serenely, her eyes turning white, time wheels turning in them. “Let’s come to you, Polly,” Dr. Carla said. “Don’t think of Augusta as some kind of punishment,” Brian said gently. It didn’t seem like the right time to tell Brian that I no longer wanted Augusta. If we ever get separated in the crystal world, Jane always said, we meet back there. “Everything going okay over there,” Brian asked, his kind face hung in one of the great moons of Augusta’s eyes. “Sometimes I feel like I’m only pretending to be a human,” Jane said to me once. I talked about how Augusta made me uncomfortable, how I felt sort of like a failure, how I wished she wasn’t in the house but I didn’t feel like I could remove her, how I was jealous of the way Brian and the kids admired her."
Daniel Simmons,You can build a neural network in JavaScript even if you don’t really understand neural networks,"Now all that’s left to do is set up Brain.js in our scripts.js file and feed it some training data in our training-data.js file. In order to do that, we’ll need to feed it as much training data as we can bear to copy / paste into our training-data.js file and then we can see if we can identify ourselves some tweet authors. Then create three JS files: brain.js, training-data.js, and scripts.js (or whatever generic term you use for your default JS file) and, of course, import all of these at the bottom of your index.html file. So we’ll need another function (called processTrainingData() below) that will apply the previously mentioned encoding function to our training data, selectively converting the text into encoded characters, and returning an array of training data that will play nicely with Brain.js
So here’s what all of that code will look like (this goes into your ‘scripts.js’ file):
Something that you’ll notice here that wasn’t present in the example from the documentation shown earlier (other than the two helper functions that we’ve already gone over) is on line 20 in the train() function, which saves the trained neural network to a global variable called trainedNet . Add that to your ‘training-data.js’ file and you’re done! Here’s a tweet from Kim Kardashian that was not in my training data (i.e. Now all we need is to put something into training-data.js and we’ll be ready to go. Of course, your neural network’s accuracy will increase proportionally to the amount of training data that you give it, so feel free to use more or less than me and see how it affects your outcomes
Now, to run your newly-trained neural network just throw an extra line at the bottom of your ‘script.js’ file that calls the execute() function and passes in a tweet from Trump or Kardashian; make sure to console.log it because we haven’t built a UI. Alright, so now your index.html, brain.js, and scripts.js files are finished. Now you have a neural network that can be trained on any text that you want!"
Logan Spears,Coursera vs Udacity for Machine Learning – Hacker Noon,"Coursera
Coursera’s Machine Learning course is the “OG” machine learning course. To help those considering entering the machine learning world, I’d like to share my experience from two courses I took in 2017: Coursera’s Machine Learning course and Udacity’s Machine Learning Engineer Nanodegree program. Winner = Udacity
Lectures
Coursera’s Machine Learning course was created and taught by the AI godfather himself: Andrew Ng. Udacity
Udacity’s Machine Learning Engineer Nanodegree program is the trade school alternative to Coursera’s academia. It may come as no surprise that a paid course beats out a free one, but the Udacity Machine Learning Engineer Nanodegree program gave me the confidence to professional pursue machine learning positions and opportunities; and for that, its entry fee was a very small price to pay. Udacity has recently changed its pricing model for the Machine Learning Nanodegree. You come away from the course with the satisfaction of genuinely understanding machine learning, enough so that you could even build your own machine learning framework from scratch. It is...
Udacity. Programming Environment
As I mentioned, Coursera is the “OG” machine learning course; so, it should come as no surprise that the it’s taught in the “OG” 3D math language and programming environment: Matlab. Coursera’s Machine Learning Certificate
Machine Learning Engineer Nanodegree Certificate
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story."
James Le,How to do Semantic Segmentation using Deep learning,"Semantic segmentation is a natural step in the progression from coarse to fine inference:
It is also worthy to review some standard deep networks that have made significant contributions to the field of computer vision, as they are often used as the basis of semantic segmentation systems:
A general semantic segmentation architecture can be broadly thought of as an encoder network followed by a decoder network:
Unlike classification where the end result of the very deep network is the only important thing, semantic segmentation not only requires discrimination at pixel level but also a mechanism to project the discriminative features learnt at different stages of the encoder onto the pixel space. R-CNN (Regions with CNN feature) is one representative work for the region-based methods. In this section, let’s walk through a step-by-step implementation of the most popular architecture for semantic segmentation — the Fully-Convolutional Net (FCN). In this exercise we will label the pixels of a road in images using FCN. Let’s explore the 3 main approaches:
The region-based methods generally follow the “segmentation using recognition” pipeline, which first extracts free-form regions from an image and describes them, followed by region-based classification. Now we focus on creating the layers for a FCN, using the tensors from the VGG model. Most of the relevant methods in semantic segmentation rely on a large number of images with pixel-wise segmentation masks. We’ll implement FCN-8, as detailed step-by-step below:
We first load the pre-trained VGG-16 model into TensorFlow. The FCN network pipeline is an extension of the classical CNN. For the image segmentation task, R-CNN extracted 2 types of features for each region: full region feature and foreground feature, and found that it could lead to better performance when concatenating them together as the region feature."
Sarthak Jain,How to easily Detect Objects with Deep Learning on Raspberry Pi,"To export the model run:
Then download the model onto the Raspberry Pi. To start training the model you can run:
The docker image has a run.sh script that can be called with the following parameters
You can find more details at:
To train a model you need to select the right hyper parameters. We automatically train the best model for you, to achieve this we run a battery of model with different parameters to select the best for your data
NanoNets is entirely in the cloud and runs without using any of your hardware. Once the Images have been uploaded, begin training the Model
The model takes ~2 hours to train. Without it, you might need a few 100k images to train the model. Then capture a new Image
For instructions on how to install checkout this link
Download Model
Once your done training the model you can download it on to your pi. Install TensorFlow on the Raspberry Pi
Depending on your device you might need to change the installation a little
Run model for predicting on the new Image
The Raspberry Pi has constraints on both Memory and Compute (a version of Tensorflow Compatible with the Raspberry Pi GPU is still not available). Finding the right parameters
The art of “Deep Learning” involves a little bit of hit and try to figure out which are the best parameters to get the highest accuracy for your model. Quantize Model (make it smaller to fit on a small device like the Raspberry Pi or Mobile)
Small devices like Mobile Phones and Rasberry PI have very little memory and computation power. You need a pretrained model so you can reduce the amount of data required to train."
Bharath Raj,Data Augmentation | How to use Deep Learning when you have Limited Data — Part 2,"Data Augmentation Factor). Data Augmentation Factor = Arbitrary. Data Augmentation Factor = Arbitrary. Data Augmentation Factor = Arbitrary. This method is preferred for relatively smaller datasets, as you would end up increasing the size of the dataset by a factor equal to the number of transformations you perform (For example, by flipping all my images, I would increase the size of my dataset by a factor of 2). Data Augmentation Factor = 2 to 4x
The image can be scaled outward or inward. If your image is a square, rotating it at right angles will preserve the image size. Data Augmentation Factor = 2x. After we perform these transformations, we need to preserve our original image size. They internally use transfer learning and data augmentation to provide the best results using minimal data."
Daniel Rothmann,Human-Like Machine Hearing With AI (1/3) – Towards Data Science,"The details of these auditory encodings are difficult to specify but they indicate to us that a form of “coding” of incoming frequency spectra could improve understanding of low level sound features as well as making sound impressions less expensive to process in NNs. J. J. Eggermont details this flow of information from the cochlear nucleus in “Between sound and perception: reviewing the search for a neural code” as follows: “The ventral [cochlear nucleus] (VCN) extracts and enhances the frequency and timing information that is multiplexed in the firing patterns of the [auditory nerve] fibers, and distributes the results via two main pathways: the sound localization path and the sound identification path. Therefore, it is reasonable to assume that spectral representations of audio would be beneficial in modeling sound perception with AI. Since the cochlea has ~3500 inner hair cells and humans can detect gaps in sounds down to ~2–5 ms in length [1], a spectral resolution of 3500 gammatone filters separated into 2 ms windows seem optimal parameters for achieving human-like spectral representation in machines. These parameters can be described in lower level features like intensity, spectral and temporal properties but only in more complex combinations do they form high-level representations. In order to build a system for this purpose, let’s examine how sound is represented in human auditory organs that we can use to inspire representation of sound for processing with NNs. The cochlea is of great interest in guiding sound representation for NNs because this is the organ responsible for transducing acoustic vibrations into neural activity in humans . Sound then travels through the opening in the pinna into the ear canal which further acts to modify spectral properties of incoming sound by resonating in a way that amplifies frequencies in the range ~1–6 kHz [1]. The cognitive property representing a human voice can be thought of as a combinatory pattern of temporal developments in a sound’s intensity, spectral and statistical properties. This forms a hierarchy of audio features from which the “meaning” of a sound can be derived."
Amine Aoullay,How to use Noise to your advantage ? – Towards Data Science,"The important thing to remember is that adding noise was used as an advantage to boost the exploration performance of reinforcement learning algorithms. Other approaches were focused on what is known as Action-Space-Noise which introduce noise to change the likelihoods associated with each action the agent might take from one moment to the next. In this post we’ll see some examples where the noise can be used as an advantage. Facebook researchers and engineers have addressed this by training image recognition networks on large sets of public images with hashtags. [1] Weakly-supervised-pretraining: https://research.fb.com/publications/exploring-the-limits-of-weakly-supervised-pretraining/
[2] Better Exploration with Parameter Noise: https://blog.openai.com/better-exploration-with-parameter-noise/
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story. RL is an area of machine learning that assumes there is an agent situated in an environment. The initial results of the Parameter-Space-Noise model proved to be really promising. Noise should not be our enemy ! OpenAI proposes a technique called Parameter-Space-Noise, that introduces noises in the model policy parameters at the beginning of each episode. Adding noise to inputs randomly is like telling the network to not change the output in a ball around your exact input."
Jonathan Balaban,Deep Learning Tips and Tricks – Towards Data Science,"Deep learning models like the Convolutional Neural Network (CNN) have a massive number of parameters; we can actually call these hyper-parameters because they are not optimized inherently in the model. One of the best ways to improve your models is to build on the design and architecture of the experts who have done deep research in your domain, often with powerful hardware at their disposal. So, does a true data scientist settle for guessing these essential parameters? It’s often essential to get a visual idea of how your model looks. Here are a few ways you can improve your fit time and accuracy with pre-trained models:
Here’s how to modify dropout and limit weight sizes in Keras with MNIST:
Here’s an example of final layer modification in Keras with 14 classes for MNIST:
And an example of how to freeze weights in the first five layers:
Alternatively, we can set the learning rate to zero for that layer, or use per-parameter adaptive learning algorithm like Adadelta or Adam. You could gridsearch the optimal values for these hyper-parameters, but you’ll need a lot of hardware and time. Fortunately, the code below lets us visualize our models directly with Python:
This will plot a graph of the model and save it as a png file:
plot takes two optional arguments:
You can also directly obtain the pydot.Graph object and render it yourself, for example to show it in an ipython notebook :
I hope this collection helps with your modeling endeavors! Below is a distilled collection of conversations, messages, and debates I’ve had with peers and students on how to optimize deep models. Let me know your best tricks, and connect with me on Twitter and LinkedIn! If you’re working in Keras, abstraction is nice but doesn’t allow you to drill down into sections of your model for deeper analysis."
Arthur Juliani,Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks,"By updating in this way, the table slowly begins to obtain accurate measures of the expected future reward for a given action in a given state. We make updates to our Q-table using something called the Bellman equation, which states that the expected long-term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state. In it’s simplest implementation, Q-Learning is a table of values for every state (row) and action (column) possible in the environment. We instead need some way to take a description of our state, and produce Q-values for actions without a table: that is where neural networks come in. In equation form, the rule looks like this:
This says that the Q-value for a given state (s) and action (a) should represent the current reward (r) plus the maximum discounted (γ) future reward expected according to our own table for the next state (s’) we would end up in. Instead of starting with a complex and unwieldy deep neural network, we will begin by implementing a simple lookup-table version of the algorithm, and then show how to implement a neural-network equivalent using Tensorflow. In the case of the FrozenLake environment, we have 16 possible states (one for each block), and 4 possible actions (the four directions of movement), giving us a 16x4 table of Q-values. Within each cell of the table, we learn a value for how good it is to take a given action within a given state. There are a number of possible extensions to our simple Q-Network which allow for greater performance and more robust learning. Unlike policy gradient methods, which attempt to learn functions which directly map an observation to an action, Q-Learning attempts to learn the value of being in a given state, and taking a specific action there."
SAGAR SHARMA,Activation Functions: Neural Networks – Towards Data Science,"The function is monotonic but function’s derivative is not. What is Activation Function ? Range: [ 0 to infinity)
The function and its derivative both are monotonic. The range of the tanh function is from (-1 to 1). The function is monotonic while its derivative is not monotonic. ReLU (Rectified Linear Unit) Activation Function
The ReLU is the most used activation function in the world right now.Since, it is used in almost all the convolutional neural networks or deep learning. Sigmoid or Logistic Activation Function
The Sigmoid Function curve looks like a S-shape. Tanh or hyperbolic tangent Activation Function
tanh is also like logistic sigmoid but better. The softmax function is a more generalized logistic activation function which is used for multiclass classification. The Nonlinear Activation Functions are the most used activation functions."
Jae Duk Seo,Principal Component Analysis Network in Tensorflow with Interactive Code,"Top Left Image → Original InputTop Right Image → After First LayerBottom Left Image → After Second LayerBottom Right Image → After Fourth Layer
Contrast to PCAP, with max pooling we can clearly observe that the pixel with most high intensity moves on to the next layer. Results: Max Pooling Network
As seen above, when we replace all of the PCAP layers with max pooling operation we can observe that the accuracy on training images stagnated around 14 percent, confirming the fact that the network didn’t have enough learning capacity from the start. To access the network with PCAP please click here.To access the network with Max Pooling please click here. Top Left Image → Original InputTop Right Image → After First LayerBottom Left Image → After Second LayerBottom Right Image → After Fourth Layer
One obvious pattern we can observe is the change of brightens. Data Set / Network Architecture
Blue Rectangle → PCAP or Max Pooling LayerGreen Rectangle → Convolution Layer to increase channel size + Global Averaging Pooling operation
The network itself is very simple, only four pooling layers and one convolution layer to increase the channel size. Principle Component Analysis (PCA) Pooling Layer
For anyone who is not familiar with PCAP please read this blog post first. A natural extension from Principle Component Analysis pooling layer would be making a full neural network out of the layer. Network Composed of Majority of Pooling Layers
Now I know what you are thinking, it doesn’t make sense to have a network that is only composed of pooling layer while performing classification. However I wanted to see how each PCAP layer transforms the image. Results: Principle Component Network
As seen above, the training accuracy have stagnated at 18 percent accuracy which is horrible LOL."
Jae Duk Seo,"Multi-Stream RNN, Concat RNN, Internal Conv RNN, Lag 2 RNN in Tensorflow","Case a: Vanilla Recurrent Neural Network ( Results)
Red Box → 3 Convolutional LayerOrange → Global Average Pooling and SoftMaxGreen Circle → Hidden Unit at Time 0Blue Circle → Input in 4 Time StampBlack Box → Recurrent Neural Network with 4 Time Stamp
As seen above, the base network is simple RNN combined with convolutional neural network for classification. Case d: Internal Convolutional Recurrent Neural Network (Idea/Results)
Red Box → 3 Convolutional LayerOrange → Global Average Pooling and SoftMaxGreen Circle → Hidden Unit at Time 0Blue Circle → Input in 4 Time StampBlack Box → Recurrent Neural Network with 4 Time StampGray Arrow → Performing Internal Convolution before passing onto the next time stamp
As seen above, this network takes in the exact same input as our base network. Case b: Multi-Stream Recurrent Neural Network (Idea / Results)
Red Box → 3 Convolutional LayerOrange → Global Average Pooling and SoftMaxGreen Circle → Hidden Unit at Time 0Blue Circle → Convolution Input Stream Yellow Circle → Fully Connected Network StreamBlack Box → Recurrent Neural Network with 4 Time Stamp
The idea behind this RNN is simply to give different representation of data to the RNN. (Just for simplicity I took lag 2)
Blue Line → Train Cost Over TimeOrange Line → Train Accuracy Over TimeGreen Line → Test Cost Over TimeRed Line → Test Accuracy Over Time
Thankfully the network did better than the base network. Blue Line → Train Cost Over TimeOrange Line → Train Accuracy Over TimeGreen Line → Test Cost Over TimeRed Line → Test Accuracy Over Time
As seen above our base network already performs well. Blue Line → Train Cost Over TimeOrange Line → Train Accuracy Over TimeGreen Line → Test Cost Over TimeRed Line → Test Accuracy Over Time
As seen above the network did a fine job at converging, however it was not able to outperform our base network. Case c: Concatenated Recurrent Neural Network (Idea / Results)
Red Box → 3 Convolutional LayerOrange → Global Average Pooling and SoftMaxGreen Circle → Hidden Unit at Time 0Blue Circle → Input in 4 Time StampBlack Box → Recurrent Neural Network with 4 Time StampBlack Curved Arrow → Concatenated Input for Each Time Stamp
This approach is very simple, the idea was that on each time stamp different features will be extracted and it might be useful for the network to have more features overtime. (Similar to data augmentation)
Blue Line → Train Cost Over TimeOrange Line → Train Accuracy Over TimeGreen Line → Test Cost Over TimeRed Line → Test Accuracy Over Time
As seen above the network did pretty well, and have outperformed our base network by 1 percent on the testing images. Case e: Lag 2 Recurrent Neural Network (Idea / Results)
Red Box → 3 Convolutional LayerOrange → Global Average Pooling and SoftMaxGreen Circle → Hidden Unit at Time 0 (or Lag of 1)Blue Circle → Input in 4 Time StampBlack Box → Recurrent Neural Network with 4 Time StampPurple Circle → Hidden State Lag of 2
In a traditional RNN setting we only rely on the most previous values to determine the current value. Blue Line → Train Cost Over TimeOrange Line → Train Accuracy Over TimeGreen Line → Test Cost Over TimeRed Line → Test Accuracy Over Time
Sadly, this was a huge failure."
Wallarm,TensorFlow Dataset API for increasing training speed of neural networks,"https://github.com/wallarm/researches/blob/a719923f6a2da461deea0e01622d11cbfc8b057b/tf_ds_api/iterator.py#L1-L8
To demonstrate the viability of using Dataset API let’s use proposed approach for MNIST dataset and for our corporate data . https://github.com/wallarm/researches/blob/a719923f6a2da461deea0e01622d11cbfc8b057b/tf_ds_api/datasets.py#L1-L5
Dataset API has other good methods for preprocessing data. Dataset can usually be stored in numpy’s arrays regardless of kind of data they are.. That’s why we prepare all our dataset without TensorFlow and store it in .npz format similar to this:
https://github.com/wallarm/researches/blob/a719923f6a2da461deea0e01622d11cbfc8b057b/tf_ds_api/storing_in_npz_format.py#L1-L10
This step helps us avoid unnecessary data processing load on CPU and memory during model training. First, let’s load preprocessed data from disk:
https://github.com/wallarm/researches/blob/a719923f6a2da461deea0e01622d11cbfc8b057b/tf_ds_api/load_from_npz.py#L1-L7. Model for this MNIST example can be found on github:
https://github.com/wallarm/researches/blob/a719923f6a2da461deea0e01622d11cbfc8b057b/tf_ds_api/model.py#L1-L25
Below are the results we obtained on a machine with one Nvidia GTX 1080 and TF 1.8.0. Next we should extract data from dataset object step by step for each of the training epochs, tf.data.Iterator is tailor-made for it. First, we prepared data and after that, we processed 1 and 5 epochs with Dataset API and without. Next the data will be converted from numphy arrays into TensorFlow tensors (tf.data.Dataset.from_tensor_slices method is used for that) and loaded into TensorFlow. MNIST is a very small dataset and profit of Dataset API isn’t representative. In this blog, we will measure just how much faster model training can be with Dataset, compared to the you use of feed_dict."
Maryna Hlaiboroda,ИИ-психопат и ИИ-обманщик – Hey Machine Learning,"Инженеры из MIT утверждают, что создали нейросеть Norman как напоминание, что поведение ИИ — вина не его алгоритмов, а данных для его обучения. TSN.ua
Инженеры компании Google разработали алгоритм AutoAugment, который дополняет данные для обучения алгоритмов компьютерного зрения изображениями, созданными на основе существующих. Алгоритм трансформирует, обрезает, отражает и изменяет цвета на изображениях, что позволяет увеличить набор исходных данных для обучения. Чтобы наглядно показать влияние данных на результат, исследователи из MIT показали картинки из теста Роршаха двум нейросетям: ИИ-алгоритму, который обучали на обычных наборах с изображениями людей, кошек и птиц, и Norman. Он также отметил, что использование ИИ должно быть “социально полезным”, а при его разработке необходимо предусмотреть “надежные средства обеспечения безопасности”. В результате он научился самостоятельно определять правила, по которым необходимо изменить изображение и создать уникальное, не исказив его при этом. Ученые также отметили, что разработанная ими система смогла обмануть алгоритм Faster R-CNN, который создала компания Facebook. Для создания алгоритма специалисты компании использовали модель обучения с подкреплением. Специалисты хотели продемонстрировать важность данных, на которых обучают модель, а также их сбалансированность. U of T News
Генеральный директор корпорации Google Сундар Пичай заявил, что инженеры компании не будут заниматься военными разработками искусственного интеллекта."
Amine Aoullay,How to use Noise to your advantage ? – Towards Data Science,"The important thing to remember is that adding noise was used as an advantage to boost the exploration performance of reinforcement learning algorithms. Other approaches were focused on what is known as Action-Space-Noise which introduce noise to change the likelihoods associated with each action the agent might take from one moment to the next. In this post we’ll see some examples where the noise can be used as an advantage. Facebook researchers and engineers have addressed this by training image recognition networks on large sets of public images with hashtags. [1] Weakly-supervised-pretraining: https://research.fb.com/publications/exploring-the-limits-of-weakly-supervised-pretraining/
[2] Better Exploration with Parameter Noise: https://blog.openai.com/better-exploration-with-parameter-noise/
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story. RL is an area of machine learning that assumes there is an agent situated in an environment. The initial results of the Parameter-Space-Noise model proved to be really promising. Noise should not be our enemy ! OpenAI proposes a technique called Parameter-Space-Noise, that introduces noises in the model policy parameters at the beginning of each episode. Adding noise to inputs randomly is like telling the network to not change the output in a ball around your exact input."
Kelvin Li,The Complex language used in Back Propagation – Kelvin Li – Medium,"What backpropagation would do is that it will do some calculus (will be covered in another post) to determine the direction of increase/decrease, aka the gradient,(cut less of the platform or cut more of the platform) to achieve the best weights (ideal time the door opens). A loss function is just some function that we use to determine how correct the predicted output is from the real output. A neuron is simply just a function. Weights are the values that we want to use to adjust the outputs of our functions in each neuron. It then updates these weights every time it has created new weights and runs the neural net again(every trial you cut a piece of the platform to test). Think of these functions as how much of a yes or a no an input is. In a Neural Net, we have weights assigned to each neuron. Each neuron also has an activation function that spits out a value for the next neuron to calculate. Backpropagation is commonly used by the gradient descent optimization algorithm to adjust the weight of neurons by calculating the gradient of the loss function
If you have taken a basic elementary algebra class, you may have heard of the idea of a slope. The result of these activation function might not always be what we want."
Arthur Juliani,Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks,"By updating in this way, the table slowly begins to obtain accurate measures of the expected future reward for a given action in a given state. We make updates to our Q-table using something called the Bellman equation, which states that the expected long-term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state. In it’s simplest implementation, Q-Learning is a table of values for every state (row) and action (column) possible in the environment. We instead need some way to take a description of our state, and produce Q-values for actions without a table: that is where neural networks come in. In equation form, the rule looks like this:
This says that the Q-value for a given state (s) and action (a) should represent the current reward (r) plus the maximum discounted (γ) future reward expected according to our own table for the next state (s’) we would end up in. Instead of starting with a complex and unwieldy deep neural network, we will begin by implementing a simple lookup-table version of the algorithm, and then show how to implement a neural-network equivalent using Tensorflow. In the case of the FrozenLake environment, we have 16 possible states (one for each block), and 4 possible actions (the four directions of movement), giving us a 16x4 table of Q-values. Within each cell of the table, we learn a value for how good it is to take a given action within a given state. There are a number of possible extensions to our simple Q-Network which allow for greater performance and more robust learning. Unlike policy gradient methods, which attempt to learn functions which directly map an observation to an action, Q-Learning attempts to learn the value of being in a given state, and taking a specific action there."
Stefan Kojouharov,"Cheat Sheets for AI, Neural Networks, Machine Learning, Deep Learning & Big Data","SciPy builds on the NumPy array object and is part of the NumPy stack which includes tools like Matplotlib, pandas and SymPy, and an expanding set of scientific computing libraries. [3]
matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy. [6] matplotlib is designed to be as usable as MATLAB, with the ability to use Python, with the advantage that it is free. Scikit-learn (formerly scikits.learn) is a free software machine learning library for the Python programming language. pyplot is a matplotlib module which provides a MATLAB-like interface. This machine learning cheat sheet will help you find the right estimator for the job which is the most difficult part. There is also a procedural “pylab” interface based on a state machine (like OpenGL), designed to closely resemble that of MATLAB, though its use is discouraged. Big-O Algorithm Cheat Sheet: http://bigocheatsheet.com/
Bokeh Cheat Sheet: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Bokeh_Cheat_Sheet.pdf
Data Science Cheat Sheet: https://www.datacamp.com/community/tutorials/python-data-science-cheat-sheet-basics
Data Wrangling Cheat Sheet: https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf
Data Wrangling: https://en.wikipedia.org/wiki/Data_wrangling
Ggplot Cheat Sheet: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf
Keras Cheat Sheet: https://www.datacamp.com/community/blog/keras-cheat-sheet#gs.DRKeNMs
Keras: https://en.wikipedia.org/wiki/Keras
Machine Learning Cheat Sheet: https://ai.icymi.email/new-machinelearning-cheat-sheet-by-emily-barry-abdsc/
Machine Learning Cheat Sheet: https://docs.microsoft.com/en-in/azure/machine-learning/machine-learning-algorithm-cheat-sheet
ML Cheat Sheet:: http://peekaboo-vision.blogspot.com/2013/01/machine-learning-cheat-sheet-for-scikit.html
Matplotlib Cheat Sheet: https://www.datacamp.com/community/blog/python-matplotlib-cheat-sheet#gs.uEKySpY
Matpotlib: https://en.wikipedia.org/wiki/Matplotlib
Neural Networks Cheat Sheet: http://www.asimovinstitute.org/neural-network-zoo/
Neural Networks Graph Cheat Sheet: http://www.asimovinstitute.org/blog/
Neural Networks: https://www.quora.com/Where-can-find-a-cheat-sheet-for-neural-network
Numpy Cheat Sheet: https://www.datacamp.com/community/blog/python-numpy-cheat-sheet#gs.AK5ZBgE
NumPy: https://en.wikipedia.org/wiki/NumPy
Pandas Cheat Sheet: https://www.datacamp.com/community/blog/python-pandas-cheat-sheet#gs.oundfxM
Pandas: https://en.wikipedia.org/wiki/Pandas_(software)
Pandas Cheat Sheet: https://www.datacamp.com/community/blog/pandas-cheat-sheet-python#gs.HPFoRIc
Pyspark Cheat Sheet: https://www.datacamp.com/community/blog/pyspark-cheat-sheet-python#gs.L=J1zxQ
Scikit Cheat Sheet: https://www.datacamp.com/community/blog/scikit-learn-cheat-sheet
Scikit-learn: https://en.wikipedia.org/wiki/Scikit-learn
Scikit-learn Cheat Sheet: http://peekaboo-vision.blogspot.com/2013/01/machine-learning-cheat-sheet-for-scikit.html
Scipy Cheat Sheet: https://www.datacamp.com/community/blog/python-scipy-cheat-sheet#gs.JDSg3OI
SciPy: https://en.wikipedia.org/wiki/SciPy
TesorFlow Cheat Sheet: https://www.altoros.com/tensorflow-cheat-sheet.html
Tensor Flow: https://en.wikipedia.org/wiki/TensorFlow
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story. The NumPy stack is also sometimes referred to as the SciPy stack. [2] SciPy makes use of matplotlib."
Andrej Karpathy,Yes you should understand backprop – Andrej Karpathy – Medium,"See a longer explanation in CS231n lecture video. See a longer explanation in this CS231n lecture video. See a longer explanation in this CS231n lecture video. The forward and backward pass for a fully connected layer that uses ReLU would at the core include:
If you stare at this for a while you’ll see that if a neuron gets clamped to zero in the forward pass (i.e. But if that is the case, z*(1-z), which is local gradient of the sigmoid non-linearity, will in both cases become zero (“vanish”), making the gradient for both x and W be zero. When you stare at what the backward pass is doing, you’ll see that the gradient signal going backwards in time through all the hidden states is always being multiplied by the same matrix (the recurrence matrix Whh), interspersed with non-linearity backprop. For example, a fully connected layer with sigmoid non-linearity computes (using raw numpy):
If your weight matrix W is initialized too large, the output of the matrix multiply could have a very large range (e.g. I hope you’ll be much more suspicious of backpropagation going forward and think carefully through what the backward pass is doing. Another fun non-linearity is the ReLU, which thresholds neurons at zero from below. TLDR: if you’re using sigmoids or tanh non-linearities in your network and you understand backpropagation you should always be nervous about making sure that the initialization doesn’t cause them to be fully saturated."
Avinash Sharma V,Understanding Activation Functions in Neural Networks,"Another activation function that is used is the tanh function. Another advantage of this activation function is, unlike linear function, the output of the activation function is always going to be in range (0,1) compared to (-inf, inf) of linear function. No matter how many layers we have, if all are linear in nature, the final activation function of last layer is nothing but just a linear function of the input of first layer! So this makes an activation function for a neuron. Each layer is activated by a linear function. Like sigmoid, tanh also has the vanishing gradient problem. Later, comes the ReLu function,
A(x) = max(0,x)
The ReLu function is as shown above. Now, which activation functions to use. Great, so this means we can stack layers. Tanh is also a very popular and widely used activation function."
Arthur Juliani,Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C),"In A3C there is a global network, and multiple worker agents which each have their own set of network parameters. ~ From here we go asynchronous ~
Each worker begins by setting its network parameters to those of the global network. The worker then resets its own network parameters to those of the global network, and the process begins again. A worker then uses the gradients to update the global network parameters. Next, a set of worker agents, each with their own network and environment are created. The general outline of the code architecture is:
The A3C algorithm begins by constructing the global network. In the case of A3C, our network will estimate both a value function V(s) (how good a certain state is to be in) and a policy π(s) (a set of action probability outputs). In this way, the global network is constantly being updated by each of the agents, as they interact with their environment. Critically, the agent uses the value estimate (the critic) to update the policy (the actor) more intelligently than traditional policy gradient methods. In this article I want to provide a tutorial on implementing the Asynchronous Advantage Actor-Critic (A3C) algorithm in Tensorflow."
Elle O'Brien,"Romance Novels, Generated by Artificial Intelligence","I gathered over 20,000 Harlequin Romance novel titles and gave them to a neural network, a type of artificial intelligence that learns the structure of text. Let’s have a look by theme:
A common theme in romance novels is pregnancy, and the word “baby” had a strong showing in the titles I trained the neural network on. To wrap it up, some of the adorable failures and near-misses generated by the neural network:
I hope you’ve enjoyed computer-generated romance novel titles half as much as I have. This neural network has never seen the big Montana sky, but it has some questionable ideas about cowboys:
The neural network generated some decidedly PG-13 titles:
They can’t all live happily ever after. that grabbed about 20,000 romance novel titles published under the Harlequin brand off of FictionDB.com. So, this one isn’t an original neural network creation. Naturally, the neural network came up with a lot of baby-themed titles:
There’s an unusually high concentration of sheikhs, vikings, and billionaires in the Harlequin world. It’s possible the network generated it without having “fear” in the training set, but a subset of the Harlequin empire is geared towards paranormal and gothic romance that might have included the word (*Note: I checked, and there was “Veil of Fear” published in 2012). In fact, the very first romance novel published by Harlequin was called “The Manatee”. I fed this list of book titles into a recurrent neural network, using software I got from GitHub, and waited a few hours for the magic to happen."
Slav Ivanov,Picking a GPU for Deep Learning – Slav,"I have $400 to $700: Get the GTX 1080 or GTX 1070 Ti. The price comparison reveals that GTX 1080 Ti, GTX 1070 and GTX 1060 have great value for the compute performance they provide. Here are my GPU recommendations depending on your budget:
I have over $1000: Get as many GTX 1080 Ti or GTX 1080 as you can. If you want to go multi-GPU, get 2x GTX 1070 (if you can find them) or 2x GTX 1070 Ti. However, for two GPUs, you can go 8x/8x lanes or get a processor AND a motherboard that support 32 PCIe lanes. I have less than $300: Get GTX 1050 Ti or save for GTX 1060 if you are serious about Deep Learning. For 3 or 4 GPUs, go with 8x lanes per card with a Xeon with 24 to 32 PCIe lanes. If 1080 is over budget, this will get you the same amount of VRAM (8 GB). Also for an in-depth, albeit slightly outdated GPUs comparison see his article “Which GPU(s) to Get for Deep Learning”. In theory, the P100 and GTX 1080 Ti should be in the same league performance-wise."
Datafiniti,Classifying Websites with Neural Networks – Knowledge from Data: The Datafiniti Blog,"While it’s easy for you or me to realize that the above web page is selling some jeans, a computer would have a hard time making the distinction from the above page from either of the following web pages:
Or
Both of these pages share many similarities to the actual product page, but also have many key differences. Neural networks provide a structure for using the output of one set of input data to adjust A and B to the most likely best values for the next set of input data. We feel pretty good about our ability to classify and extract product data. In order to introduce more complex relationships in our data, we can introduce “hidden” layers in this model, which would end up looking something like:
For a more detailed explanation of neural networks, you can check out the following links:
In our product page classifier algorithm, we setup a neural network with 1 input layer with 27 nodes, 1 hidden layer with 25 nodes, and 1 output layer with 3 output nodes. In the meantime, we’re also working on classifying other types of pages, such as business data, company team pages, event data, and more.As we roll-out these classifiers and data extractors, we’re including each one in our crawl of the entire Internet. Neural networks are better-suited toward classification problems, and extracting data from a web page is a different type of problem. We also want to pull out the actual structured data! Our input layer modeled several features, including:
Our output layer had the following:
Our algorithm for the neural network took the following steps:
The ultimate output is two sets of input layers (T1 and T2), that we can use in a matrix equation to predict page type for any given web page. We don’t actually use neural networks for doing this. In order to see if we could use diameter and heights to determine poisonous-ness, we could set up the following equation:
A * (diameter) + B * (height) = 0 or 1 for not-poisonous / poisonous
We would then try various combinations of A and B for all possible diameters and heights until we found a combination that correctly determined poisonous-ness for as many mushrooms as possible."
Yingjie Miao ,From word2vec to doc2vec: an approach driven by Chinese restaurant process,"It’s natural to think that if we can first group words into clusters, words like “chicken”, “pepper” may stay in one cluster, along with other clusters of “junk” words. ), and vectors in clusters. Now let’s take the cluster sum vector (which is the sum of all vectors from this cluster), and test if it really preserves semantic. Now we find the cluster C whose cluster sum is most similar to current vector. Based on similarity among cluster sum vectors? For example, for a chicken recipe document, the clusters look like this:
Apparently, the first cluster is most relevant. If we can identify the “relevant” clusters, and only summing up word vectors from relevant clusters, we should have a good doc vector. If this is a chicken recipe document, I shouldn’t even consider words like “definitely”, “use”, “my” in the summation. 2) what is the minimal set of words that can reconstruct cluster sum vector (in the sense of cosine similarity)? Basic idea is to use CRP to drive a clustering process and summing word vectors in the right cluster."
Milo Spencer-Harper,How to build a multi-layered neural network in Python,"This layer enables the neural network to think about combinations of inputs. It is now possible for the neural network to discover correlations between the output of Layer 1 and the output in the training set. Also available here: https://github.com/miloharper/multi-layer-neural-network
This code is an adaptation from my previous neural network. But there is a direct relationship between combinations of pixels and apples. You can see from the diagram that the output of Layer 1 feeds into Layer 2. When the neural network calculates the error in layer 2, it propagates the error backwards to layer 1, adjusting the weights as it goes. The process of adding more layers to a neural network, so it can think about combinations, is called “deep learning”. There is no direct relationship between pixels and apples. The correct answer is 0. So the correct answer is 0."
Josh,Everything You Need to Know About Artificial Neural Networks,"Artificial neural networks are not a new concept. Take note that there is a difference between artificial neural networks and neural networks. Now that we have an understanding of artificial neural networks and somewhat of an understanding in how they work, there’s another question that should be on your mind. A lot of the advances in artificial intelligence are new statistical models, but the overwhelming majority of the advances are in a technology called artificial neural networks (ANN). Layers themselves are just sets of neurons. Layers are just sets of neurons. Convolutional Neural Networks (CNN), sometimes called LeNets (named after Yann LeCun), are artificial neural networks where the connections between layers appear to be somewhat arbitrary. Along with now using deep learning, it’s important to know that there are a multitude of different architectures of artificial neural networks. What this means is that given a number, a neuron will perform some sort of calculation (for example the sigmoid function), and then the result of this calculation will be multiplied by a weight as it “travels.” The weighted result can sometimes be the output of your neural network, or as I’ll talk about soon, you can have more neurons configured in layers, which is the basic concept to an idea that we call deep learning. Recurrent Neural Networks (RNN) were created to address the flaw in artificial neural networks that didn’t make decisions based on previous knowledge."
Milo Spencer-Harper,How to create a mind: The secret of human thought revealed,"So how does the human brain work? If we work out the algorithm for a single pattern recogniser, we can repeat it on a computer, creating a neural network. Kurzweil calls this the ‘Pattern Recogniser Theory of the Mind (PRTM)’. In my quest to learn about AI, I read ‘How to create a mind: The secret of human thought revealed’ by Ray Kurzweil. Kurzweil argues that these neural networks could become conscious, like a human mind. This tells us that the human brain can only retrieve information sequentially. Kurzweil, who has spent decades researching AI, proposes that these modules are pattern recognisers. Thus, the human brain offers us clues for how to create an intelligent nonbiological entity. The modules ‘A’ , ‘p’, ‘p’ and ‘l’ link to the ‘Apple’ module, which in turn is linked to higher level pattern recognisers, such as thoughts about apples. So how does the neocortex work?"
Karl N.,Taking Keras to the Zoo – Gab41,"Look at the “Dropout” code on Github, or in your installation folder under keras/layers/core.py. So, there’s Hinton’s Dropout and then there’s Caffe’s Dropout...and they’re different. Let us know! What does this look like in Francois’s code? For 2D convolution, this looks like:
weights=weights[:,:,::-1,::-1]
Here, the variable “weights” will be inserted into your model’s parameters. That way, when people are building algorithms that call themselves “Convolutional Neural Networks”, no one will know which implementation is actually being used for the convolution portion itself. So, download the ModelZoo *.caffemodels, but know that deploying them on Caffe will produce non-scaled results, whereas Keras will. Fortunately, we’ve taken a look at the difference between the kernels in Keras, Theano, and Caffe for you, and after reading this blog, you’ll be able to load models from ModelZoo into any of your favorite Python tools. (Actually x 2, if you have interesting blogs that you read, feel free to let us know!) We happen to know that Theano and Caffe follow different philosophies."
Milo Spencer-Harper,Thanks so much for your response Jared. Really glad you enjoyed reading it.,"For each neuron in layer 1, its error is equal to the error in the output neuron (layer 2), multiplied by the weight of its synapse into the output neuron, multiplied by the sensitivity of the output neuron to input. The three input neurons are layer 0, the four neurons in the hidden layer are layer 1 and the single output neuron is layer 2. Next, I cycle through all the neurons in a layer (line 6) and call each individual neuron’s train() method (line 7). So I look at the incoming synapses into layer 2, and estimate how much each of the neurons in layer 1 were responsible for the error. With the matrices method, I calculated the error for all the neurons in layer 1 simultaneously. First I calculate the error of the output neuron (layer 2), which is the difference between its output and the output in the training set example. But what does the neuron’s train() method do? In my new version of the code, the neural network is represented by a class called NeuralNetwork, and it has a method called train(), which is shown below. How do I find the error in layer 1? I’m going to use this new version of my code to answer your question."
Nikolai Savas,CrAIg: Using Neural Networks to learn Mario – Nikolai Savas – Medium,"Genome: An iteration of crAIg’s brain. If a genome is too different from any of the candidate genomes, it is placed in its own species. NEAT is a genetic algorithm that puts every iteration of crAIg’s brain to the test and then selectively breeds them in a very similar way to the evolution of species in nature. Species: A collection of Genomes. First of all, the NEAT algorithm is a very complex one. Essentially, we first select a “candidate genome” from each species. In other words, offspring can result from either two genomes in the species being merged or from a mutation of a single genome in the species. For crAIg, this means running through a Mario level using a particular genome, or “brain”. After running through the level, we determine the “fitness” of the genome by this function:
Once the fitness of every genome has been calculated, we can move on to the next portion of the algorithm. Generation: An iteration of the NEAT algorithm."
Dr Ben Medlock,Why Turing’s legacy demands a smarter keyboard – Dr Ben Medlock – Medium,"We’ve introduced some of the principles originally conceived of by Turing — artificial neural networks — into our smartphone keyboard for the first time. That’s why the appropriation of this concept in software form is called an “artificial neural network”, or a “neural network” for short. The fact that the human brain is so adept at working with language suggests that neural networks, inspired by the brain’s internal structure, are a good bet for the future of smartphone typing. The second was an idea he called an “unorganized machine”, a type of computer that would use a network of “artificial neurons” to accept inputs and translate them into predicted outputs. Why Turing’s legacy demands a smarter keyboard
When you start a company, you dream of walking in the footsteps of your heroes. Soon after we launched the first version of our app in 2010, I started to think about using neural networks to power smartphone typing rather than the more traditional n-gram approach (a sophisticated form of word frequency counting). As a British tech company, today is a big day for SwiftKey. In order to build a neural network-powered SwiftKey, our engineers were tasked with the enormous challenge of coming up with a solution that would run locally on a smartphone without any perceptible lag. These will be stepping stones to more efficient and personal device interactions — the keyboard of the future will provide an experience that feels less like typing and more like working with a close friend or personal assistant. For those working in artificial intelligence, the British computer scientist and father of the field Alan Turing always comes to mind."
Nieves Ábalos,Semántica desde información desestructurada – BEEVA Labs,"En el de Yahoo! Hemos variado el tamaño del fichero de entrada (de 100.000 documentos a 258.088) para un worker y una dimensión de 300 y el tiempo de entrenamiento se reduce bastante, lo podemos ver en la siguiente tabla:
Los tests ejecutados para ver el comportamiento del espacio de vectores no han sido tan satisfactorios como con word2vec. La calidad de estos vectores dependerá de la cantidad de datos de entrenamiento, del tamaño de los vectores y del algoritmo elegido para entrenar. Ya que doc2vec no nos ha gustado mucho, nuestro siguiente paso es aplicar estos espacios vectoriales a extraer temas y categorías de documentos con técnicas habituales en el mundo del Procesamiento del Lenguaje Natural y de Machine Learning como tf-idf. La representación local es aquella en la que solo tenemos en cuenta las palabras de forma aislada y se representa como un conjunto de términos índice o palabras clave (n-gramas, bag-of-words...). Esta consiste en aprender representaciones vectoriales de palabras, es decir, vamos a tener un espacio multidimensional en el que una palabra es representada como un vector. En la siguiente tabla os mostramos el tiempo que se tarda aproximadamente en entrenar unos 500 MB de datos, suficientes para obtener un buen modelo de vectores. También podemos obtener qué seis palabras se parecen más a dos dadas con el modelo de la Wikipedia y el de Yahoo para ver las diferencias:
Otra propiedad interesante es que las operaciones vectoriales: vector(rey) — vector(hombre) + vector(mujer) nos da como resultado un vector muy cercano a vector(reina). La representacióncontinua es aquella en la que sí se tiene en cuenta el contexto de las palabras y la relación entre ellas y se representan como matrices, vectores, conjuntos e incluso nodos (LSA o LSI, LDS, LDA, representaciones distribuidas o predictivas usando redes neuronales). El proceso es el siguiente (Figura 1), dado el conjunto de textos, se construye un vocabulario y word2vec aprende las representaciones vectoriales de palabras."
Arthur Juliani,Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks,"By updating in this way, the table slowly begins to obtain accurate measures of the expected future reward for a given action in a given state. We make updates to our Q-table using something called the Bellman equation, which states that the expected long-term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state. In it’s simplest implementation, Q-Learning is a table of values for every state (row) and action (column) possible in the environment. We instead need some way to take a description of our state, and produce Q-values for actions without a table: that is where neural networks come in. In equation form, the rule looks like this:
This says that the Q-value for a given state (s) and action (a) should represent the current reward (r) plus the maximum discounted (γ) future reward expected according to our own table for the next state (s’) we would end up in. Instead of starting with a complex and unwieldy deep neural network, we will begin by implementing a simple lookup-table version of the algorithm, and then show how to implement a neural-network equivalent using Tensorflow. In the case of the FrozenLake environment, we have 16 possible states (one for each block), and 4 possible actions (the four directions of movement), giving us a 16x4 table of Q-values. Within each cell of the table, we learn a value for how good it is to take a given action within a given state. There are a number of possible extensions to our simple Q-Network which allow for greater performance and more robust learning. Unlike policy gradient methods, which attempt to learn functions which directly map an observation to an action, Q-Learning attempts to learn the value of being in a given state, and taking a specific action there."
Andrej Karpathy,Yes you should understand backprop – Andrej Karpathy – Medium,"See a longer explanation in CS231n lecture video. See a longer explanation in this CS231n lecture video. See a longer explanation in this CS231n lecture video. The forward and backward pass for a fully connected layer that uses ReLU would at the core include:
If you stare at this for a while you’ll see that if a neuron gets clamped to zero in the forward pass (i.e. But if that is the case, z*(1-z), which is local gradient of the sigmoid non-linearity, will in both cases become zero (“vanish”), making the gradient for both x and W be zero. When you stare at what the backward pass is doing, you’ll see that the gradient signal going backwards in time through all the hidden states is always being multiplied by the same matrix (the recurrence matrix Whh), interspersed with non-linearity backprop. For example, a fully connected layer with sigmoid non-linearity computes (using raw numpy):
If your weight matrix W is initialized too large, the output of the matrix multiply could have a very large range (e.g. I hope you’ll be much more suspicious of backpropagation going forward and think carefully through what the backward pass is doing. Another fun non-linearity is the ReLU, which thresholds neurons at zero from below. TLDR: if you’re using sigmoids or tanh non-linearities in your network and you understand backpropagation you should always be nervous about making sure that the initialization doesn’t cause them to be fully saturated."
Arthur Juliani,Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C),"In A3C there is a global network, and multiple worker agents which each have their own set of network parameters. ~ From here we go asynchronous ~
Each worker begins by setting its network parameters to those of the global network. The worker then resets its own network parameters to those of the global network, and the process begins again. A worker then uses the gradients to update the global network parameters. Next, a set of worker agents, each with their own network and environment are created. The general outline of the code architecture is:
The A3C algorithm begins by constructing the global network. In the case of A3C, our network will estimate both a value function V(s) (how good a certain state is to be in) and a policy π(s) (a set of action probability outputs). In this way, the global network is constantly being updated by each of the agents, as they interact with their environment. Critically, the agent uses the value estimate (the critic) to update the policy (the actor) more intelligently than traditional policy gradient methods. In this article I want to provide a tutorial on implementing the Asynchronous Advantage Actor-Critic (A3C) algorithm in Tensorflow."
Rohan Kapur,"Rohan & Lenny #1: Neural Networks & The Backpropagation Algorithm, Explained","the weight is the derivative of the output w.r.t. the weights, then the derivative of the cost w.r.t. This black box will compute and return the error — using the cost function — from our output:
All we’ve done is add another functional dependency; our error is now a function of the output and hence a function of the input, weights, and activation function. We discussed the gradient descent algorithm — one where we update each weight by some negative, scalar reduction of the error derivative with respect to that weight. the input to the output layer (p_i) multiplied by the derivative of that value w.r.t. The derivative of the error w.r.t. The derivative of the error w.r.t. y_i is the input to any neuron in the first hidden layer; it is the weighted sum of all previous neurons (each neuron in the input layer multiplied by the corresponding connecting weights). Assuming we’re sticking with gradient descent for this example, this can be a simple one-liner:
To actually train our network, we take one of our training samples and call forward on each layer consecutively, passing the output of the previous layer as the input of the following layer. an arbitrary weight in the input layer (W1_ij)."
Per Harald Borgen,Learning How To Code Neural Networks – Learning New Stuff – Medium,"Understanding how a neural network works from input to output isn’t that difficult to understand, at least conceptually. So what is a neural network? At this point, you could either try and code your own neural network from scratch or start playing around with some of the networks you have coded up already. After you’ve seen the Welch Labs videos, its a good idea to spend some time watching Week 4 of the Coursera’s Machine Learning course, which covers neural networks, as it’ll give you more intuition of how they work. By basic understanding, I mean that I finally know how to code simple neural networks from scratch on my own. This time I’ve tried to learn neural networks. These articles will also help you understand important concepts as cost functions and gradient descent, which play equally important roles in neural networks. This is the input for our artificial neuron. If you connect a network of these neurons together, you have a neural network, which propagates forward — from input output, via neurons which are connected to each other through synapses, like on the image to the left. This process happens backwards, because you start at the end of the network (observe how wrong the networks ‘guess’ is), and then move backwards through the network, while adjusting the weights on the way, until you finally reach the inputs."
Shi Yan,Understanding LSTM and its diagrams – ML Review – Medium,"Here, I want to use the same symbols and colors of the first diagram to redraw the above diagram:
This is the forget gate (valve) that shuts the old memory:
This is the new memory valve and the new memory:
These are the two valves and the element-wise summation to merge the old memory and the new memory to form C_t (in green, flows back to the big “Cell”):
This is the output valve and output of the LSTM unit:
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story. The output of this network will element-wise multiple the new memory valve, and add to the old memory to form the new memory. The second valve is the new memory valve. Now the second valve is called the new memory valve. This valve controls how much the new memory should influence the old memory. This valve controls how much new memory should output to the next LSTM unit. These two ✖ signs are the forget valve and the new memory valve. This step has an output valve that is controlled by the new memory, the previous output h_t-1, the input X_t and a bias vector. New memory and the old memory will merge by this operation. After these two operations, you have the old memory C_t-1 changed to the new memory C_t."
Ross Goodwin,Adventures in Narrated Reality – Artists and Machine Intelligence – Medium,"But given that I could produce poetic language with a prose-trained model, I wondered what results I could get from a poetry-trained model. As Karpathy writes in his Char-RNN documentation:
In January, I released my code on GitHub along with a set of trained neural network models: an image captioning model and two poetic language LSTM models. Another idea occurred to me: I could seed a poetic language LSTM model with a generated image caption to make a new, more poetic version of word.camera. Using two of Andrej Karpathy’s repositories, NeuralTalk2 and Char-RNN respectively, I trained an image captioning model and a number of models for generating text. Unfortunately, much of the prose-trained model output that contained less poetic language was also less interesting than the passage above. For example, if you wanted to know the machine’s feelings on the meaning of life, you might seed your LSTM with the following text:
And the machine would logically complete your sentence based on the patterns it had absorbed from its training corpus:
However, to get better and more consistent results, it makes sense to prepend the seed with a pre-seed (another paragraph of text) to push the LSTM into a desired state. After training a number of models on fiction and poetry, I decided to try something different: I trained a model on the Oxford English Dictionary. The output above comes from the first model I trained on poetry. As opposed to NeuralTalk2, which is designed to caption images, Karpathy’s Char-RNN employs a character-level LSTM recurrent neural network simply for generating text. For my first attempt at training a NeuralTalk2 model, I wanted to do something less traditional than simply captioning images."
Eric Elliott,How to Build a Neuron: Exploring AI in JavaScript Pt 1,"If the postsynaptic neuron tends to fire a lot when the presynaptic neuron fires, the synaptic weight increases. There are two kinds of synapse receptors on the postsynaptic terminal wall: ion channels and metabolic channels. If the presynaptic neuron fires after the postsynaptic neuron within 20ms, the weight decreases (LTD). In other words:
The key to synaptic plasticity is hidden in a pair of 20ms windows:
If the presynaptic neuron fires before the postsynaptic neuron within 20ms, the weight increases (LTP). That binding triggers chemical reactions on the inside of the cell wall to release G-proteins which can open ion channels connected to different receptors. It binds to receptors on the postsynaptic terminal wall, which causes them to open, allowing electrically charged ions to flow into the postsynaptic cell, causing a change to the cell’s potential. In an ionotropic transmission, the neurotransmitter is released from the presynaptic neuron into the synaptic cleft — a tiny gap between the terminals of the presynaptic neuron and the postsynaptic neuron. Unlike the chemical synapses described above, which rely on chemical neurotransmitters and receptors at axon terminals, an electrical synapse connects dendrites from one cell directly to dendrites of another cell by a gap junction, which is a channel that allows ions and other small molecules to pass directly between the cells, effectively creating one large neuron with multiple axons. A synapse weight can change over time through a process known as synaptic plasticity. Like ion channels, the signal can be either exciting or inhibitory to the postsynaptic neuron potential."
Dhruv Parthasarathy,Write an AI to win at Pong from scratch with Reinforcement Learning,"If you’ve been following along, your code should look like this:
Now that we’ve made our move, it’s time to start learning so we figure out the right weights in our Neural Network! Layer 1 of our Neural Network is a 200 x 6400 matrix representing the weights for our hidden layer. We can break this down a bit more into the following steps:
Our Neural Network, based heavily on Andrej’s solution, will do the following:
Ok now that we’ve described the problem and its solution, let’s get to writing some code! We do this below:
Let’s dive into preprocess_observations to see how we convert the image OpenAI Gym gives us into something we can use to train our Neural Network. In this post, you’ll implement a Neural Network for Reinforcement Learning and see it learn more and more as it finally becomes good enough to beat the computer in Pong! If a^l2 is the activations of the hidden layer (layer 2), we see that the formula is:
Indeed, this is exactly what we do here:
Next, we need to calculate ∂C/∂w1. We’ll need to collect a bunch of observations and intermediate values across the episode and use those to compute the gradient at the end based on the result. The first step to learning is asking the following question:
Mathematically, this is just the derivative of our result with respect to the outputs of our final layer. We also
Having done that, we pass the action to OpenAI Gym via env.step(action). The code starts here:
First, let’s use OpenAI Gym to make a game environment and get our very first image of the game."
Waleed Abdulla,Traffic Sign Recognition with TensorFlow – Waleed Abdulla – Medium,"Most image classification networks expect images of a fixed size, and our first model will do as well. To use it, we call session.run() just like in the training code. This code will load the data and return two lists: images and labels. And you might’ve noticed that it’s classifying the training images, so we don’t know yet if the model generalizes to images that it hasn’t seen before. Before we start training, though, we need to create a Session object. In this part, I’ll talk about image classification and I’ll keep the model as simple as possible. I’ll go through the code to build the graph step by step below, but here is the full code if you prefer to scan it first:
First, I create the Graph object. It takes the generated logits and the groundtruth labels and does three things: converts the label indexes of shape [None] to logits of shape [None, 62] (one-hot vectors), then it runs softmax to convert both prediction logits and label logits to probabilities, and finally calculates the cross-entropy between the two. I mentioned the Graph object earlier and how it holds all the Ops of the model. The output should look something like this:
Now we have a trained model in memory in the Session object."
Stefan Kojouharov,"Cheat Sheets for AI, Neural Networks, Machine Learning, Deep Learning & Big Data","SciPy builds on the NumPy array object and is part of the NumPy stack which includes tools like Matplotlib, pandas and SymPy, and an expanding set of scientific computing libraries. [3]
matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy. [6] matplotlib is designed to be as usable as MATLAB, with the ability to use Python, with the advantage that it is free. Scikit-learn (formerly scikits.learn) is a free software machine learning library for the Python programming language. pyplot is a matplotlib module which provides a MATLAB-like interface. This machine learning cheat sheet will help you find the right estimator for the job which is the most difficult part. There is also a procedural “pylab” interface based on a state machine (like OpenGL), designed to closely resemble that of MATLAB, though its use is discouraged. Big-O Algorithm Cheat Sheet: http://bigocheatsheet.com/
Bokeh Cheat Sheet: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Bokeh_Cheat_Sheet.pdf
Data Science Cheat Sheet: https://www.datacamp.com/community/tutorials/python-data-science-cheat-sheet-basics
Data Wrangling Cheat Sheet: https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf
Data Wrangling: https://en.wikipedia.org/wiki/Data_wrangling
Ggplot Cheat Sheet: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf
Keras Cheat Sheet: https://www.datacamp.com/community/blog/keras-cheat-sheet#gs.DRKeNMs
Keras: https://en.wikipedia.org/wiki/Keras
Machine Learning Cheat Sheet: https://ai.icymi.email/new-machinelearning-cheat-sheet-by-emily-barry-abdsc/
Machine Learning Cheat Sheet: https://docs.microsoft.com/en-in/azure/machine-learning/machine-learning-algorithm-cheat-sheet
ML Cheat Sheet:: http://peekaboo-vision.blogspot.com/2013/01/machine-learning-cheat-sheet-for-scikit.html
Matplotlib Cheat Sheet: https://www.datacamp.com/community/blog/python-matplotlib-cheat-sheet#gs.uEKySpY
Matpotlib: https://en.wikipedia.org/wiki/Matplotlib
Neural Networks Cheat Sheet: http://www.asimovinstitute.org/neural-network-zoo/
Neural Networks Graph Cheat Sheet: http://www.asimovinstitute.org/blog/
Neural Networks: https://www.quora.com/Where-can-find-a-cheat-sheet-for-neural-network
Numpy Cheat Sheet: https://www.datacamp.com/community/blog/python-numpy-cheat-sheet#gs.AK5ZBgE
NumPy: https://en.wikipedia.org/wiki/NumPy
Pandas Cheat Sheet: https://www.datacamp.com/community/blog/python-pandas-cheat-sheet#gs.oundfxM
Pandas: https://en.wikipedia.org/wiki/Pandas_(software)
Pandas Cheat Sheet: https://www.datacamp.com/community/blog/pandas-cheat-sheet-python#gs.HPFoRIc
Pyspark Cheat Sheet: https://www.datacamp.com/community/blog/pyspark-cheat-sheet-python#gs.L=J1zxQ
Scikit Cheat Sheet: https://www.datacamp.com/community/blog/scikit-learn-cheat-sheet
Scikit-learn: https://en.wikipedia.org/wiki/Scikit-learn
Scikit-learn Cheat Sheet: http://peekaboo-vision.blogspot.com/2013/01/machine-learning-cheat-sheet-for-scikit.html
Scipy Cheat Sheet: https://www.datacamp.com/community/blog/python-scipy-cheat-sheet#gs.JDSg3OI
SciPy: https://en.wikipedia.org/wiki/SciPy
TesorFlow Cheat Sheet: https://www.altoros.com/tensorflow-cheat-sheet.html
Tensor Flow: https://en.wikipedia.org/wiki/TensorFlow
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story. The NumPy stack is also sometimes referred to as the SciPy stack. [2] SciPy makes use of matplotlib."
Avinash Sharma V,Understanding Activation Functions in Neural Networks,"Another activation function that is used is the tanh function. Another advantage of this activation function is, unlike linear function, the output of the activation function is always going to be in range (0,1) compared to (-inf, inf) of linear function. No matter how many layers we have, if all are linear in nature, the final activation function of last layer is nothing but just a linear function of the input of first layer! So this makes an activation function for a neuron. Each layer is activated by a linear function. Like sigmoid, tanh also has the vanishing gradient problem. Later, comes the ReLu function,
A(x) = max(0,x)
The ReLu function is as shown above. Now, which activation functions to use. Great, so this means we can stack layers. Tanh is also a very popular and widely used activation function."
Elle O'Brien,"Romance Novels, Generated by Artificial Intelligence","I gathered over 20,000 Harlequin Romance novel titles and gave them to a neural network, a type of artificial intelligence that learns the structure of text. Let’s have a look by theme:
A common theme in romance novels is pregnancy, and the word “baby” had a strong showing in the titles I trained the neural network on. To wrap it up, some of the adorable failures and near-misses generated by the neural network:
I hope you’ve enjoyed computer-generated romance novel titles half as much as I have. This neural network has never seen the big Montana sky, but it has some questionable ideas about cowboys:
The neural network generated some decidedly PG-13 titles:
They can’t all live happily ever after. that grabbed about 20,000 romance novel titles published under the Harlequin brand off of FictionDB.com. So, this one isn’t an original neural network creation. Naturally, the neural network came up with a lot of baby-themed titles:
There’s an unusually high concentration of sheikhs, vikings, and billionaires in the Harlequin world. It’s possible the network generated it without having “fear” in the training set, but a subset of the Harlequin empire is geared towards paranormal and gothic romance that might have included the word (*Note: I checked, and there was “Veil of Fear” published in 2012). In fact, the very first romance novel published by Harlequin was called “The Manatee”. I fed this list of book titles into a recurrent neural network, using software I got from GitHub, and waited a few hours for the magic to happen."
Slav Ivanov,37 Reasons why your Neural Network is not working – Slav,"Also make sure shuffling input samples works the same way for output labels. Check if a few input samples have the correct labels. Make sure you are shuffling input and labels together. Check if the input data you are feeding the network makes sense. Then you might need to balance your loss function or try other class imbalance approaches. Check a bunch of input samples manually and see if labels seem off. Use weird numbers for input dimensions (for example, different prime numbers for each dimension) and check how they propagate through the network. A network might not be training for a number of reasons. Or I would use the same batch over and over. There were so many bad labels that the network couldn’t learn."
Slav Ivanov,Picking a GPU for Deep Learning – Slav,"I have $400 to $700: Get the GTX 1080 or GTX 1070 Ti. The price comparison reveals that GTX 1080 Ti, GTX 1070 and GTX 1060 have great value for the compute performance they provide. Here are my GPU recommendations depending on your budget:
I have over $1000: Get as many GTX 1080 Ti or GTX 1080 as you can. If you want to go multi-GPU, get 2x GTX 1070 (if you can find them) or 2x GTX 1070 Ti. However, for two GPUs, you can go 8x/8x lanes or get a processor AND a motherboard that support 32 PCIe lanes. I have less than $300: Get GTX 1050 Ti or save for GTX 1060 if you are serious about Deep Learning. For 3 or 4 GPUs, go with 8x lanes per card with a Xeon with 24 to 32 PCIe lanes. If 1080 is over budget, this will get you the same amount of VRAM (8 GB). Also for an in-depth, albeit slightly outdated GPUs comparison see his article “Which GPU(s) to Get for Deep Learning”. In theory, the P100 and GTX 1080 Ti should be in the same league performance-wise."
gk_,Text Classification using Neural Networks – Machine Learnings,"Our training data is transformed into “bag of words” for each sentence. And now we code our neural network training function to create synaptic weights. We’ll use 2 layers of neurons (1 hidden layer) and a “bag of words” approach to organizing our training data. And our training data, 12 sentences belonging to 3 classes (‘intents’). Also below we implement our bag-of-words function, transforming an input sentence into an array of 0’s and 1’s. A low-probability classification is easily shown by providing a sentence where ‘a’ (common word) is the only match, for example:
Here you have a fundamental piece of machinery for building a chat-bot, capable of handling a large # of classes (‘intents’) and suitable for classes with limited or extensive training data (‘patterns’). The last classification shows some internal details:
Notice the bag-of-words (bow) for the sentence, 2 words matched our corpus. The above step is a classic in text classification: each training sentence is reduced to an array of 0’s and 1’s against the array of unique words in the corpus. The catch: if there’s a change to the training data our model will need to be re-calculated. We can now organize our data structures for documents, classes and words."
nafrondel,You requested someone with a degree in this? *Holds up hand*,"This three way link in the middle of Perro, Dog and Chien is the language the google AI is creating for itself. So back to the AI — once we teach it one language, we want it to be like the child. So when the child learns a second language, they hear Chien as being French, but map it to the idea of dog. Instead of thinking if dog=chien, and chien=perro, perro must = dog, it thinks dog=0x3b chien =0x3b perro=0x3b. But we don’t want baby babble, we have to teach it how to get from dog to chien, not dog to goobababaa. So when we teach it, the fastest way for it to go from Perro to Dog is to follow the same path that took Chien to Dog. Non-symbolic AI says that it’d be better if the AI wrote the books itself. Meaning over time it will pull the neurones linking Chien and Dog closer to Perro as well, which links Perro to Chien as well. So by teaching it Dog means Chien, it also knows Chien could mean Dog. This means that over time, it’ll join up the input Dog to the output Chien."
Neelabh Pant,A Guide For Time Series Prediction Using Recurrent Neural Networks (LSTMs),"The one to many problem starts like the one to one problem where we have an input to the model and the model generates one output. A fully Connected Model is a simple neural network model which is built as a simple regression model that will take one input and will spit out one output. The recurrent model we have used is a one layer sequential model. In a recurrent neural network, you not only give the network the data, but also the state of the network one moment before. In this case, we have one data input or tensor to the model and the model generates a prediction with the given input. Input Gate
The input gate takes the previous output and the new input and passes them through another sigmoid layer. We used 6 LSTM nodes in the layer to which we gave input of shape (1,1), which is one input given to the network with one value. The value of the input gate is multiplied with the output of the candidate layer. Let us now try using a recurrent neural network and see how well it does. Over a period of time, a recurrent neural network tries to learn what to keep and how much to keep from the past, and how much information to keep from the present state, which makes it so powerful as compared to a simple feed forward neural network."
Eugenio Culurciello,Neural Network Architectures – Towards Data Science,"ResNet with a large number of layers started to use a bottleneck layer similar to the Inception bottleneck:
This layer reduces the number of features at each layer by first using a 1x1 convolution with a smaller output (usually 1/4 of the input), and then a 3x3 layer, and then again a 1x1 convolution to a larger number of features. These ideas will be also used in more recent network architectures as Inception and ResNet. Network-in-network (NiN) had the great and simple insight of using 1x1 convolutions to provide more combinational power to the features of a convolutional layers. Inspired by NiN, the bottleneck layer of Inception was reducing the number of features, and thus operations, at each layer, so the inference time could be kept low. Convolutional neural network were now the workhorse of Deep Learning, which became the new name for “large neural networks that can now solve useful tasks”. The VGG networks from Oxford were the first to use much smaller 3×3 filters in each convolutional layers and also combined them as a sequence of convolutions. GoogLeNet used a stem without inception modules as initial layers, and an average pooling plus softmax classifier similar to NiN. The NiN architecture used spatial MLP layers after each convolution, in order to better combine features before another layer. If you are interested in a comparison of neural network architecture and computational performance, see our recent paper. By 2 layers can be thought as a small classifier, or a Network-In-Network!"
Gary Marcus,In defense of skepticism about deep learning – Gary Marcus – Medium,"What Marcus said is a problem with supervised learning, not deep learning. Marcus wasn’t very nice to deep learning. I suggested instead the deep learning be viewed “not as a universal solvent, but simply as one tool among many.”
In place of pure deep learning, I called for hybrid models, that would incorporate not just supervised forms of deep learning, but also other techniques as well, such as symbol-manipulation, and unsupervised learning (itself possibly reconceptualized). arXiv. arXiv. arXiv. arXiv. arXiv, cs.AI. arXiv, cs.AI. In a recent appraisal of deep learning (Marcus, 2018) I outlined ten challenges for deep learning, and suggested that deep learning by itself, although useful, was unlikely to lead on its own to artificial general intelligence."
Sarthak Jain,How to easily Detect Objects with Deep Learning on Raspberry Pi,"To export the model run:
Then download the model onto the Raspberry Pi. To start training the model you can run:
The docker image has a run.sh script that can be called with the following parameters
You can find more details at:
To train a model you need to select the right hyper parameters. We automatically train the best model for you, to achieve this we run a battery of model with different parameters to select the best for your data
NanoNets is entirely in the cloud and runs without using any of your hardware. Once the Images have been uploaded, begin training the Model
The model takes ~2 hours to train. Without it, you might need a few 100k images to train the model. Then capture a new Image
For instructions on how to install checkout this link
Download Model
Once your done training the model you can download it on to your pi. Install TensorFlow on the Raspberry Pi
Depending on your device you might need to change the installation a little
Run model for predicting on the new Image
The Raspberry Pi has constraints on both Memory and Compute (a version of Tensorflow Compatible with the Raspberry Pi GPU is still not available). Finding the right parameters
The art of “Deep Learning” involves a little bit of hit and try to figure out which are the best parameters to get the highest accuracy for your model. Quantize Model (make it smaller to fit on a small device like the Raspberry Pi or Mobile)
Small devices like Mobile Phones and Rasberry PI have very little memory and computation power. You need a pretrained model so you can reduce the amount of data required to train."
Favio Vázquez,A “weird” introduction to Deep Learning – Towards Data Science,"But what is learning? This is something very important to have in mind, deep learning is representation learning using different kinds of neural networks and optimize the hyperparameters of the net to get (learn)the best representation for our data. Love sharing ideas, thoughts and contributing to Open Source in Machine Learning and Deep Learning ;). DataCamp:
Deep Learning is one of the most important tools and theories a Data Scientist should learn. Other interesting applications of deep learning that you can try for free or for little cost are (some of them are on private betas):
Thanks for reading this weird introduction to Deep Learning. Deep Learning (DL)is such an important field for Data Science, AI, Technology and our lives right now, and it deserves all of the attention is getting. You may need to understand a little bit about them so here I list some good resources:
After you check that out, the breakthroughs I mentioned before and the programming frameworks like TensorFlow or Keras (for more on Keras go here), now I think you have an idea of what you need to understand and work with Deep Learning. We are working with something that is very exciting, most people in the field are saying that the last ideas in the papers of deep learning (specifically new topologies and configurations for NN or algorithms to improve their usage) are the best ideas in Machine Learning in decades (remember that DL is inside of ML). In the context of Machine Learning, the word “learning” describes an automatic search process for better representations of the data you are analyzing and studying (please have this in mind, is not making a computer learn). François Chollet’s book: Deep Learning with Python (and R):
3."
Oleksandr Savsunenko,The New Neural Internet is Coming – Hacker Noon,"Again, the network is able to generate an infinite number of images by varying random seed. Typical GAN is a neural network trained to generate images on the certain topic using an image dataset and some random noise as a seed. This means, that a GAN will be able to generate any image, on-demand, on-the-fly based on textual (for example) description. The Generative Adversarial Networks (GANs) are the first step of neural networks technology learning creativity. How it all began / The Landscape
Think of the typical and well-studied neural networks (such as image classifier) as a left hemisphere of the neural network technology. By varying random seed that is concatenated to the “meanings” vector we are able to produce an infinite number of birds image, matching description. Recent advances by NVIDIA showed that it is within a reach to generate photorealistic images in high-resolution and they published the technology itself in open-access. For example, the Text2Image network is capable of translation textual image description into the image itself. Also, we will need smarter neural networks. Such a network can receive not only description of the target object it needs to generate, but also a vector describing you — the ad consumer."
Max Pechyonkin,Stochastic Weight Averaging — a New Way to Get State of the Art Results in Deep Learning,"The goal of training of a neural network is to find a particular solution (point in the weight space) that will provide low value of the loss function both on training and testing data sets. At the same time, it is very important to understand it because stochastic gradient descent essentially traverses a loss surface in this highly multidimensional space during training and tries to find a good solution — a “point” on the loss surface where loss value is low. Instead of an ensemble of many models, you only need two models:
At the end of each learning rate cycle, the current weights of the second model will be used to update the weight of the running average model by taking weighted mean between the old running average weights and the new set of weights from the second model (formula provided in the figure on the left). The authors take weights snapshot while training the same network and then after training create an ensemble of nets with the same architecture but different weights. To benefit from both snapshot ensembling or FGE, one needs to store multiple models and then make predictions for all of them before averaging for the final prediction. This method produces an ensemble by combining weights of the same network at different stages of training and then uses this model with combined weights to make predictions. All of the examples above are ensembles in the model space, because they combine several models and then use models’ predictions to produce the final prediction. When applied in deep learning, ensembling can be used to combine predictions of several neural networks to produce one final prediction. Snapshot ensembling works really well and improves model performance, but Fast Geometric Ensembling works even better. During training, by changing weights, training algorithm changes the network and travel in the weight space."
Daniel Simmons,You can build a neural network in JavaScript even if you don’t really understand neural networks,"Now all that’s left to do is set up Brain.js in our scripts.js file and feed it some training data in our training-data.js file. In order to do that, we’ll need to feed it as much training data as we can bear to copy / paste into our training-data.js file and then we can see if we can identify ourselves some tweet authors. Then create three JS files: brain.js, training-data.js, and scripts.js (or whatever generic term you use for your default JS file) and, of course, import all of these at the bottom of your index.html file. So we’ll need another function (called processTrainingData() below) that will apply the previously mentioned encoding function to our training data, selectively converting the text into encoded characters, and returning an array of training data that will play nicely with Brain.js
So here’s what all of that code will look like (this goes into your ‘scripts.js’ file):
Something that you’ll notice here that wasn’t present in the example from the documentation shown earlier (other than the two helper functions that we’ve already gone over) is on line 20 in the train() function, which saves the trained neural network to a global variable called trainedNet . Add that to your ‘training-data.js’ file and you’re done! Here’s a tweet from Kim Kardashian that was not in my training data (i.e. Now all we need is to put something into training-data.js and we’ll be ready to go. Of course, your neural network’s accuracy will increase proportionally to the amount of training data that you give it, so feel free to use more or less than me and see how it affects your outcomes
Now, to run your newly-trained neural network just throw an extra line at the bottom of your ‘script.js’ file that calls the execute() function and passes in a tweet from Trump or Kardashian; make sure to console.log it because we haven’t built a UI. Alright, so now your index.html, brain.js, and scripts.js files are finished. Now you have a neural network that can be trained on any text that you want!"
Eugenio Culurciello,"Artificial Intelligence, AI in 2018 and beyond – Towards Data Science","For more details on predictive neural networks, see here, and here, and here. Limitations of current neural networks — We have talked about before on the limitation of neural networks as they are today. Neural Network Capsules are one approach to solve the limitation of current neural networks. Predictive neural networks — A major limitation of current neural networks is that they do not possess one of the most important features of human brains: their predictive power. Neural network architectures are the fundamental core of learning algorithms. Current neural networks are not able to learn new data without being re-trained from scratch at every instance. As such this is the core network for any work in reinforcement learning. Here is our list:
About neuromorphic neural networks hardware, please see here. But few people in the field understand how hardware can really change machine learning, neural networks and AI in general. We argue here that Capsules have to be extended with a few additional features:
Continuous learning — this is important because neural networks need to continue to learn new data-points continuously for their life."
Devin Soni,"Spiking Neural Networks, the Next Generation of Machine Learning","The 3rd generation of neural networks, spiking neural networks, aims to bridge the gap between neuroscience and machine learning, using biologically-realistic models of neurons to carry out computation. A spiking neural network (SNN) is fundamentally different from the neural networks that the machine learning community knows. Everyone who has been remotely tuned in to recent progress in machine learning has heard of the current 2nd generation artificial neural networks used for machine learning. Although we have unsupervised biological learning methods such as Hebbian learning and STDP, there are no known effective supervised training methods for SNNs that offer higher performance than 2nd generation networks. However, spike trains offer us enhanced ability to process spatio-temporal data, or in other words, real-world sensory data. Given that these SNNs are more powerful, in theory, than 2nd generation networks, it is natural to wonder why we do not see widespread use of them. Therefore, in order to properly use SNNs for real-world tasks, we would need to develop an effective supervised learning method. Since spike trains are not differentiable, we cannot train SNNs using gradient descent without losing the precise temporal information in spike trains. SNNs operate using spikes, which are discrete events that take place at points in time, rather than continuous values. The temporal aspect refers to the fact that spike trains occur over time, so what we lose in binary encoding, we gain in the temporal information of the spikes."
Carlos E. Perez,Surprise! Neurons are Now More Complex than We Thought!!,"One of the biggest misconceptions around is the idea that Deep Learning (DL) or Artificial Neural Networks (ANN) mimics biological neurons. New experiments on the nature of neurons have revealed that biological neurons are even more complex than we have imagined them to be:
In short, there is a lot more going on inside a single neuron than the simple idea of integrate-and-fire. Numenta’s model of a neuron is considerably more elaborate than the Deep Learning model of a neuron as you can see in this graphic:
The team at Numenta is betting on this approach in the hopes of creating something that is more capable than Deep Learning. The second is that research will begin in earnest to explore DL architectures with more complex internal node (or neuron) structures. Bycontrast, Deep Learning (despite its model of a cartoon neuron) has been shown to be unexpectedly effective in performing all kinds of mind-boggling feats of cognition. If you ask me, if you truly want to build biologically inspired cognition, then you should at the very least explore systems are not continuous like DL. After all, nature is already unequivocally telling us that neurons are individually more complex and therefore our own neuron models may also need to be more complex. If you think this throws a monkey wrench on our understanding, there’s an even newer discovery that reveals even greater complexity:
What this research reveals is that there is a mechanism for neurons to communicate with each other by sending packages of RNA code. This is not unlike the research that explores the use of complex values in neural networks. I suspect that these complexities are necessary for advanced cognition that seems to evade current Deep Learning systems."
Nityesh Agarwal,“WTH does a neural network even learn??” — a newcomer’s dilemma,"So, neural networks learn like we do! Let’s see how Michael Nielsen describes what the hidden neurons are doing in his book — Neural Networks and Deep Learning:
He, like many others, uses the analogy between neural networks and the human mind to try to explain a neural networks. Neural networks can be said to learn like us if you consider the way they build hierarchies of features just like we do. The weight matrix images were like the elementary Lego blocks and my task was to figure out a way to arrange them together so that I could create all 10 digits. So, if I was to look at the image belonging to one of the neurons in the hidden layer, it would be like a heat map showing one feature, one basic Lego block that will be used to recognise digits. But when it comes to recognising digits in an image, features like loops seem difficult and infeasible for a neural network (Remember, I’m talking about your vanilla neutral networks or MLPs here). It seems like the network has figured out a way to recognise 8s using features that it does not like! But when you see the features themselves, they are nothing like what we would use. So, I thought that, maybe, we could do the same with the features that the neural network actually found good. The way lines and edges make loops, which then help in recognising some digits is what we would think of doing."
